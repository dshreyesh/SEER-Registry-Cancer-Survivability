{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Survival Period Prediction: Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with importing the datset from a.csv file and replacing entries with codes representing missing values. We aslo drop any rows with missing values in either Diagnosis year (YEAR_DX) or Survival Time in Months (SRV_TIME_MON) as these values directly affect the target variable and hence, removed from model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dshre\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (134,135,136,137,141) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\dshre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\dshre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  \n",
      "C:\\Users\\dshre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\dshre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\dshre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "C:\\Users\\dshre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "C:\\Users\\dshre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "C:\\Users\\dshre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "C:\\Users\\dshre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              REG  MAR_STAT  RACE1V  NHIADE  SEX  AGE_DX  YR_BRTH  SEQ_NUM  \\\n",
      "PUBCSNUM                                                                     \n",
      "5100108.0  1502.0       5.0     1.0     NaN  2.0    65.0   1946.0      2.0   \n",
      "5100156.0  1502.0       2.0     1.0     NaN  2.0    78.0   1926.0      2.0   \n",
      "5100170.0  1502.0       1.0     1.0     NaN  2.0    67.0   1940.0      3.0   \n",
      "\n",
      "           MDXRECMP  YEAR_DX PRIMSITE  LATERAL  HISTO2V  BEHO2V  HISTO3V  \\\n",
      "PUBCSNUM                                                                   \n",
      "5100108.0      10.0   2011.0     C504      2.0   8230.0     2.0   8230.0   \n",
      "5100156.0       6.0   2005.0     C501      1.0   8500.0     2.0   8500.0   \n",
      "5100170.0       4.0   2008.0     C504      2.0   8500.0     2.0   8523.0   \n",
      "\n",
      "           BEHO3V  GRADE  DX_CONF  REPT_SRC  EOD10_SZ  EOD10_EX  EOD10_PE  \\\n",
      "PUBCSNUM                                                                    \n",
      "5100108.0     2.0    2.0      1.0       1.0       NaN       NaN       NaN   \n",
      "5100156.0     2.0    1.0      1.0       1.0       NaN       NaN       NaN   \n",
      "5100170.0     2.0    1.0      1.0       1.0       NaN       NaN       NaN   \n",
      "\n",
      "           EOD10_ND  EOD10_PN  EOD10_NE  EOD13  EOD2  EOD4  EOD_CODE  \\\n",
      "PUBCSNUM                                                               \n",
      "5100108.0       NaN       0.0       3.0    NaN   NaN   NaN       NaN   \n",
      "5100156.0       NaN       0.0       NaN    NaN   NaN   NaN       NaN   \n",
      "5100170.0       NaN       NaN       0.0    NaN   NaN   NaN       NaN   \n",
      "\n",
      "           TUMOR_1V  TUMOR_2V  TUMOR_3V  CSTUMSIZ  CSEXTEN  CSLYMPHN  \\\n",
      "PUBCSNUM                                                               \n",
      "5100108.0       NaN       NaN       NaN       5.0      0.0       0.0   \n",
      "5100156.0       NaN       NaN       NaN       8.0      0.0       0.0   \n",
      "5100170.0       NaN       NaN       NaN      36.0      0.0       0.0   \n",
      "\n",
      "           CSMETSDX  CS1SITE  CS2SITE  CS3SITE  CS4SITE  CS5SITE  CS6SITE  \\\n",
      "PUBCSNUM                                                                    \n",
      "5100108.0       0.0     10.0     10.0      0.0      1.0      0.0     10.0   \n",
      "5100156.0       0.0     10.0     10.0     98.0      0.0      0.0     10.0   \n",
      "5100170.0       0.0    998.0    998.0     98.0      0.0      0.0     10.0   \n",
      "\n",
      "           CS25SITE  DAJCCT  DAJCCN  DAJCCM  DAJCCSTG  DSS1977S  SCSSM2KO  \\\n",
      "PUBCSNUM                                                                    \n",
      "5100108.0     988.0     5.0     1.0     0.0       0.0       0.0       0.0   \n",
      "5100156.0     988.0     5.0     0.0     0.0       0.0       0.0       0.0   \n",
      "5100170.0     988.0     5.0     0.0     0.0       0.0       0.0       0.0   \n",
      "\n",
      "           DAJCCFL  CSVFIRST  CSVLATES  CSVCURRENT  SURGPRIF  SURGSCOF  \\\n",
      "PUBCSNUM                                                                 \n",
      "5100108.0      NaN   20302.0   20550.0     20540.0      20.0       NaN   \n",
      "5100156.0      NaN   10300.0   20550.0     20510.0      20.0       NaN   \n",
      "5100170.0      NaN   10401.0   20550.0     20510.0       0.0       NaN   \n",
      "\n",
      "           SURGSITF  NUMNODES  NO_SURG  SS_SURG  SURGSCOP  SURGSITE  REC_NO  \\\n",
      "PUBCSNUM                                                                      \n",
      "5100108.0       0.0       NaN      0.0      NaN       NaN       NaN     1.0   \n",
      "5100156.0       0.0       NaN      0.0      NaN       NaN       NaN     1.0   \n",
      "5100170.0       0.0       NaN      1.0      NaN       NaN       NaN     2.0   \n",
      "\n",
      "           TYPE_FU  AGE_1REC  SITERWHO  ICDOTO9V ICDOT10V  ICCC3WHO  \\\n",
      "PUBCSNUM                                                              \n",
      "5100108.0      2.0      14.0   26000.0    2330.0     D059     999.0   \n",
      "5100156.0      2.0      16.0   26000.0    2330.0     D059     999.0   \n",
      "5100170.0      2.0      14.0   26000.0    2330.0     D059     999.0   \n",
      "\n",
      "           ICCC3XWHO  BEHTREND  HISTREC  HISTRECB  CS0204SCHEMA  RAC_RECA  \\\n",
      "PUBCSNUM                                                                    \n",
      "5100108.0      999.0       2.0      5.0      98.0          13.0       1.0   \n",
      "5100156.0      999.0       2.0      9.0      98.0          13.0       1.0   \n",
      "5100170.0      999.0       2.0      9.0      98.0          13.0       1.0   \n",
      "\n",
      "           RAC_RECY  ORIGRECB  HST_STGA  AJCC_STG  AJ_3SEER  SSS77VZ  \\\n",
      "PUBCSNUM                                                               \n",
      "5100108.0       1.0       0.0       0.0       NaN       NaN      NaN   \n",
      "5100156.0       1.0       0.0       0.0       NaN       NaN      NaN   \n",
      "5100170.0       1.0       0.0       0.0       NaN       NaN      NaN   \n",
      "\n",
      "           SSSM2KPZ  FIRSTPRM  ST_CNTY   CODPUB  CODPUBKM  STAT_REC  IHSLINK  \\\n",
      "PUBCSNUM                                                                       \n",
      "5100108.0       NaN       0.0   9013.0      0.0       0.0       1.0      0.0   \n",
      "5100156.0       NaN       0.0   9011.0  50060.0   50060.0       0.0      0.0   \n",
      "5100170.0       NaN       0.0   9003.0      0.0       0.0       1.0      0.0   \n",
      "\n",
      "           SUMM2K  AYASITERWHO  LYMSUBRWHO  VSRTSADX  ODTHCLASS  CSTSEVAL  \\\n",
      "PUBCSNUM                                                                    \n",
      "5100108.0     0.0         99.0        99.0       9.0        9.0       3.0   \n",
      "5100156.0     0.0         99.0        99.0       9.0        9.0       3.0   \n",
      "5100170.0     0.0         99.0        99.0       9.0        9.0       1.0   \n",
      "\n",
      "           CSRGEVAL  CSMTEVAL  INTPRIM  ERSTATUS  PRSTATUS  CSSCHEMA  CS8SITE  \\\n",
      "PUBCSNUM                                                                        \n",
      "5100108.0       3.0       0.0      NaN       1.0       1.0      58.0      NaN   \n",
      "5100156.0       9.0       0.0      NaN       1.0       1.0      58.0      NaN   \n",
      "5100170.0       0.0       0.0      NaN       4.0       4.0      58.0      NaN   \n",
      "\n",
      "           CS10SITE  CS11SITE  CS13SITE  CS15SITE  CS16SITE  VASINV  \\\n",
      "PUBCSNUM                                                              \n",
      "5100108.0       NaN       NaN       NaN     998.0       NaN     NaN   \n",
      "5100156.0       NaN       NaN       NaN       NaN       NaN     NaN   \n",
      "5100170.0       NaN       NaN       NaN       NaN       NaN     NaN   \n",
      "\n",
      "           SRV_TIME_MON  SRV_TIME_MON_FLAG  INSREC_PUB  DAJCC7T  DAJCC7N  \\\n",
      "PUBCSNUM                                                                   \n",
      "5100108.0          62.0                1.0         3.0     50.0     10.0   \n",
      "5100156.0          54.0                1.0         NaN      NaN      NaN   \n",
      "5100170.0         104.0                1.0         3.0      NaN      NaN   \n",
      "\n",
      "           DAJCC7M  DAJCC7STG  ADJTM_6VALUE  ADJNM_6VALUE  ADJM_6VALUE  \\\n",
      "PUBCSNUM                                                                 \n",
      "5100108.0      0.0        0.0           5.0           0.0          0.0   \n",
      "5100156.0      NaN        NaN           5.0           0.0          0.0   \n",
      "5100170.0      NaN        NaN           5.0           0.0          0.0   \n",
      "\n",
      "           ADJAJCCSTG  CS7SITE  CS9SITE  CS12SITE  HER2  BRST_SUB  ANNARBOR  \\\n",
      "PUBCSNUM                                                                      \n",
      "5100108.0         0.0    999.0      NaN       NaN   4.0       5.0       8.0   \n",
      "5100156.0         0.0      NaN      NaN       NaN   NaN       NaN       8.0   \n",
      "5100170.0         0.0      NaN      NaN       NaN   NaN       NaN       8.0   \n",
      "\n",
      "           SCMETSDXB_PUB  SCMETSDXBR_PUB  SCMETSDXLIV_PUB  SCMETSDXLUNG_PUB  \\\n",
      "PUBCSNUM                                                                      \n",
      "5100108.0            0.0             0.0              0.0               0.0   \n",
      "5100156.0            NaN             NaN              NaN               NaN   \n",
      "5100170.0            NaN             NaN              NaN               NaN   \n",
      "\n",
      "           T_VALUE  N_VALUE  M_VALUE  MALIGCOUNT  BENBORDCOUNT  TUMSIZS  \\\n",
      "PUBCSNUM                                                                  \n",
      "5100108.0      NaN      NaN      NaN         2.0           0.0      NaN   \n",
      "5100156.0      NaN      NaN      NaN         2.0           0.0      NaN   \n",
      "5100170.0      NaN      NaN      NaN         4.0           0.0      NaN   \n",
      "\n",
      "          DSRPSG DASRCT DASRCN DASRCM  DASRCTS  DASRCNS  DASRCMS TNMEDNUM  \\\n",
      "PUBCSNUM                                                                    \n",
      "5100108.0    NaN    NaN    NaN    NaN      NaN      NaN      NaN      NaN   \n",
      "5100156.0    NaN    NaN    NaN    NaN      NaN      NaN      NaN      NaN   \n",
      "5100170.0    NaN    NaN    NaN    NaN      NaN      NaN      NaN      NaN   \n",
      "\n",
      "           METSDXLN  METSDXO  RADIATNR  RAD_BRNR  RAD_SURG  CHEMO_RX_REC  \\\n",
      "PUBCSNUM                                                                   \n",
      "5100108.0       NaN      NaN       2.0       NaN       3.0           0.0   \n",
      "5100156.0       NaN      NaN       0.0       NaN       0.0           0.0   \n",
      "5100170.0       NaN      NaN       1.0       NaN       0.0           0.0   \n",
      "\n",
      "           1year_survival  5year_survival  \n",
      "PUBCSNUM                                   \n",
      "5100108.0             1.0             1.0  \n",
      "5100156.0             1.0             0.0  \n",
      "5100170.0             1.0             1.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "breast_cancer=pd.read_csv('final_breast_cancer.csv',index_col=0)\n",
    "\n",
    "breast_cancer.replace(to_replace={'MAR_STAT':9,'RACE1V':99,'AGE_DX':999,'SEQ_NUM':99,\n",
    "                                  'Lateral':9,'GRADE':9,'DX_CONF':9,'CSEXTEN':999,\n",
    "                                  'CSLYMPHN':999,'DAJCCT':88,'DAJCCN':88,'DAJCCM':88,\n",
    "                                  'SURGSCOF':9,'SURGSITF':9,'NO_SURG':9,'AGE_1REC':99,\n",
    "                                  'RAC_RECA':9,'RAC_RECY':9,'HST_STGA':9,'INTPRIM':9,\n",
    "                                  'ERSTATUS':9,'PRSTATUS':9,'SRV_TIME_MON':9999,'SRV_TIME_MON_FLAG':9,\n",
    "                                  'HER2':9,'BRST_SUB':9,'MALIGCOUNT':99,'BENBORDCOUNT':99,\n",
    "                                  'RAD_SURG':9},value=pd.np.nan,inplace=True)\n",
    "\n",
    "breast_cancer.replace({'EOD10_PN':{95:pd.np.nan,96:pd.np.nan,97:pd.np.nan,98:pd.np.nan,99:pd.np.nan},\n",
    "                       'EOD10_NE':{95:pd.np.nan,96:pd.np.nan,97:pd.np.nan,98:pd.np.nan,99:pd.np.nan},\n",
    "                       'CSTUMSIZ':{990:0,991:10,992:20,993:30,994:40,995:50,996:pd.np.nan,997:pd.np.nan,998:pd.np.nan,\n",
    "                                   999:pd.np.nan,888:pd.np.nan},\n",
    "                       'DAJCCSTG':{88:pd.np.nan,90:pd.np.nan,99:pd.np.nan},\n",
    "                       'DSS1977S':{8:pd.np.nan,9:pd.np.nan},'SURGPRIF':{90:pd.np.nan,98:pd.np.nan,99:pd.np.nan},\n",
    "                       'ADJTM_6VALUE':{88:pd.np.nan,99:pd.np.nan},'ADJNM_6VALUE':{88:pd.np.nan,99:pd.np.nan},\n",
    "                       'ADJM_6VALUE':{88:pd.np.nan,99:pd.np.nan},'ADJAJCCSTG':{88:pd.np.nan,90:pd.np.nan,99:pd.np.nan}\n",
    "                       },inplace=True)\n",
    "\n",
    "breast_cancer.dropna(axis=0,how='any',subset=['YEAR_DX','SRV_TIME_MON'],inplace=True)\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(breast_cancer.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking for % Missing in each column of the dataset. Those columns with more than 20% missing, are measured across different timeframes or not related measurements to this type of cancer. Hence, they are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Missing in each column\n",
      "REG                  0.000000\n",
      "MAR_STAT             0.054712\n",
      "RACE1V               0.005031\n",
      "NHIADE               1.000000\n",
      "SEX                  0.000000\n",
      "AGE_DX               0.000039\n",
      "YR_BRTH              0.000039\n",
      "SEQ_NUM              0.000008\n",
      "MDXRECMP             0.000000\n",
      "YEAR_DX              0.000000\n",
      "PRIMSITE             0.000000\n",
      "LATERAL              0.000000\n",
      "HISTO2V              0.000000\n",
      "BEHO2V               0.000000\n",
      "HISTO3V              0.000000\n",
      "BEHO3V               0.000000\n",
      "GRADE                0.104800\n",
      "DX_CONF              0.002178\n",
      "REPT_SRC             0.000000\n",
      "EOD10_SZ             1.000000\n",
      "EOD10_EX             1.000000\n",
      "EOD10_PE             1.000000\n",
      "EOD10_ND             1.000000\n",
      "EOD10_PN             0.268726\n",
      "EOD10_NE             0.029288\n",
      "EOD13                1.000000\n",
      "EOD2                 1.000000\n",
      "EOD4                 1.000000\n",
      "EOD_CODE             1.000000\n",
      "TUMOR_1V             1.000000\n",
      "TUMOR_2V             1.000000\n",
      "TUMOR_3V             1.000000\n",
      "CSTUMSIZ             0.176855\n",
      "CSEXTEN              0.105456\n",
      "CSLYMPHN             0.110372\n",
      "CSMETSDX             0.084777\n",
      "CS1SITE              0.000000\n",
      "CS2SITE              0.000000\n",
      "CS3SITE              0.000000\n",
      "CS4SITE              0.000000\n",
      "CS5SITE              0.000000\n",
      "CS6SITE              0.000000\n",
      "CS25SITE             0.000000\n",
      "DAJCCT               0.085879\n",
      "DAJCCN               0.085879\n",
      "DAJCCM               0.085879\n",
      "DAJCCSTG             0.122652\n",
      "DSS1977S             0.097760\n",
      "SCSSM2KO             0.000000\n",
      "DAJCCFL              1.000000\n",
      "CSVFIRST             0.084777\n",
      "CSVLATES             0.084777\n",
      "CSVCURRENT           0.084777\n",
      "SURGPRIF             0.004743\n",
      "SURGSCOF             1.000000\n",
      "SURGSITF             0.002879\n",
      "NUMNODES             1.000000\n",
      "NO_SURG              0.002530\n",
      "SS_SURG              1.000000\n",
      "SURGSCOP             1.000000\n",
      "SURGSITE             1.000000\n",
      "REC_NO               0.000000\n",
      "TYPE_FU              0.000000\n",
      "AGE_1REC             0.000039\n",
      "SITERWHO             0.000000\n",
      "ICDOTO9V             0.000000\n",
      "ICDOT10V             0.000000\n",
      "ICCC3WHO             0.000000\n",
      "ICCC3XWHO            0.000000\n",
      "BEHTREND             0.000000\n",
      "HISTREC              0.000000\n",
      "HISTRECB             0.000000\n",
      "CS0204SCHEMA         0.000000\n",
      "RAC_RECA             0.007466\n",
      "RAC_RECY             0.007466\n",
      "ORIGRECB             0.000000\n",
      "HST_STGA             0.097760\n",
      "AJCC_STG             1.000000\n",
      "AJ_3SEER             1.000000\n",
      "SSS77VZ              1.000000\n",
      "SSSM2KPZ             1.000000\n",
      "FIRSTPRM             0.000000\n",
      "ST_CNTY              0.000000\n",
      "CODPUB               0.000000\n",
      "CODPUBKM             0.000000\n",
      "STAT_REC             0.000000\n",
      "IHSLINK              0.001078\n",
      "SUMM2K               0.000000\n",
      "AYASITERWHO          0.000000\n",
      "LYMSUBRWHO           0.084777\n",
      "VSRTSADX             0.000000\n",
      "ODTHCLASS            0.000000\n",
      "CSTSEVAL             0.110234\n",
      "CSRGEVAL             0.110231\n",
      "CSMTEVAL             0.110255\n",
      "INTPRIM              0.197065\n",
      "ERSTATUS             0.000000\n",
      "PRSTATUS             0.000000\n",
      "CSSCHEMA             0.000000\n",
      "CS8SITE              1.000000\n",
      "CS10SITE             1.000000\n",
      "CS11SITE             1.000000\n",
      "CS13SITE             1.000000\n",
      "CS15SITE             0.429285\n",
      "CS16SITE             1.000000\n",
      "VASINV               1.000000\n",
      "SRV_TIME_MON         0.000000\n",
      "SRV_TIME_MON_FLAG    0.000000\n",
      "INSREC_PUB           0.204482\n",
      "DAJCC7T              0.514062\n",
      "DAJCC7N              0.514062\n",
      "DAJCC7M              0.514062\n",
      "DAJCC7STG            0.514062\n",
      "ADJTM_6VALUE         0.123883\n",
      "ADJNM_6VALUE         0.129343\n",
      "ADJM_6VALUE          0.104682\n",
      "ADJAJCCSTG           0.122652\n",
      "CS7SITE              0.429285\n",
      "CS9SITE              1.000000\n",
      "CS12SITE             1.000000\n",
      "HER2                 0.429285\n",
      "BRST_SUB             0.429285\n",
      "ANNARBOR             0.000000\n",
      "SCMETSDXB_PUB        0.429285\n",
      "SCMETSDXBR_PUB       0.429285\n",
      "SCMETSDXLIV_PUB      0.429285\n",
      "SCMETSDXLUNG_PUB     0.429285\n",
      "T_VALUE              1.000000\n",
      "N_VALUE              1.000000\n",
      "M_VALUE              1.000000\n",
      "MALIGCOUNT           0.000008\n",
      "BENBORDCOUNT         0.000000\n",
      "TUMSIZS              0.915223\n",
      "DSRPSG               0.915223\n",
      "DASRCT               0.916053\n",
      "DASRCN               0.916053\n",
      "DASRCM               0.916053\n",
      "DASRCTS              0.916121\n",
      "DASRCNS              0.916121\n",
      "DASRCMS              0.916121\n",
      "TNMEDNUM             0.915223\n",
      "METSDXLN             0.915223\n",
      "METSDXO              0.915223\n",
      "RADIATNR             0.000000\n",
      "RAD_BRNR             1.000000\n",
      "RAD_SURG             0.000765\n",
      "CHEMO_RX_REC         0.000000\n",
      "1year_survival       0.000000\n",
      "5year_survival       0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "columns=breast_cancer.isna().sum(axis=0)/len(breast_cancer)\n",
    "columns_list=list(columns[columns<0.2].index)\n",
    "\n",
    "breast_cancer=breast_cancer.filter(items=columns_list,axis=1)\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print('% Missing in each column')\n",
    "    print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are treating all the other columns for their missing values. For the categorical variables, the missing values are filled with the mode while the median is used for Quantitative Variables. The columns which directly affect the target variable or are irrelevant to this type of cancer from domain knowledge are removed.\n",
    "\n",
    "In addition, classes of survival months are created for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'REG': 1541.0, 'MAR_STAT': 2.0, 'RACE1V': 1.0, 'SEX': 2.0, 'PRIMSITE': 'C504', 'LATERAL': 2.0, 'BEHO2V': 3.0, 'BEHO3V': 3.0, 'GRADE': 2.0, 'DX_CONF': 1.0, 'REPT_SRC': 1.0, 'CSMETSDX': 0.0, 'DAJCCT': 18.0, 'DAJCCN': 0.0, 'DAJCCM': 0.0, 'DAJCCSTG': 10.0, 'DSS1977S': 1.0, 'SCSSM2KO': 1.0, 'SURGPRIF': 22.0, 'SURGSITF': 0.0, 'NO_SURG': 0.0, 'TYPE_FU': 2.0, 'AGE_1REC': 13.0, 'SITERWHO': 26000.0, 'ICDOTO9V': 1744.0, 'ICDOT10V': 'C504', 'BEHTREND': 3.0, 'HISTREC': 9.0, 'HISTRECB': 98.0, 'CS0204SCHEMA': 13.0, 'RAC_RECA': 1.0, 'RAC_RECY': 1.0, 'ORIGRECB': 0.0, 'HST_STGA': 1.0, 'FIRSTPRM': 1.0, 'SUMM2K': 1.0, 'AYASITERWHO': 36.0, 'LYMSUBRWHO': 99.0, 'INTPRIM': 1.0, 'ERSTATUS': 1.0, 'PRSTATUS': 1.0, 'CSSCHEMA': 58.0, 'ADJTM_6VALUE': 18.0, 'ADJNM_6VALUE': 0.0, 'ADJM_6VALUE': 0.0, 'ADJAJCCSTG': 10.0, 'ANNARBOR': 8.0, 'RADIATNR': 0.0, 'RAD_SURG': 0.0, 'CHEMO_RX_REC': 0.0, 'AGE_DX': 61.0, 'YR_BRTH': 1949.0, 'SEQ_NUM': 0.0, 'EOD10_NE': 2.0, 'CSTUMSIZ': 16.0, 'CSEXTEN': 100.0, 'CSLYMPHN': 0.0, 'HISTO2V': 8500.0, 'HISTO3V': 8500.0, 'CS1SITE': 10.0, 'CS2SITE': 10.0, 'CS3SITE': 0.0, 'CS4SITE': 0.0, 'CS5SITE': 0.0, 'CS6SITE': 20.0, 'CS25SITE': 988.0, 'REC_NO': 1.0, 'MALIGCOUNT': 1.0, 'BENBORDCOUNT': 0.0}\n"
     ]
    }
   ],
   "source": [
    "stats=breast_cancer.describe().loc['50%']\n",
    "catg=breast_cancer.mode()\n",
    "\n",
    "drop_cols=['MDXRECMP','YEAR_DX','CSVFIRST','CSVLATES','CSVCURRENT','ICCC3WHO',\n",
    "           'ICCC3XWHO','CODPUB','CODPUBKM','STAT_REC','IHSLINK','VSRTSADX','ODTHCLASS',\n",
    "           'CSTSEVAL','CSRGEVAL','CSMTEVAL','ST_CNTY','SRV_TIME_MON','SRV_TIME_MON_FLAG',\n",
    "           '1year_survival','5year_survival']\n",
    "catg_cols=['REG','MAR_STAT','RACE1V','SEX','PRIMSITE','LATERAL','BEHO2V', 'BEHO3V','GRADE',\n",
    "           'DX_CONF','REPT_SRC','CSMETSDX','DAJCCT','DAJCCN','DAJCCM','DAJCCSTG','DSS1977S',\n",
    "           'SCSSM2KO','SURGPRIF','SURGSITF','NO_SURG','TYPE_FU','AGE_1REC','SITERWHO',\n",
    "           'ICDOTO9V','ICDOT10V','BEHTREND','HISTREC','HISTRECB','CS0204SCHEMA','RAC_RECA',\n",
    "           'RAC_RECY','ORIGRECB','HST_STGA','FIRSTPRM','SUMM2K','AYASITERWHO','LYMSUBRWHO',\n",
    "           'INTPRIM','ERSTATUS','PRSTATUS','CSSCHEMA','ADJTM_6VALUE','ADJNM_6VALUE',\n",
    "           'ADJM_6VALUE','ADJAJCCSTG','ANNARBOR','RADIATNR','RAD_SURG','CHEMO_RX_REC']\n",
    "num_cols=['AGE_DX','YR_BRTH','SEQ_NUM','EOD10_NE','CSTUMSIZ','CSEXTEN','CSLYMPHN',\n",
    "          'HISTO2V','HISTO3V','CS1SITE','CS2SITE','CS3SITE','CS4SITE','CS5SITE',\n",
    "          'CS6SITE','CS25SITE','REC_NO','MALIGCOUNT','BENBORDCOUNT']\n",
    "\n",
    "values=dict()\n",
    "for i in catg_cols:\n",
    "    values[i]=catg[i][0]\n",
    "for i in num_cols:\n",
    "    values[i]=stats[i]\n",
    "\n",
    "breast_cancer.fillna(value=values,inplace=True)\n",
    "\n",
    "breast_cancer['survival_classes']=breast_cancer.apply(lambda row: \n",
    "    '<=5yrs' if (row.SRV_TIME_MON<=60) \n",
    "    else ('5-10yrs'  if (row.SRV_TIME_MON<=120) else '>10yrs'),axis=1)\n",
    "\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              REG  MAR_STAT  RACE1V  SEX  AGE_DX  YR_BRTH  SEQ_NUM  MDXRECMP  \\\n",
      "PUBCSNUM                                                                       \n",
      "5100108.0  1502.0       5.0     1.0  2.0    65.0   1946.0      2.0      10.0   \n",
      "5100156.0  1502.0       2.0     1.0  2.0    78.0   1926.0      2.0       6.0   \n",
      "5100170.0  1502.0       1.0     1.0  2.0    67.0   1940.0      3.0       4.0   \n",
      "\n",
      "           YEAR_DX PRIMSITE  LATERAL  HISTO2V  BEHO2V  HISTO3V  BEHO3V  GRADE  \\\n",
      "PUBCSNUM                                                                        \n",
      "5100108.0   2011.0     C504      2.0   8230.0     2.0   8230.0     2.0    2.0   \n",
      "5100156.0   2005.0     C501      1.0   8500.0     2.0   8500.0     2.0    1.0   \n",
      "5100170.0   2008.0     C504      2.0   8500.0     2.0   8523.0     2.0    1.0   \n",
      "\n",
      "           DX_CONF  REPT_SRC  EOD10_NE  CSTUMSIZ  CSEXTEN  CSLYMPHN  CSMETSDX  \\\n",
      "PUBCSNUM                                                                        \n",
      "5100108.0      1.0       1.0       3.0       5.0      0.0       0.0       0.0   \n",
      "5100156.0      1.0       1.0       2.0       8.0      0.0       0.0       0.0   \n",
      "5100170.0      1.0       1.0       0.0      36.0      0.0       0.0       0.0   \n",
      "\n",
      "           CS1SITE  CS2SITE  CS3SITE  CS4SITE  CS5SITE  CS6SITE  CS25SITE  \\\n",
      "PUBCSNUM                                                                    \n",
      "5100108.0     10.0     10.0      0.0      1.0      0.0     10.0     988.0   \n",
      "5100156.0     10.0     10.0     98.0      0.0      0.0     10.0     988.0   \n",
      "5100170.0    998.0    998.0     98.0      0.0      0.0     10.0     988.0   \n",
      "\n",
      "           DAJCCT  DAJCCN  DAJCCM  DAJCCSTG  DSS1977S  SCSSM2KO  CSVFIRST  \\\n",
      "PUBCSNUM                                                                    \n",
      "5100108.0     5.0     1.0     0.0       0.0       0.0       0.0   20302.0   \n",
      "5100156.0     5.0     0.0     0.0       0.0       0.0       0.0   10300.0   \n",
      "5100170.0     5.0     0.0     0.0       0.0       0.0       0.0   10401.0   \n",
      "\n",
      "           CSVLATES  CSVCURRENT  SURGPRIF  SURGSITF  NO_SURG  REC_NO  TYPE_FU  \\\n",
      "PUBCSNUM                                                                        \n",
      "5100108.0   20550.0     20540.0      20.0       0.0      0.0     1.0      2.0   \n",
      "5100156.0   20550.0     20510.0      20.0       0.0      0.0     1.0      2.0   \n",
      "5100170.0   20550.0     20510.0       0.0       0.0      1.0     2.0      2.0   \n",
      "\n",
      "           AGE_1REC  SITERWHO  ICDOTO9V ICDOT10V  ICCC3WHO  ICCC3XWHO  \\\n",
      "PUBCSNUM                                                                \n",
      "5100108.0      14.0   26000.0    2330.0     D059     999.0      999.0   \n",
      "5100156.0      16.0   26000.0    2330.0     D059     999.0      999.0   \n",
      "5100170.0      14.0   26000.0    2330.0     D059     999.0      999.0   \n",
      "\n",
      "           BEHTREND  HISTREC  HISTRECB  CS0204SCHEMA  RAC_RECA  RAC_RECY  \\\n",
      "PUBCSNUM                                                                   \n",
      "5100108.0       2.0      5.0      98.0          13.0       1.0       1.0   \n",
      "5100156.0       2.0      9.0      98.0          13.0       1.0       1.0   \n",
      "5100170.0       2.0      9.0      98.0          13.0       1.0       1.0   \n",
      "\n",
      "           ORIGRECB  HST_STGA  FIRSTPRM  ST_CNTY   CODPUB  CODPUBKM  STAT_REC  \\\n",
      "PUBCSNUM                                                                        \n",
      "5100108.0       0.0       0.0       0.0   9013.0      0.0       0.0       1.0   \n",
      "5100156.0       0.0       0.0       0.0   9011.0  50060.0   50060.0       0.0   \n",
      "5100170.0       0.0       0.0       0.0   9003.0      0.0       0.0       1.0   \n",
      "\n",
      "           IHSLINK  SUMM2K  AYASITERWHO  LYMSUBRWHO  VSRTSADX  ODTHCLASS  \\\n",
      "PUBCSNUM                                                                   \n",
      "5100108.0      0.0     0.0         99.0        99.0       9.0        9.0   \n",
      "5100156.0      0.0     0.0         99.0        99.0       9.0        9.0   \n",
      "5100170.0      0.0     0.0         99.0        99.0       9.0        9.0   \n",
      "\n",
      "           CSTSEVAL  CSRGEVAL  CSMTEVAL  INTPRIM  ERSTATUS  PRSTATUS  \\\n",
      "PUBCSNUM                                                               \n",
      "5100108.0       3.0       3.0       0.0      1.0       1.0       1.0   \n",
      "5100156.0       3.0       9.0       0.0      1.0       1.0       1.0   \n",
      "5100170.0       1.0       0.0       0.0      1.0       4.0       4.0   \n",
      "\n",
      "           CSSCHEMA  SRV_TIME_MON  SRV_TIME_MON_FLAG  ADJTM_6VALUE  \\\n",
      "PUBCSNUM                                                             \n",
      "5100108.0      58.0          62.0                1.0           5.0   \n",
      "5100156.0      58.0          54.0                1.0           5.0   \n",
      "5100170.0      58.0         104.0                1.0           5.0   \n",
      "\n",
      "           ADJNM_6VALUE  ADJM_6VALUE  ADJAJCCSTG  ANNARBOR  MALIGCOUNT  \\\n",
      "PUBCSNUM                                                                 \n",
      "5100108.0           0.0          0.0         0.0       8.0         2.0   \n",
      "5100156.0           0.0          0.0         0.0       8.0         2.0   \n",
      "5100170.0           0.0          0.0         0.0       8.0         4.0   \n",
      "\n",
      "           BENBORDCOUNT  RADIATNR  RAD_SURG  CHEMO_RX_REC  1year_survival  \\\n",
      "PUBCSNUM                                                                    \n",
      "5100108.0           0.0       2.0       3.0           0.0             1.0   \n",
      "5100156.0           0.0       0.0       0.0           0.0             1.0   \n",
      "5100170.0           0.0       1.0       0.0           0.0             1.0   \n",
      "\n",
      "           5year_survival survival_classes  \n",
      "PUBCSNUM                                    \n",
      "5100108.0             1.0          5-10yrs  \n",
      "5100156.0             0.0           <=5yrs  \n",
      "5100170.0             1.0          5-10yrs  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(breast_cancer.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from numpy.random import seed\n",
    "#from tensorflow import set_random_seed\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report,roc_curve,auc\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Flatten, Conv1D\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, upsampling the dataset for the low_survival and mid_survival classes to the size of high survival class.\n",
    "\n",
    "Then, seperating the upsampled dataset into Predictors(data) set and Target Variable and getting dummy variables for all the categorical variables identified before. This inflates our dataset significantly as seen with the shape below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(373626, 414)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE_DX</th>\n",
       "      <th>YR_BRTH</th>\n",
       "      <th>SEQ_NUM</th>\n",
       "      <th>HISTO2V</th>\n",
       "      <th>HISTO3V</th>\n",
       "      <th>EOD10_NE</th>\n",
       "      <th>CSTUMSIZ</th>\n",
       "      <th>CSEXTEN</th>\n",
       "      <th>CSLYMPHN</th>\n",
       "      <th>CS1SITE</th>\n",
       "      <th>...</th>\n",
       "      <th>RADIATNR_7.0</th>\n",
       "      <th>RADIATNR_8.0</th>\n",
       "      <th>RAD_SURG_0.0</th>\n",
       "      <th>RAD_SURG_2.0</th>\n",
       "      <th>RAD_SURG_3.0</th>\n",
       "      <th>RAD_SURG_4.0</th>\n",
       "      <th>RAD_SURG_5.0</th>\n",
       "      <th>RAD_SURG_6.0</th>\n",
       "      <th>CHEMO_RX_REC_0.0</th>\n",
       "      <th>CHEMO_RX_REC_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUBCSNUM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38324058.0</th>\n",
       "      <td>77.0</td>\n",
       "      <td>1928.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19586522.0</th>\n",
       "      <td>55.0</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19632960.0</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1942.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8010.0</td>\n",
       "      <td>8575.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28546964.0</th>\n",
       "      <td>88.0</td>\n",
       "      <td>1915.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38793477.0</th>\n",
       "      <td>45.0</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 414 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            AGE_DX  YR_BRTH  SEQ_NUM  HISTO2V  HISTO3V  EOD10_NE  CSTUMSIZ  \\\n",
       "PUBCSNUM                                                                     \n",
       "38324058.0    77.0   1928.0      0.0   8500.0   8500.0       2.0      18.0   \n",
       "19586522.0    55.0   1949.0      0.0   8000.0   8000.0       0.0      16.0   \n",
       "19632960.0    64.0   1942.0      0.0   8010.0   8575.0      13.0      45.0   \n",
       "28546964.0    88.0   1915.0      0.0   8500.0   8500.0       7.0      22.0   \n",
       "38793477.0    45.0   1960.0      0.0   8500.0   8500.0       0.0      16.0   \n",
       "\n",
       "            CSEXTEN  CSLYMPHN  CS1SITE  ...  RADIATNR_7.0  RADIATNR_8.0  \\\n",
       "PUBCSNUM                                ...                               \n",
       "38324058.0    100.0       0.0     10.0  ...             0             0   \n",
       "19586522.0    100.0       0.0    998.0  ...             0             0   \n",
       "19632960.0    100.0     600.0     10.0  ...             0             0   \n",
       "28546964.0    100.0     250.0     20.0  ...             0             0   \n",
       "38793477.0    100.0       0.0     10.0  ...             0             0   \n",
       "\n",
       "            RAD_SURG_0.0  RAD_SURG_2.0  RAD_SURG_3.0  RAD_SURG_4.0  \\\n",
       "PUBCSNUM                                                             \n",
       "38324058.0             0             0             1             0   \n",
       "19586522.0             1             0             0             0   \n",
       "19632960.0             0             0             1             0   \n",
       "28546964.0             1             0             0             0   \n",
       "38793477.0             1             0             0             0   \n",
       "\n",
       "            RAD_SURG_5.0  RAD_SURG_6.0  CHEMO_RX_REC_0.0  CHEMO_RX_REC_1.0  \n",
       "PUBCSNUM                                                                    \n",
       "38324058.0             0             0                 1                 0  \n",
       "19586522.0             0             0                 0                 1  \n",
       "19632960.0             0             0                 0                 1  \n",
       "28546964.0             0             0                 1                 0  \n",
       "38793477.0             0             0                 1                 0  \n",
       "\n",
       "[5 rows x 414 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=breast_cancer[breast_cancer['YEAR_DX'].between(2004,2006)].drop(columns=drop_cols)\n",
    "low_survival=data[data['survival_classes']=='<=5yrs']\n",
    "mid_survival=data[data['survival_classes']=='5-10yrs']\n",
    "high_survival=data[data['survival_classes']=='>10yrs']\n",
    "\n",
    "low_survival=resample(low_survival,replace=True,n_samples=len(high_survival),random_state=21)\n",
    "mid_survival=resample(mid_survival,replace=True,n_samples=len(high_survival),random_state=21)\n",
    "\n",
    "data_upsampled=pd.concat([low_survival,mid_survival,high_survival],axis=0)\n",
    "    \n",
    "data=data_upsampled.drop(columns=['survival_classes'])\n",
    "target=data_upsampled['survival_classes']\n",
    "data=pd.get_dummies(data,prefix=catg_cols,columns=catg_cols,drop_first=False)\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole dataset is also split into training and test sets in the ratio of 80:20.\n",
    "\n",
    "Normalizing the training set using StandardScaler (z-score normalization). Normalization is required to help with faster and more accurate training of Neural Networks.\n",
    "Test set is also normalized using he same scaler used to normalize the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(298900, 414)\n",
      "(74726, 414)\n"
     ]
    }
   ],
   "source": [
    "train_data,test_data,train_target,test_target=train_test_split(data,target,test_size=0.2,random_state=101)\n",
    "\n",
    "scaler=StandardScaler()\n",
    "train_data=pd.DataFrame(scaler.fit_transform(train_data),index=train_data.index,columns=train_data.columns)\n",
    "test_data=pd.DataFrame(scaler.transform(test_data),index=test_data.index,columns=test_data.columns)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the different classes in the target set and then performing one hot encoding on them to get dummies of the target set. This is done for both training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_target)\n",
    "encoded_train_target = encoder.transform(train_target)\n",
    "dummy_train_target = np_utils.to_categorical(encoded_train_target)\n",
    "\n",
    "encoded_test_target=encoder.transform(test_target)\n",
    "dummy_test_target = np_utils.to_categorical(encoded_test_target)\n",
    "\n",
    "print(dummy_train_target[:5])\n",
    "print()\n",
    "print(dummy_test_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the structure of the ANN with 4 hidden layers with [300,212,106,25] neurons each and ReLU activation function on the hidden layers and softmax activation on the output layer. The output layer has three neuron in line with the number of classes for prediction. 10% dropout layer is used after first and second hidden layers.\n",
    "\n",
    "Loss Function- Categorical Crossentropy\n",
    "\n",
    "Optimizer- ADAM\n",
    "\n",
    "Observed Metrics- Accuracy\n",
    "\n",
    "An initial fit of the model is developed over a large number of epochs to determine point of overfitting of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 298900 samples, validate on 74726 samples\n",
      "Epoch 1/150\n",
      "298900/298900 [==============================] - 26s 85us/step - loss: 0.8998 - acc: 0.5633 - val_loss: 0.8895 - val_acc: 0.5697\n",
      "Epoch 2/150\n",
      "298900/298900 [==============================] - 27s 89us/step - loss: 0.8741 - acc: 0.5804 - val_loss: 0.8692 - val_acc: 0.5819\n",
      "Epoch 3/150\n",
      "298900/298900 [==============================] - 37s 123us/step - loss: 0.8534 - acc: 0.5931 - val_loss: 0.8505 - val_acc: 0.5956\n",
      "Epoch 4/150\n",
      "298900/298900 [==============================] - 32s 107us/step - loss: 0.8294 - acc: 0.6075 - val_loss: 0.8274 - val_acc: 0.6092\n",
      "Epoch 5/150\n",
      "298900/298900 [==============================] - 26s 86us/step - loss: 0.8036 - acc: 0.6237 - val_loss: 0.8067 - val_acc: 0.6232\n",
      "Epoch 6/150\n",
      "298900/298900 [==============================] - 29s 96us/step - loss: 0.7785 - acc: 0.6378 - val_loss: 0.7841 - val_acc: 0.6404\n",
      "Epoch 7/150\n",
      "298900/298900 [==============================] - 28s 94us/step - loss: 0.7566 - acc: 0.6523 - val_loss: 0.7629 - val_acc: 0.6497\n",
      "Epoch 8/150\n",
      "298900/298900 [==============================] - 27s 90us/step - loss: 0.7363 - acc: 0.6640 - val_loss: 0.7470 - val_acc: 0.6613\n",
      "Epoch 9/150\n",
      "298900/298900 [==============================] - 26s 88us/step - loss: 0.7193 - acc: 0.6732 - val_loss: 0.7311 - val_acc: 0.6727\n",
      "Epoch 10/150\n",
      "298900/298900 [==============================] - 27s 90us/step - loss: 0.7046 - acc: 0.6816 - val_loss: 0.7197 - val_acc: 0.6782\n",
      "Epoch 11/150\n",
      "298900/298900 [==============================] - 26s 87us/step - loss: 0.6911 - acc: 0.6900 - val_loss: 0.7064 - val_acc: 0.6870\n",
      "Epoch 12/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.6794 - acc: 0.6963 - val_loss: 0.6938 - val_acc: 0.6952\n",
      "Epoch 13/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.6660 - acc: 0.7028 - val_loss: 0.6825 - val_acc: 0.7007\n",
      "Epoch 14/150\n",
      "298900/298900 [==============================] - 24s 81us/step - loss: 0.6571 - acc: 0.7078 - val_loss: 0.6731 - val_acc: 0.7051\n",
      "Epoch 15/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.6485 - acc: 0.7129 - val_loss: 0.6618 - val_acc: 0.7125\n",
      "Epoch 16/150\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.6393 - acc: 0.7175 - val_loss: 0.6537 - val_acc: 0.7156\n",
      "Epoch 17/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.6329 - acc: 0.7217 - val_loss: 0.6497 - val_acc: 0.7210\n",
      "Epoch 18/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.6238 - acc: 0.7256 - val_loss: 0.6459 - val_acc: 0.7225\n",
      "Epoch 19/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.6186 - acc: 0.7294 - val_loss: 0.6373 - val_acc: 0.7277\n",
      "Epoch 20/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.6122 - acc: 0.7327 - val_loss: 0.6314 - val_acc: 0.7296\n",
      "Epoch 21/150\n",
      "298900/298900 [==============================] - 23s 79us/step - loss: 0.6059 - acc: 0.7352 - val_loss: 0.6221 - val_acc: 0.7339\n",
      "Epoch 22/150\n",
      "298900/298900 [==============================] - 25s 82us/step - loss: 0.5998 - acc: 0.7383 - val_loss: 0.6164 - val_acc: 0.7394979 - a - ETA: 4s - l -  - ETA: 1s - loss: 0.6001 - a - ETA: 0s - loss: 0.5999 - ac\n",
      "Epoch 23/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5960 - acc: 0.7411 - val_loss: 0.6157 - val_acc: 0.7405\n",
      "Epoch 24/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.5914 - acc: 0.7432 - val_loss: 0.6113 - val_acc: 0.7421\n",
      "Epoch 25/150\n",
      "298900/298900 [==============================] - 23s 79us/step - loss: 0.5847 - acc: 0.7458 - val_loss: 0.6059 - val_acc: 0.7446\n",
      "Epoch 26/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5812 - acc: 0.7493 - val_loss: 0.6056 - val_acc: 0.7458\n",
      "Epoch 27/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5783 - acc: 0.7498 - val_loss: 0.6002 - val_acc: 0.7498\n",
      "Epoch 28/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5726 - acc: 0.7528 - val_loss: 0.5964 - val_acc: 0.7516\n",
      "Epoch 29/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5691 - acc: 0.7544 - val_loss: 0.5933 - val_acc: 0.7514\n",
      "Epoch 30/150\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.5659 - acc: 0.7567 - val_loss: 0.5873 - val_acc: 0.7558\n",
      "Epoch 31/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5619 - acc: 0.7590 - val_loss: 0.5827 - val_acc: 0.7578\n",
      "Epoch 32/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5587 - acc: 0.7597 - val_loss: 0.5834 - val_acc: 0.7590\n",
      "Epoch 33/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.5533 - acc: 0.7625 - val_loss: 0.5764 - val_acc: 0.7630\n",
      "Epoch 34/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5528 - acc: 0.7629 - val_loss: 0.5749 - val_acc: 0.7635\n",
      "Epoch 35/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5495 - acc: 0.7652 - val_loss: 0.5753 - val_acc: 0.7635\n",
      "Epoch 36/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.5464 - acc: 0.7673 - val_loss: 0.5701 - val_acc: 0.7676\n",
      "Epoch 37/150\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.5421 - acc: 0.7677 - val_loss: 0.5749 - val_acc: 0.7657\n",
      "Epoch 38/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.5400 - acc: 0.7698 - val_loss: 0.5700 - val_acc: 0.7690\n",
      "Epoch 39/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.5384 - acc: 0.7701 - val_loss: 0.5641 - val_acc: 0.7719\n",
      "Epoch 40/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.5360 - acc: 0.7715 - val_loss: 0.5632 - val_acc: 0.7692\n",
      "Epoch 41/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.5316 - acc: 0.7734 - val_loss: 0.5592 - val_acc: 0.7740\n",
      "Epoch 42/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.5313 - acc: 0.7740 - val_loss: 0.5578 - val_acc: 0.7727\n",
      "Epoch 43/150\n",
      "298900/298900 [==============================] - 23s 75us/step - loss: 0.5289 - acc: 0.7747 - val_loss: 0.5574 - val_acc: 0.7748\n",
      "Epoch 44/150\n",
      "298900/298900 [==============================] - 23s 75us/step - loss: 0.5273 - acc: 0.7752 - val_loss: 0.5554 - val_acc: 0.7737\n",
      "Epoch 45/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.5232 - acc: 0.7787 - val_loss: 0.5527 - val_acc: 0.7763\n",
      "Epoch 46/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.5201 - acc: 0.7798 - val_loss: 0.5535 - val_acc: 0.7750\n",
      "Epoch 47/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5197 - acc: 0.7789 - val_loss: 0.5469 - val_acc: 0.7779\n",
      "Epoch 48/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.5177 - acc: 0.7808 - val_loss: 0.5506 - val_acc: 0.7766\n",
      "Epoch 49/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.5162 - acc: 0.7821 - val_loss: 0.5465 - val_acc: 0.7788\n",
      "Epoch 50/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.5137 - acc: 0.7833 - val_loss: 0.5434 - val_acc: 0.7811\n",
      "Epoch 51/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.5110 - acc: 0.7846 - val_loss: 0.5413 - val_acc: 0.7826\n",
      "Epoch 52/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.5108 - acc: 0.7842 - val_loss: 0.5382 - val_acc: 0.7838\n",
      "Epoch 53/150\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.5090 - acc: 0.7855 - val_loss: 0.5388 - val_acc: 0.7841\n",
      "Epoch 54/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.5070 - acc: 0.7857 - val_loss: 0.5364 - val_acc: 0.7865\n",
      "Epoch 55/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.5055 - acc: 0.7864 - val_loss: 0.5371 - val_acc: 0.7854\n",
      "Epoch 56/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.5032 - acc: 0.7881 - val_loss: 0.5325 - val_acc: 0.7877\n",
      "Epoch 57/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.5022 - acc: 0.7881 - val_loss: 0.5330 - val_acc: 0.7874\n",
      "Epoch 58/150\n",
      "298900/298900 [==============================] - 22s 74us/step - loss: 0.4996 - acc: 0.7893 - val_loss: 0.5315 - val_acc: 0.7885\n",
      "Epoch 59/150\n",
      "298900/298900 [==============================] - 22s 74us/step - loss: 0.4988 - acc: 0.7909 - val_loss: 0.5303 - val_acc: 0.7898\n",
      "Epoch 60/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4988 - acc: 0.7904 - val_loss: 0.5316 - val_acc: 0.7892\n",
      "Epoch 61/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4968 - acc: 0.7910 - val_loss: 0.5256 - val_acc: 0.7908\n",
      "Epoch 62/150\n",
      "298900/298900 [==============================] - 23s 75us/step - loss: 0.4951 - acc: 0.7924 - val_loss: 0.5291 - val_acc: 0.7900\n",
      "Epoch 63/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4932 - acc: 0.7928 - val_loss: 0.5286 - val_acc: 0.7893\n",
      "Epoch 64/150\n",
      "298900/298900 [==============================] - 22s 74us/step - loss: 0.4934 - acc: 0.7931 - val_loss: 0.5269 - val_acc: 0.7911\n",
      "Epoch 65/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4899 - acc: 0.7940 - val_loss: 0.5238 - val_acc: 0.7913\n",
      "Epoch 66/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4894 - acc: 0.7948 - val_loss: 0.5241 - val_acc: 0.7930\n",
      "Epoch 67/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4883 - acc: 0.7951 - val_loss: 0.5257 - val_acc: 0.7916\n",
      "Epoch 68/150\n",
      "298900/298900 [==============================] - 23s 75us/step - loss: 0.4868 - acc: 0.7963 - val_loss: 0.5194 - val_acc: 0.7946\n",
      "Epoch 69/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4866 - acc: 0.7962 - val_loss: 0.5202 - val_acc: 0.7964\n",
      "Epoch 70/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4843 - acc: 0.7970 - val_loss: 0.5222 - val_acc: 0.7944\n",
      "Epoch 71/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4840 - acc: 0.7981 - val_loss: 0.5211 - val_acc: 0.7955\n",
      "Epoch 72/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4817 - acc: 0.7984 - val_loss: 0.5193 - val_acc: 0.7951\n",
      "Epoch 73/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4813 - acc: 0.7987 - val_loss: 0.5133 - val_acc: 0.7982\n",
      "Epoch 74/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4814 - acc: 0.7992 - val_loss: 0.5153 - val_acc: 0.7986\n",
      "Epoch 75/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4795 - acc: 0.7992 - val_loss: 0.5188 - val_acc: 0.7953\n",
      "Epoch 76/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4772 - acc: 0.8010 - val_loss: 0.5168 - val_acc: 0.7978\n",
      "Epoch 77/150\n",
      "298900/298900 [==============================] - 26s 88us/step - loss: 0.4774 - acc: 0.8011 - val_loss: 0.5154 - val_acc: 0.7997\n",
      "Epoch 78/150\n",
      "298900/298900 [==============================] - 27s 89us/step - loss: 0.4770 - acc: 0.8010 - val_loss: 0.5114 - val_acc: 0.8005\n",
      "Epoch 79/150\n",
      "298900/298900 [==============================] - 26s 88us/step - loss: 0.4739 - acc: 0.8029 - val_loss: 0.5131 - val_acc: 0.8010\n",
      "Epoch 80/150\n",
      "298900/298900 [==============================] - 24s 81us/step - loss: 0.4732 - acc: 0.8031 - val_loss: 0.5084 - val_acc: 0.8013\n",
      "Epoch 81/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4728 - acc: 0.8036 - val_loss: 0.5086 - val_acc: 0.8023\n",
      "Epoch 82/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4720 - acc: 0.8030 - val_loss: 0.5071 - val_acc: 0.8051\n",
      "Epoch 83/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4721 - acc: 0.8039 - val_loss: 0.5068 - val_acc: 0.8032\n",
      "Epoch 84/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4689 - acc: 0.8055 - val_loss: 0.5088 - val_acc: 0.8015\n",
      "Epoch 85/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.4703 - acc: 0.8040 - val_loss: 0.5084 - val_acc: 0.8023\n",
      "Epoch 86/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4690 - acc: 0.8050 - val_loss: 0.5094 - val_acc: 0.8012\n",
      "Epoch 87/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4675 - acc: 0.8055 - val_loss: 0.5102 - val_acc: 0.8016\n",
      "Epoch 88/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4652 - acc: 0.8072 - val_loss: 0.5071 - val_acc: 0.8048\n",
      "Epoch 89/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4645 - acc: 0.8068 - val_loss: 0.5121 - val_acc: 0.8004\n",
      "Epoch 90/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4638 - acc: 0.8082 - val_loss: 0.5091 - val_acc: 0.8007\n",
      "Epoch 91/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.4647 - acc: 0.8073 - val_loss: 0.5031 - val_acc: 0.8046\n",
      "Epoch 92/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.4619 - acc: 0.8088 - val_loss: 0.5034 - val_acc: 0.8044\n",
      "Epoch 93/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4622 - acc: 0.8077 - val_loss: 0.5037 - val_acc: 0.8066\n",
      "Epoch 94/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4620 - acc: 0.8085 - val_loss: 0.5007 - val_acc: 0.8082\n",
      "Epoch 95/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4629 - acc: 0.8077 - val_loss: 0.4987 - val_acc: 0.8078\n",
      "Epoch 96/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4616 - acc: 0.8086 - val_loss: 0.5027 - val_acc: 0.8073\n",
      "Epoch 97/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4609 - acc: 0.8084 - val_loss: 0.4975 - val_acc: 0.8084\n",
      "Epoch 98/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4580 - acc: 0.8103 - val_loss: 0.4978 - val_acc: 0.8104\n",
      "Epoch 99/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4594 - acc: 0.8091 - val_loss: 0.5022 - val_acc: 0.8077\n",
      "Epoch 100/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4571 - acc: 0.8104 - val_loss: 0.4952 - val_acc: 0.8100\n",
      "Epoch 101/150\n",
      "298900/298900 [==============================] - 25s 82us/step - loss: 0.4541 - acc: 0.8119 - val_loss: 0.4968 - val_acc: 0.8099\n",
      "Epoch 102/150\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.4556 - acc: 0.8114 - val_loss: 0.4950 - val_acc: 0.8092\n",
      "Epoch 103/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4538 - acc: 0.8126 - val_loss: 0.5021 - val_acc: 0.8064\n",
      "Epoch 104/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4522 - acc: 0.8128 - val_loss: 0.4944 - val_acc: 0.8102\n",
      "Epoch 105/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4535 - acc: 0.8126 - val_loss: 0.4942 - val_acc: 0.8107\n",
      "Epoch 106/150\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.4517 - acc: 0.8129 - val_loss: 0.5020 - val_acc: 0.8058\n",
      "Epoch 107/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.4528 - acc: 0.8126 - val_loss: 0.4914 - val_acc: 0.8126\n",
      "Epoch 108/150\n",
      "298900/298900 [==============================] - 25s 84us/step - loss: 0.4513 - acc: 0.8134 - val_loss: 0.5006 - val_acc: 0.8091\n",
      "Epoch 109/150\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.4511 - acc: 0.8131 - val_loss: 0.4987 - val_acc: 0.8094\n",
      "Epoch 110/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4517 - acc: 0.8135 - val_loss: 0.4913 - val_acc: 0.8135\n",
      "Epoch 111/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4500 - acc: 0.8148 - val_loss: 0.4989 - val_acc: 0.8097\n",
      "Epoch 112/150\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.4490 - acc: 0.8143 - val_loss: 0.4920 - val_acc: 0.8123\n",
      "Epoch 113/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4484 - acc: 0.8148 - val_loss: 0.5005 - val_acc: 0.8084\n",
      "Epoch 114/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4480 - acc: 0.8148 - val_loss: 0.4917 - val_acc: 0.8111\n",
      "Epoch 115/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.4464 - acc: 0.8167 - val_loss: 0.4925 - val_acc: 0.8127\n",
      "Epoch 116/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4454 - acc: 0.8169 - val_loss: 0.4882 - val_acc: 0.8135\n",
      "Epoch 117/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4462 - acc: 0.8157 - val_loss: 0.4874 - val_acc: 0.8131\n",
      "Epoch 118/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4469 - acc: 0.8157 - val_loss: 0.4934 - val_acc: 0.8134\n",
      "Epoch 119/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4431 - acc: 0.8181 - val_loss: 0.4841 - val_acc: 0.8162\n",
      "Epoch 120/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4433 - acc: 0.8176 - val_loss: 0.4870 - val_acc: 0.8138\n",
      "Epoch 121/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.4436 - acc: 0.8168 - val_loss: 0.4867 - val_acc: 0.8145\n",
      "Epoch 122/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4419 - acc: 0.8181 - val_loss: 0.4859 - val_acc: 0.8154\n",
      "Epoch 123/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.4419 - acc: 0.8176 - val_loss: 0.4858 - val_acc: 0.8139\n",
      "Epoch 124/150\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4411 - acc: 0.8182 - val_loss: 0.4843 - val_acc: 0.8154\n",
      "Epoch 125/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4412 - acc: 0.8187 - val_loss: 0.4837 - val_acc: 0.8159\n",
      "Epoch 126/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.4403 - acc: 0.8191 - val_loss: 0.4875 - val_acc: 0.8145\n",
      "Epoch 127/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4413 - acc: 0.8190 - val_loss: 0.4894 - val_acc: 0.8147\n",
      "Epoch 128/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.4412 - acc: 0.8183 - val_loss: 0.4834 - val_acc: 0.8169\n",
      "Epoch 129/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4382 - acc: 0.8205 - val_loss: 0.4845 - val_acc: 0.8162\n",
      "Epoch 130/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4381 - acc: 0.8196 - val_loss: 0.4846 - val_acc: 0.8168\n",
      "Epoch 131/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4373 - acc: 0.8207 - val_loss: 0.4919 - val_acc: 0.8147\n",
      "Epoch 132/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4380 - acc: 0.8193 - val_loss: 0.4853 - val_acc: 0.8176\n",
      "Epoch 133/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4367 - acc: 0.8207 - val_loss: 0.4848 - val_acc: 0.8168\n",
      "Epoch 134/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4371 - acc: 0.8203 - val_loss: 0.4841 - val_acc: 0.8165\n",
      "Epoch 135/150\n",
      "298900/298900 [==============================] - 23s 75us/step - loss: 0.4330 - acc: 0.8222 - val_loss: 0.4808 - val_acc: 0.8176\n",
      "Epoch 136/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.4351 - acc: 0.8210 - val_loss: 0.4823 - val_acc: 0.8179\n",
      "Epoch 137/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4353 - acc: 0.8215 - val_loss: 0.4810 - val_acc: 0.8190\n",
      "Epoch 138/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4346 - acc: 0.8211 - val_loss: 0.4790 - val_acc: 0.8205\n",
      "Epoch 139/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4342 - acc: 0.8216 - val_loss: 0.4751 - val_acc: 0.8204\n",
      "Epoch 140/150\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.4332 - acc: 0.8223 - val_loss: 0.4751 - val_acc: 0.8203\n",
      "Epoch 141/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4330 - acc: 0.8223 - val_loss: 0.4778 - val_acc: 0.8190\n",
      "Epoch 142/150\n",
      "298900/298900 [==============================] - 23s 75us/step - loss: 0.4312 - acc: 0.8235 - val_loss: 0.4755 - val_acc: 0.8203\n",
      "Epoch 143/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4327 - acc: 0.8224 - val_loss: 0.4764 - val_acc: 0.8209\n",
      "Epoch 144/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4320 - acc: 0.8233 - val_loss: 0.4816 - val_acc: 0.8177\n",
      "Epoch 145/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.4321 - acc: 0.8231 - val_loss: 0.4831 - val_acc: 0.8150\n",
      "Epoch 146/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.4303 - acc: 0.8238 - val_loss: 0.4767 - val_acc: 0.8200\n",
      "Epoch 147/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.4316 - acc: 0.8233 - val_loss: 0.4879 - val_acc: 0.8156\n",
      "Epoch 148/150\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.4300 - acc: 0.8239 - val_loss: 0.4750 - val_acc: 0.8214\n",
      "Epoch 149/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4300 - acc: 0.8236 - val_loss: 0.4780 - val_acc: 0.8190\n",
      "Epoch 150/150\n",
      "298900/298900 [==============================] - 22s 75us/step - loss: 0.4290 - acc: 0.8243 - val_loss: 0.4748 - val_acc: 0.8211\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "    \n",
    "model.add(Dense(300, input_dim=train_data.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(212, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(106, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(dummy_train_target.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "hist=model.fit(train_data, dummy_train_target, epochs=150, batch_size=200, verbose=1, validation_data=(test_data,dummy_test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd3hUVfrA8e87k0kmvZBAQgIkdAIhCb0oRUQQESygCLr28rOjuOi6llXZXXtby+qqKKKAINgQFEERpGMooSSUQEICpDdSppzfHzNgwCQMkGQScj7PM48z995zz3uHOO+9595zjiil0DRN05ovg7sD0DRN09xLJwJN07RmTicCTdO0Zk4nAk3TtGZOJwJN07RmTicCTdO0Zk4nAq3OiYhRREpEpG1dbttcicinIvK0u+PQzl86EWg4f4iPv+wiUlbl85Qz3Z9SyqaU8lNKHazLbc+UiDwnIjPrer+NkYhcLCJKRB5ydyxa06MTgYbzh9hPKeUHHAQur7Js9qnbi4hHw0epncaNQJ7zvw1K/z00fToRaKflPLOeKyKfi0gxcL2IDBSRtSJSICJZIvKGiJic23s4z06jnZ8/da7/XkSKRWSNiMSc6bbO9ZeKSIqIFIrImyKyWkRuOotj6i4ivzjj3yYil1VZN1ZEdjrrzxCRqc7lLUVksbNMnoisrGX//3GWLRKRDSIy6JTv83PnsRaLyHYR6VVlfW8RSXKu+xzwOs2x+AFXAf8HxIpIwinrhzj/rQpFJF1EbnAu9xGRV0XkoHPdShHxcl5dpJ2yjwwRGVYlfpf/Hpxl4kRkmfN7OywifxWRSBE5JiJBVbbr71yvk0sD0olAc9WVwGdAIDAXsAIPAKHAYGA0cGct5ScDTwAhOK46nj3TbUWkJTAPeMRZ736g35keiIh4At8C3wFhwFRgroh0dG7yEXCrUsof6An84lz+CLDPWSbcGWNN1jnLhgDzgS9EpOoP+hXALCAI+B54wxmbF/AV8KGz7FfObWszEch31rMM+EuVY41xHucrQAsgEdjmXP2qM8b+zrr+BthPU9dxLv89iEigM65vgAigM/CzUuoQsMoZ/3HXA58rpawuxqHVAZ0INFetUkp9o5SyK6XKlFIblFLrlFJWpdQ+4D1gaC3l5yulNiqlLMBsIOEsth0LJCmlvnKuexXIOYtjGQx4Ai8qpSxKqWU4fownOddbcJxZ+yul8pRSm6ssbw20VUpVKqV++dOenZRSs5xlrcALQADQscomvyilliqlbDgSwvFjHAwo4E1nbHOA309zPDcCc5RSdhw/zlOqnFFfDyxRSs1z/lvlKKWSRMQI3ATcr5TKct6rWeX8Xl1xJn8P44B0pdTrSqkKpVSRUmq9c93HzhiPNzFd6/w+tAakE4HmqvSqH0Skq4h857yMLwKewXE2WJPDVd4fA/zOYtvWVeNQjhETM1yI/VStgYPq5BEXDwCRzvdX4vjxOigiP4tIf+fyfzu3+0lE9orIIzVV4Gz62CUihTjO1n05+fs59Rh9q8SWUU1sNdUTDQzBkTABFuL4vkY7P7cB9lZTtBWOZFjdOlecyd9DG2BPDftZCMSL46mx0UB2lcSrNRCdCDRXnTpM7X+B7UBHpVQA8CQg9RxDFhB1/IOICH/8eJ+JTKCNs/xxbYFDAM4z23FASxxNSHOcy4uUUlOVUtE4mmumi8ifroJEZDjwEHA1jqafYKAE176fk46xSmw1+Ytzv9+LyGEcP7ie/NE8lA50qKbcEaCyhnWlgM/xD84z9RanbHMmfw81xYBS6hiwAJgC3IC+GnALnQi0s+UPFAKlItKN2u8P1JVvgV4icrnzx+kBHO31tTGKiLnKywv4DUeb9sMiYhKRi4AxwDwR8RaRySIS4GwmKQZsAM56OzgTSKFzua2aOv2d+88BTMDT/HHGfzqrAIOI3CuOG+kTgV61bP8XHD+6CVVe1wLjRCQY+BQYLSJXO/cXKiLxziapmcBrIhIujv4cg503eHcB/iIyyvn5Kedx1Ka2v4evgbbOY/IUkQARqXpv5xPgFuAyZ7xaA9OJQDtbD+Nomy7GcTY4t74rVEodwfEj9wqQi+Ms83egopZi1wNlVV67lVIVwOXAeBw/1m8Ak5VSKc4yNwIHnE0ct+I4UwXoAizHcXa/GnhdKbWqmjoX47g5mgqkAUU4zvRdOcYKHE1Tt+NoUroKWFTdtiJyAY6mpLeUUoePv3A0t6QB1yql9juPdTqOx0s3A3HOXUwFdgKbnOv+CYhSKh+4D0f7/SHnuqpNWdWp8e9BKVUIjMRxhXQUSOHk+0krASOwTil1Nk192jkSPTGN1lQ5b3hmAhOUUr+6Ox7t7InjUdwPlVIz3R1Lc6SvCLQmRURGi0igs4nnCRxNMOtPU0xrxERkANAD+MLdsTRXOhFoTc0FOJ7lz8HxlMkVzuYUrQkSkdnAEuABpVSpu+NprnTTkKZpWjOnrwg0TdOauSY3nkdoaKiKjo52dxiapmlNyqZNm3KUUtU+bt3kEkF0dDQbN250dxiapmlNiojU2ENdNw1pmqY1czoRaJqmNXM6EWiapjVzOhFomqY1czoRaJqmNXP1mgicwwHsFpE9IvJoNevbichPIrLVOe77qcPvapqmafWs3hKBc0Cwt4BLgVjgOhGJPWWzl4BPlFI9cUxk8a/6ikfTNE2rXn1eEfQD9iil9imlKnFM7jH+lG1igZ+c71dUs77ObEzL49/f70IPqaFpmnay+kwEkZw8nV0Gf55NaguOMcrBMQa7v4icOhMSInKHiGwUkY3Z2dlnFUxyZhHv/rKXI0V6fDJN07Sq6jMRVDct36mn49OAoSLyO46JKg7hGFb45EJKvaeU6qOU6hMWdroJqaoX2zoAgB1ZhWdVXtM07XxVn4kgA8ek1cdF4ZhE5ASlVKZS6iqlVCLwuHNZvfxSdw33ByD5UFF97F7TNK3Jqs9EsAHoJCIxIuIJTMIxd+kJzvlTj8fwGPBhfQXjb7AwJiidHVk6EWiaplVVb4lAKWUF7gWW4pgXdZ5SKllEnhGRcc7NhgG7RSQFaAXMqK94+O0N/lP+KAcyXZo6VtM0rdmo19FHlVKLcUzkXXXZk1Xezwfm12cMJ7QdiAFFy4IkispHEWA2NUi1mqZpjV3z6Vkc1Re7eNDPsJtdWcXujkbTNK3RaD6JwNMHa3g8fQ27SM7UTw5pmqYd13wSAWCKGUy8YR8pGWfXF0HTNO181KwSgbQbhCdWbOl6hjNN07TjmlUioO0AFEJE4WYqrXZ3R6NpmtYoNK9E4B1McUAneqPvE2iaph3XvBIB4BEzmF6GVDbt0/cJNE3ToBkmAp9OQ/CTcrJT1rk7FE3TtEah2SUCYoYA4J+1Wg9JrWmaRnNMBL6h5Pt3JsG6lX05pe6ORtM0ze2aXyIApP1Q+hhS2LxXjzukaZrWLBNBYOzFmMVCzs5f3R2Kpmma2zXLRCDtBmHDgF/maneHomma5nbNMhFgDiA7oAc9KpI4Wlzu7mg0TdPcqnkmAsAePYSespcd+zLcHYqmaZpbNdtEEBw7DKMoclPWuDsUTdM0t2q2icC7XR8A1KHNbo5E0zTNvZptIsA7mKOebWhRuF13LNM0rVlrvokAKG7Rk272VA4X6RvGmqY1X806EXi27UO45LM7NcXdoWiaprlNs04ELbsNAqBA3zDWNK0Za9aJwCsyAStGjId/d3comqZpbtOsEwEmM4fNHWlZpG8Ya5rWfDXvRACUhsXTTe0lI0+PRKppWvPU7BOBObovAVJG2m7dPKRpWvPk4e4A3K1V7FD4Fcr3/gaDLnR3OJqmnWeKKovYeHgj6cXpxLaIJTogmoPFBzlUcghPgycIZBRncKT0CCHmEILNweSW53L02FE8DZ6YPcxklWZxoOgAd/S8g5HtRtZ5jM0+EZjDO5Mvgfgc3uDuUDRNa2CHSw/jZfQi2BxMubWcHbk7CDGHEB0Yjc1uIzk3mRJLCaHeoWzN3sqClAWUWEroG96XVj6tOHzsMJW2SlpZffA7WkqBtYijlbmkl2aSIQUU+Ruw2W2I3U5EHoQWKgwKktoLyiAnxeJv8qPEUopCYa6EiZu92N7eyI4IGzGqBZduMeDbsgja1f330OwTASIc9O1Ju5Kt7o5E07RaWO1W0ovTCfMOw8/T78Ryi81CWlEaAZ4BBHoFkl+ez5FjRzh87DBHS4+SV55HQUUBCoVBDET4RtDKpxVL05by6yHHnCSh3qEUVBRgtVsBaOPfhpLKEvIr8k+KoVNwJ6J9o1i1/TuCDpeSmOVFr31WYg5UYqzmeZOS1kFUtAwkJPUoUlr2R8zdO+D/5HRUu0isykrQqh0UPP8yodP+gXXEQMqnP0PpihVcvgLMPXpQkZqKqqigVWI59K/771YnAqAsvC9Re34l7/ABQsLrId1qmlarosoiluxfwtbsrZTbyvEyehEfFk+XkC4YxcjOvJ0sWPUePnsySWovePsGEukXiY/Jh+Ts7dgqyrGY/jjD9jumaFEMfmWKjHATHkFBGA1GrHYroXtyuWiLnZ6eZi7tOgCzBcoPpVMZk0iLqyZQumE9wa9/g/mYFdpH4xEeTpkn+BVW4Ln7INYjO6tEfgyvrl3xu2Mo5p5xCKAsVpTNivXwYXzXrMWSlYXPmLH49OmNqU0bKtMOcPT556m4/h6Cr78ery6dyXr874iXF0ce/Rs+ffpwbONGWj4yDYDCRYsIHD+ekL/cgFfHjvXy/UtTe2yyT58+auPGjXW6z61rf6LnkqvYccEbxF58Y53uW9POd5W2SjwMHhjEQJm1jP2F+7Harfh7+mOxW8hPS2F38V5WV+ygpLKEMJ8wBCG7LJtSSylKKTJKMqiwVdDK1ILoIk+koJgsQzGVHuBTAQn7FFeuBU+LncpgX/ZeGIM1N5fAzCLCj1rwLK3kWMfW5HduRcjuI3jvzfwjQIMB77g4xMcbW04OFal7wNcHg9EDe1ERiGAMDsaWlwcmE1gseHbsgHd8PBV79mDLzsFeWoohKBDvuJ54xkRjDAzCFNkan8REjEFBZ/ydWXNyOPrKqxQuXAhK4Z2QQNQ7b3P4mWco/n4JwZMnE/7kE3X3jwSIyCalVJ9q1+lEAAXFpXi9FE1q1NX0vP3dOt23pjU1hRWF/Jb5G5uObMLX5EugVyAp+Skk5yRTUFFAmbUMoxICLR7kmiqptFfiaYW/LoSMIDuzh8mJs/PeqXYe+MqO1Qjzr4uiMiaC+MWp2AQ2XN0Vb98gzKUWEraWELe1GNmeAlZrtXH5j7qEgLFjyfv4Y8o2bsIQGIhXx454deyIMSSYY7+toWzbNrx79sRv+HA8o6Mx+PlStmkTpevWg92OwccHv+HDCbryCsTHB3thIeLjg8HTk7LtyRQuWoRndDTB116DmEz1/l2X79pF8Q8/EHLzzRj9/VF2O2VJSXjHxyNGY53WpROBCzY9M5gwUwVtH6v7fWtafbErOzZlw2Rw/GgdKDrAkdIjxLaIxc/Tj6KKIoqLsmkV0g4PoweFFYVklGTgiYmizRs4OmsmQbuySL2gLSXjhlC57Gei1x5E7Aqrp5Fcf8XhQMWWfqG06dSLzllCnw/X4ZtZgMGuODS4I/vuvYzomctps3QbAJZ2EVRcPADb4cMELF6LvXM0XnhgTdmDeHqi7HawWDD36IHfsGHkffQR9tJSPDt2wH/YMLy6dMEjLAxbURGqogKDrx+myEjMXToDoJTCXlKCwc8PkZNvuCqbrc5/QM8XtSUCfY/AKSsggYSCz6CiGLz83R2Opv1JWmEaPx74kaPHjhJiDuFQySF+PfQrRZVFdArqhMVuYU/BHgBa5wujtnuQuL2M8ALINkGRn5FcHxs2I8QccTS5tPSCvOgQ+i7eD4v3A1DcLhTfiNb42IxYsw5jTT7CNRuKCBjjQ9F33+ERGkrAnddiLyyCzz6jY/kqyjZtI+TGv+B74RAyH3sU0wcLwWgk4NJLiZjxHABHnn8eVV5B6L33ULF7N5mP/JWc7dvxGzGCsPvuxdy1q0vfg4hg9K/+/1GdBM5OvSYCERkNvA4Ygf8ppf59yvq2wMdAkHObR5VSi+szpppYIvtjLPiUsv3r8O56sTtC0JoJq91KQUUBngePUDJzNoQGkzMsjl8Mqfyc/jMBngEMiRqCQQwk5ySTUZLBkWNHyC45wsVJCqPZzJzYSvw9/bkg8gLCfcMpXr+OdsnFRLUcTGh6MZ6rNqPESlGPdhy+rAPH8rOR3AIiSgVvm4GKi1uTF9ue+Al34hcYStmWLeQv/o7AERfj07fvSWfalqwsjr74EoULF+I7aBCtX34Jj+BgADxatSL71Vfx6tqVsIcfxuDpSafly1FWK2I2n7SfiKefPvHeMyqKmK8WYSsoxDuuR4N991r16q1pSESMQAowEsgANgDXKaV2VNnmPeB3pdQ7IhILLFZKRde23/pqGvo5KZVhi/pwKPFhIsc/Wef715oHq92KXdmxKzsKRVFFEUvSlvBj2g8cK8zFmF9Mi4OFxO23M3SbwuIBnlYwKEiJFNIGRVPgUYnxQCbHzEJe+1BahEYRecxM3+/T8Et13AQNvu1WQqc+iNFgJO/Djzj68suOAOx2DP7+BF93HSE3XI9HWFidHZvl0CE8wsP/dNZdsnIlXl27YmrZss7q0uqeu5qG+gF7lFL7nEHMAcYDO6pso4AA5/tAIBM36RITxV57BJ4Z+h6BVjulFBW2CkosJRSXF3HMVkZ6cTrf7vuW1YdWE5xvYcAuRcwRRVSOolMp9C4Do/2Pfdg9PTg8sjubLu9IiDGA7pvz6LZiK52/cDTPYDCA3Q7LjwJHATAGBdHqhec5tmkz+f/7gNLlK7CXlWHNysJ/1Cha/3MGmEyISL3c6DRFRla73G/IkDqvS2tY9ZkIIoH0Kp8z+HNXiKeBH0TkPsAXqLZNRkTuAO4AaNu2bZ0HChAeYOYbYxeG520BpeCUm1Da+U8pRZm1DANCcVkhu4v3sCtvFztzd3Kw+CDFlcWUlZcwaE0hF2+2ElIMomDeEAPf9hUuzArkjQ1BhO48DEB5WADlbcIIadOR4JZtMQYHYwwOxqtzJ8ydO9PdZGLE8covAvWwoiIlFQS8oqOxFRdTtnUrqqISjxYheHXpgjEggIDLL8ezXTtK167BIzgY78REgq699k83TjXNVfXZNDQRGKWUus35+Qagn1LqvirbPOSM4WURGQh8APRQStmr3Sn11zQEMPP1v3NT/pvwwBYIjq6XOjT3O95DdU/BHrJKsiiqLGJ/4X72pK6j75o8LkxWBJbCmm5CcluhZ44vbYpN2P19CMkqJehgPoWxUVR2jMInqxDfDTtRIYFIXiEeEREEXzORwCuuwBQR4e5D1bQT3NU0lAG0qfI5ij83/dwKjAZQSq0RETMQyvFr4QZmj+wN+WA9uAEPnQiatOLKYtZnrSe9OJ2CioITr0Mlh9iXv5fwI5UM2mnHuwKSOhjoWuzPjctL8KiEgp7RFIcGMvS33QzbVo742PCMjsJ+qBjxCCTstSfpOmoUIoJSiqKvv6bgy4UEjB5F0NVXI56e7j58TTsj9ZkINgCdRCQGOARMAiafss1BYAQwU0S6AWYgux5jqlVYh16UbzNRmrqGFvET3RWG5qKcshx+2b2E1N++x2pQeLYKx2fvYcI276eooojMYMgKgewQDzoVmLko2Up4tg1zuR1TuQ1lNGAwmbh0UwVQgN/QobT6++N4tnGcv9hKSrFmZeIZE4N4VP+/iogQOH48gePHN+CRa1rdqrdEoJSyisi9wFIcj4Z+qJRKFpFngI1Kqa+Bh4H3RWQqjhvHNyk39nCLjWrBVtWe9of0DePGxK7sbNr7K3sXfILFWkGFp2BP2UfEnjy6HIbYU/5ijvmZEG9fBu8oQZQCKoFKTG3a4DO8D8YAfzyjo/G/5BIMfn4cW78e8fDAZ8CAk9rZjX6+GDt1atBj1TR3qNd+BM4+AYtPWfZklfc7gMH1GcOZiGnhy0rpRK+CH8BaCR76Er++2ew2MkszCfIKwmQwsfnIZrbu+43wb9YT9eseckM8OBBkJTG5jPjyP8pZPYTSTpEYRw0iavBIRAxYD2fh2a4d3r16IUYj9ooKLAcPUpGWhiksDHN8fLU3VP0u1PNQaM2b7llchcEg5Ab1xKPwWziyDSJ7uzuk845Sin2F+1ibtZZ1WevYuX89HXYXE3tQ0TFL4VMBA0rAywo7OpkJLrcxKKmc8n49CL//r/hFtsNeUoopsjUGL69a6zJ4eeHVqRNe+qxe02qlE8GpovpCIdgPrsOgE8FZs9gslFpKCTIHYbVb+TXjV5YeWMr6rPXklB6lX4pi7FZP7txfgcGusPp4UdCxJX4tWxPaMprQidfSrVs3wJE8TjqT1/2WNK1O6URwijbRHcnYHkrQ3t/wG3i3u8NpcpamLWXu7rlsy95Gua2cKHsQkUetJIUUE2QO4fr94fReUYFnVh6myBYE3DoG/4tHYO7Ro8ZxYvTz8ZpWv3QiOEWPyEA22zsxMmO97ljmIovdwuGSw7y7+S1KF33D4AJ/hvXuTVilFxGf/YxXaSXKaMDoU469eBvmnj1pMf1J/EderAcJ07RGQCeCU3Rp5c9C6cq48jVQmA5B9dOTuSmy2Cz8/t9/UbBzG/vae1ORc5Qeqw4RUGglpbUwvFARmQviVY5a+wsAPn37EjxlCuXJyVizswmacDXevXvrs3xNa0R0IjiFh9FASavekP0RpK9v1omgIj2dI2++Tp6U8cvFYah53zLq11LMRmjzo2Ob3HZBFMZG0H1PLl4hfkQ98xB+Q4dStm0bqrz8xCOZAaNHufdgNE2rkU4E1WjRPpHSo154pa3BI26Cu8NpUMWVxSxO/YYj77/L0GXZ2AU8bHDRN+Bhh2NjLyTu2dewpaQiJhPdYmOr3Y9PYmIDR65p2tnSiaAaie3C+P23jiTu/+28/4IsdgvLl32A17/eo8ijkp9jFcO32hmZBVn9Ysi4ZSQx3lG0+2It5sg2dH3wAUezTny8u0PXNK2OnO+/c2cloW0Qs1UXBuUtOu9mLLMfO4YlM5PiQwdYdeAXft/2I1csKaDcx4MIb19uX1qECvSn9WvP0G306D8K9tdDbmja+UongmqE+nmR7huHoeJLx32CjiNOX6iRU0qx+pMX8H/pYzwtjjEZujhflm4x9P7vR5jCWlKxcyceEREnZqDSNO38pxNBDQzRgyjf7YlXyhKkiSaCY5Zj/JC2lKzdv2Na/CuDlx/mQDtvjl7aG2OrliRE9qV9SCfMXTqfmMjEXEObv6Zp5y+dCGoQFx3Orzt6MHznYjwufaFJ9ScoKC/gy5Xvkj9vLgN/L6dLqWN53ogELnr5AzzNPu4NUNO0RkUnghoktg1mlr03I4vfhyPJEN64J9hWSpGSn8KiPYvImfs5Ny6uQAA1qDctR47Fr08funXs6O4wNU1rhHQiqEG3iADWevTFzv8w7F7caBOBUoofdn3D99+9TmbZEXqmwS2rbcjA3nT454t6lixN005LJ4IaGA1CdLsYdh3qTOzuxTD0r+4O6YQyaxmrDq0id/8uTB/OJybpKHdW/rE+cPx4Ip57tl4mMNc07fyjE0Et+sWE8M3eRGIz50BRJgS0dndI7M7bzV9X/hXjjr08ssCGuRJKhibQccKdGM1mxGTCOzERMRjcHaqmaU2ETgS16BcTwmP2XkxnDuz+Hvre6rZYSipL+HTN2+z/8lMmHDLSe7fg0SqStu++i1m3/Wuadg50IqhFz6hADhrbkOcVSYibEkFOWQ6L1n9M3mezGbGmjKGVYGgZhP/YC2j510f08/6app0znQhq4eVhJKFNMCvz+3HF/u8arJexsljIXr2CVUs+RDZtZWC6wgCo4QOImfoYXp066dE7tfOKxWIhIyOD8vLy02+s1cpsNhMVFYXpDO4RupQIRGQj8BHwmVIq/yzja5L6x4Qw92APrjAthL3LIXZ8vdZnr6hg800T8f09lS5AYdtgvG6/jDbjrtFTLmrnrYyMDPz9/YmOjtYnOedAKUVubi4ZGRnExMS4XM7VO4qTgNbABhGZIyKjpJn8a/WNDmG9rTMWzyDHfYJ6tDN3J/Nvuxjf31NZckUU8sMsBv3wGx0fflwnAe28Vl5eTosWLXQSOEciQosWLc74ysqlRKCU2qOUehzoDHwGfAgcFJF/iEjIGUfbhPSJDsZoNLHTfwCkLAGbtc7rOFB0gIeWT+W7B68mbkMORydfxAP/Wkps2z51XpemNVY6CdSNs/keXX7GUER6Ai8DLwILgAlAEbD8jGttQnw8PejfPoSFx+KhLB/S19bZvu3Kzuyds7lm4VX0eOcnLtug8J18DUOe+A8G0Y9/aprWMFy9R7AJKAA+AB5VSlU4V60TkcH1FVxjMbRzGK+kduEJf18Mm2dB9AXnvM/88nz+sWQafkvW8mqyF8HZFsKmTqXFHbfrMyNN0xqUq6edE5VSI5RSn1VJAgAopa6qh7galWFdwjiGmdTW4yD5Syg5ek77256znds/n8D459cw5Wc7rdvGEvXWfwi98w6dBDTNDQoKCnj77bfPuNyYMWMoKCg443I33XQT8+fPP+Ny9cXVx0dvE5EXlFIFACISDDyslPp7/YXWeHQI8yMyyJtZtlE8Z/scNs086yEnFu9bzPM//p2nZltpVe5Fu88+wKdXr7oNWNOasH98k8yOzKI63Wds6wCeurx7jeuPJ4K77777pOU2mw2j0VhjucWLF9dZjO7k6hXBpceTAIDzEdIx9RNS4yMiDOsSxsKD3tjbXwQbPwSb5Yz2YVd2Ppn/JAemT+PV/1qIKBDavfOuTgKa1gg8+uij7N27l4SEBPr27cvw4cOZPHkycXFxAFxxxRX07t2b7t278957750oFx0dTU5ODmlpaXTr1o3bb7+d7t27c8kll1BWVuZS3T/99BOJiYnExcVxyy23UFFRcSKm2NhYevbsybRp0wD44osv6NGjB/Hx8QwZMqTuvgCl1GlfwFbAq8pnbyDZlbJ1/erdu7dyhx+SD6t2079VySvmKfVUgFLbFrhc9pjlmHr+fzerpO5dVVJCnM/Ed/8AACAASURBVEqfNk2Vbtpcj9FqWtOyY8cOt9a/f/9+1b17d6WUUitWrFA+Pj5q3759J9bn5uYqpZQ6duyY6t69u8rJyVFKKdWuXTuVnZ2t9u/fr4xGo/r999+VUkpNnDhRzZo1q8b6brzxRvXFF1+osrIyFRUVpXbv3q2UUuqGG25Qr776qsrNzVWdO3dWdrtdKaVUfn6+UkqpHj16qIyMjJOWVae67xPYqGr4XXX1iuBT4CcRuVVEbgF+BD6uu3TU+A3u2AKzycC8gi4Q2AZ+/9SlcsWVxTz6yRSGv7kGW1gwsctWEPXii/j0SqzniDVNO1v9+vU7qUPWG2+8QXx8PAMGDCA9PZ3U1NQ/lYmJiSEhIQGA3r17k5aWdtp6du/eTUxMDJ07dwbgxhtvZOXKlQQEBGA2m7ntttv48ssv8fFxTCY1ePBgbrrpJt5//31sNlsdHKmDq/0IXgBmAN2A7sCzzmXNho+nB8M6t+S75KPYe05y9DIuPFRrmeLyIv777ARueH0n3j4B9Jj1BaYWLRooYk3Tzpavr++J9z///DPLli1jzZo1bNmyhcTExGo7bHl5eZ14bzQasVpP3+fIcaL+Zx4eHqxfv56rr76aRYsWMXr0aADeffddnnvuOdLT00lISCA3N/dMD61aLj+srpT6Xik1TSn1sFJqaZ3U3sRc1jOC7OIKtoaOARRsnVPjtsXlRSyfPJLLvjiIR9dOdJ7zBZ5RkQ0XrKZpLvP396e4uLjadYWFhQQHB+Pj48OuXbtYu7bu+hJ17dqVtLQ09uzZA8CsWbMYOnQoJSUlFBYWMmbMGF577TWSkpIA2Lt3L/379+eZZ54hNDSU9PT0OonD1X4EA4A3cVwReAJGoFQpFVAnUTQRF3VtiZeHgYVpniS0HQRJn8EFD/1pPuNSSyn/e3YCo3cUUXTrePpN+5d+LFTTGrEWLVowePBgevTogbe3N61atTqxbvTo0bz77rv07NmTLl26MGDAgDqr12w289FHHzFx4kSsVit9+/blrrvuIi8vj/Hjx1NeXo5SildffRWARx55hNTUVJRSjBgxgvj4+DqJQ2q6NDlpI8egc5OAL4A+wF+Ajsox7ESD6tOnj9q4cWNDV3vCXbM2selgPusuzcLw9b1wyw/Qtv+J9aWWUqbPvZlbn98GPbuS8OmXOglo2mns3LmTbt26uTuM80Z136eIbFJKVTtuzZk0De0BjEopm1LqI2D46cqIyGgR2S0ie0Tk0WrWvyoiSc5Xioicec+MBjbG2Ty0yXcImHwgafaJdaWWUu5bfAcXfbwdTw8verzwH50ENE1r9FztUHZMRDyBJBF5AcgCfGsrICJG4C1gJJCBY+TSr5VSO45vo5SaWmX7+4BG/yjNiK4tMZsMLNpRRN/Y8ZC8EEb/G4vRxMPf3MXlb/9Ol0MQ+fwzmCL1PQFNa87uueceVq9efdKyBx54gJtvvtlNEVXP1URwA46rh3uBqUAb4OrTlOkH7FFK7QMQkTnAeGBHDdtfBzzlYjxu4+vlwSWx4Xy7NYunp0zGtOVz2PUtb+WlMP61jbTLMxL16ssEjB7l7lA1TXOzt956y90huOS0TUPOM/sZSqlypVSRUuofSqmHnE1FtYkEqt7SznAuq66OdkAMNYxkKiJ3iMhGEdmYnZ19upDr3ZW9Iikss7CivCMEteWnDf+lzYzZtM010Pa//9VJQNO0JuW0iUApZQPCnE1DZ6K6xvGa7kxPAuY766ouhveUUn2UUn3CwsLOMIy6d2HHUFr4erJoSxYHYy8n/ZtMYtMh/F//xG/weT8Yq6Zp5xlXm4bSgNUi8jVQenyhUuqVWspk4GhCOi4KyKxh20nAPS7G4nYeRgOXx7fmsw37CN23jut2gefY7rQYV7/TWGqaptUHV58aygS+dW7vX+VVmw1AJxGJcV5NTAK+PnUjEekCBANrXA26MbgyMRIv/0Vc/E0mlpYm2rfcdsYD0Wma1jT5+fnVuC4tLY0ePXo0YDTnzqUrAqXUP850x0opq4jcCyzF0QHtQ6VUsog8g2Pwo+NJ4TpgjnKlQ0MjUmlK5dpdawgrgraP3YVseQx2fAVxE9wdmqZp2hlxtWfxCqpp31dKXVRbOaXUYmDxKcuePOXz067E0JjY7Db+u+RZ7l+nWN4mgUG9riUx/X1Y/55OBJp2rr5/FA5vq9t9hsfBpf+ucfX06dNp167difkInn76aUSElStXkp+fj8Vi4bnnnmP8+DNr/i0vL+f//u//2LhxIx4eHrzyyisMHz6c5ORkbr75ZiorK7Hb7SxYsIDWrVtzzTXXkJGRgc1m44knnuDaa689p8N2lav3CKZVeW/G8eho3c/i3kQsSF1Av2/2YDSamBM/nr3r0knsdwcseRQObYZIPceApjUlkyZN4sEHHzyRCObNm8eSJUuYOnUqAQEB5OTkMGDAAMaNG3dGnUSPPz66bds2du3axSWXXEJKSgrvvvsuDzzwAFOmTKGyshKbzcbixYtp3bo13333HeAY46ihuNo0tOmURatF5Jd6iKfRK6wo5KvvXuXvyYoWd97MsJgezN2YzhMXTyR4+XPw2xswcaa7w9S0pquWM/f6kpiYyNGjR8nMzCQ7O5vg4GAiIiKYOnUqK1euxGAwcOjQIY4cOUJ4eLjL+121ahX33Xcf4Bhgrl27dqSkpDBw4EBmzJhBRkYGV111FZ06dSIuLo5p06Yxffp0xo4dy4UXXlhfh/snLt0sFpGQKq9QERkFuP5tnEfeSXqbK5YWQlAALW67jRsGtqPSamfetkLof5ejp3FdX9ZqmlbvJkyYwPz585k7dy6TJk1i9uzZZGdns2nTJpKSkmjVqlW1w0/XpqZbn5MnT+brr7/G29ubUaNGsXz5cjp37symTZuIi4vjscce45lnnqmLw3KJq08NbQI2Ov+7BngYuLW+gmqs9uTvwfr+bHocULS6/wGM/v50buVPr7ZBfLn5EGrQvWAOhOUz3B2qpmlnaNKkScyZM4f58+czYcIECgsLadmyJSaTiRUrVnDgwIEz3ueQIUOYPdsxHllKSgoHDx6kS5cu7Nu3j/bt23P//fczbtw4tm7dSmZmJj4+Plx//fVMmzaNzZs31/Uh1sjVpqGY0291flNKsfi1qVy9yob3FWMJvu66E+uu7BXFE4u2syPfQPdB98PyZyF9A7Tp68aINU07E927d6e4uJjIyEgiIiKYMmUKl19+OX369CEhIYGuXbue8T7vvvtu7rrrLuLi4vDw8GDmzJl4eXkxd+5cPv30U0wmE+Hh4Tz55JNs2LCBRx55BIPBgMlk4p133qmHo6yeq8NQ3wPMVs4J7EUkGLhOKfV2Pcf3J+4ahnr12vkE3vwEJYkd6P/xQsRkOrEuv7SSfv9cxo0Do/n7yLbwejxE9YHJcxs8Tk1rivQw1HWrvoahvv14EgBQSuUDt591lE2MXdnZ/b/XUAaIf+X9k5IAQLCvJ8O6tOSrLZlYPXyg762QshTy9rkpYk3TNNe5mggMUuWZKedAdGc69lCT9cPOr4nfkEvZhYl4h0dUu81ViZFkF1fw295c6HMLGIyw/n8NHKmmaQ1l27ZtJCQknPTq37//6Qs2Qq72I1gKzBORd3F0LLsLWFJvUTUiFruFjTNf5uoKaHvnIzVud1G3lgT5mPhg1X6G3NIPul8Jv8+C4X8Dr5q7o2ua1jTFxcWdmEu4qXP1imA68BPwfzgGh/sJ+Gt9BdWY/HRgGf1+y6GyU1t8EhJq3M7Lw8jdwzrwS0o2v+3JcTxKWlHkmNdY0zStEXM1EXgD7yulJiilrgb+B3jVX1iNx5pv3qdNDrS5+c7T9ij8y8BoIoO8+df3u7C37g1tBsDKF6G84XoIapqmnSlXE8FPOJLBcd7AsroPp3HZk7+HmGW7sAR4E3TZZafd3mwyMm1UZ7YdKuSbrZkw+l9Qmg0/N3xPSU3TNFe5mgjMSqmS4x+c733qJ6TG4+vVH9A3VRE0YSIGL9cugMbHRxIbEcCLS3dT0Soe+twM6/4Lh7fXc7Sapmlnx9VEUCoiJ0ZSE5HeQFn9hNQ4lFpKsS/8HoCI6290uZzBIDw2pisZ+WXMWnMALnrC0dt47hQ4klxf4Wqadg4KCgp4++0z7xY1ZswYCgoKTr9hI+dqIngQ+EJEfhWRX4G5wH31F5b7rUhdyoWbK1AX9sXUuvUZlb2wUxgXdgrlzeV7KMQfJs8DSzn8b6RjzgJN0xqVmhKBzVbt7LknLF68mKCgoPoKq8G4OsTEBhHpCnTBMRfxrnqNqhE49NlMOpRB21vvPavyj17albFvruLtX/bw2KV94Y6fYd4NMO8vcOE0GP44GFzNw5rWfDy//nl25dXtT0zXkK5M7ze9xvWPPvooe/fuJSEhAZPJhJ+fHxERESQlJbFjxw6uuOIK0tPTKS8v54EHHuCOO+4AIDo6mo0bN1JSUsKll17KBRdcwG+//UZkZCRfffUV3t7e1db3/vvv895771FZWUnHjh2ZNWsWPj4+HDlyhLvuuot9+xydUd955x0GDRrEJ598wksvvYSI0LNnT2bNmlWn34/Lv0RKKQuQDIQB7+CYk/i8lJ+fRfziVHK6t8a3f7+z2kf31oFcmRjJR6vTOFRQBgERcNN3kHgD/PoSfNVkpmjWtPPev//9bzp06EBSUhIvvvgi69evZ8aMGezYsQOADz/8kE2bNrFx40beeOMNcnNz/7SP1NRU7rnnHpKTkwkKCmLBggU11nfVVVexYcMGtmzZQrdu3fjggw8AuP/++xk6dChbtmxh8+bNdO/eneTkZGbMmMHy5cvZsmULr7/+ep0fv6szlPUHJgNXAiE4+hLU3Luqidv21r8IOwb2+8/uauC4hy/pwrdbs3jlhxReviYePLxg3JvgGwqrXoXEKRB9QR1FrWnnh9rO3BtKv379iIn5Y6zNN954g4ULFwKQnp5OamoqLVq0OKlMTEwMCc6+Rr179yYtLa3G/W/fvp2///3vFBQUUFJSwqhRowBYvnw5n3zyCQBGo5HAwEA++eQTJkyYQGhoKAAhISF1dpzH1XpFICIzRCQV+CewDUgEspVSHzvHGzrv2AoLCViwnG3dvIkdcsU57SsyyJubB0Xz5e8Z7MgsciwUgaHTISAKljwG9trbIDVNa3i+vr4n3v/8888sW7aMNWvWsGXLFhITE6udl8CrypOFRqMRq7XmSRxvuukm/vOf/7Bt2zaeeuqpWuc5UEqd0axoZ+N0TUN3AEdwNAV9qpTKpZq5i88nh5d8jVeZjZLrRtfJl3/3sI4EmE088dV2LDa7Y6HJG0b+Aw5v1T2PNa0R8Pf3p7i4uNp1hYWFBAcH4+Pjw65du1i7du0511dcXExERAQWi+XEfAUAI0aMODH8tM1mo6ioiBEjRjBv3rwTzVF5eXnnXP+pTpcIwoEZwDhgj4jMArxFxNUxipqcgz9+TZ4fDBxxQ53sL9DHxDPju7PpQD4vLd39x4oeV0Ob/vDD45C7t07q0jTt7LRo0YLBgwfTo0cPHnnk5Fbv0aNHY7Va6dmzJ0888QQDBgw45/qeffZZ+vfvz8iRI0+a5+D1119nxYoVxMXF0bt3b5KTk+nevTuPP/44Q4cOJT4+noceeuic6z+VS/MRAIiIGRgLXAdcAPyklJpc5xGdRn3OR6CsVn7vl8j2bt7c8Om6Or0ce3zhNmavO8j7f+nDyNhWjoX5afDecPANg9uWgTmgzurTtKZEz0dQt+p0PgIRGXh8+GmlVLlSar5zrKFOOEYkPa8c3rgK72NWzIMH1nmb3BNjY4mNCOCJRdsptzjvCwRHwzUfQ+4e/RSRpmluc7qmoRuBTSIyR0RuEpFwAKVUkVLq4/oPr2HtWvwZdoHEMTfV+b7NJiNPXR7L4aJyZv6W9seKmCGOoap3fg0HfqvzejVNc5977rnnT3MWfPTRR+4O609qbetXSt0F4OxMdikwU0QCgRU45iNYrZQ6bx57sa/dTHpbb0ZHJ9bL/vu3b8HwLmG8vWIP1/VtS6CPc6azAXfD+vfhp2fg5u8dTxZpmtbkvfXWW+4OwSUudShTSu1SSr2qlBoNXASsAiYC6+ozuIaUlbGb8IOl2PvH12s9fx3dleIKK28sT/1joacPDP0rHFwDe877QV01TWtkXEoEItJBRI4/JNsf6Ag8UdONh6Zo61cfYgA6XzqpXuvpFhHAdf3a8sGq/Xy27uAfKxJvcNwz+OEJqCyt1xg0TdOqcnWIiQWATUQ6Ah8AMcD59QD80pXkhHjQvv/Ieq/qH+O6c1HXljy+aBtfb8l0LPTwhDEvQc5uWHC77mimaVqDcTUR2JVSVhxDTLymlJoKVD+LexNUlHmAqJQC8ob0wNAAA8GZjAbentKLvtEhTPtiC9synDOYdRoJo/8Nu7+DL++A5IVQkF7v8Wia1ry5+qtnEZHrcDxF9K1zmal+Qmp4yXP/iwGIvLJ+m4WqMpuMvHt9b0J9Pfm/2ZsoOFbpWNH/TrhgKmxfAF/cBG/2gu1fNlhcmqadnp+fn7tDqFOuJoKbgYHADKXUfhGJAT6tv7Aalm3pCg5EGInvO6ZB6w3x9eStKb04UlTO1LlJ2O3Ozn0XPw1/y4Q7foHI3jD/Flj7boPGpmla8+HqfAQ7gPsBRCQY8FdKnRcT8Zbt20uLtAL2TuyOydDwFzmJbYN5cmwsT3yVzFsr9nDfiE6OFZ4+0DoBblgIC26DJdMd9xH63NLgMWpaQzr8z39SsbNu5yPw6taV8L/9rcb106dPp127dtx9990APP3004gIK1euJD8/H4vFwnPPPcf48eNPW1dJSQnjx4+vtlx18wrUNAdBQ3J1GOqfcYw35AEkAdki8otSqu4HvWhge+Z9hAGIGHe122K4fkA7Nh3I55VlKSS0DeLCTmF/rDR5w8SZMGcyfPcwmIOgyxgwmd0Wr6adbyZNmsSDDz54IhHMmzePJUuWMHXqVAICAsjJyWHAgAGMGzfutKMOmM1mFi5c+KdyO3bsYMaMGaxevZrQ0NATg8cdn4Ng4cKF2Gw2SkpKat1/fXB18LhApVSRiNwGfKSUekpEtp6ukIiMBl4HjMD/qruKEJFrgKdxjGq6pSHHL1JKUf79D6RFGxgdP7ahqv0TEeGfV8WxM6uY+z//nW/vv5DIoCozGxlNjmTw8TiYf7NjWUAkTJ4L4XFuiVnT6kttZ+71JTExkaNHj5KZmUl2djbBwcFEREQwdepUVq5cicFg4NChQxw5coTw8PBa96WU4m9/+9ufyi1fvrzaeQWqm4Ogobl6j8BDRCKAa/jjZnGtRMQIvIWjR3IscJ2IxJ6yTSfgMWCwUqo7jrmRG8yxbVvxO1JM9uAu+Hv6N2TVf+Lj6cE71/fCYlPcPXszFdZTHh/19IUbvoTLX4eLngCl4NOrIW+/ewLWtPPMhAkTmD9/PnPnzmXSpEnMnj2b7OxsNm3aRFJSEq1atap13oDjairXEPMKnC1XE8EzOAaZ2+ucv7g9kHqaMv2APUqpfUqpSmAOcGoD2+3AW8cnuVFKHXU99HO3b95MLEaIHn9dQ1Zbo/Zhfrw0sSdb0gt4+uvkP24eH2cOhN43wZBpjnsHtkqYdaUexlrT6sCkSZOYM2cO8+fPZ8KECRQWFtKyZUtMJhMrVqzgwIEDLu2npnI1zStQ3RwEDc3VISa+UEr1VEr9n/PzPucopLWJBKo+BJ/hXFZVZ6CziKwWkbXOpqQ/EZE7RGSjiGzMzs52JeTTUjYb1h9/JqmjkaHdGvZpodqM7hHB3cM68Pn6dG6auYG80srqN2zZFabMh/ICx1DWu5c0bKCadp7p3r07xcXFREZGEhERwZQpU9i4cSN9+vRh9uzZJ80bUJuaytU0r0B1cxA0NJfmIxCRKOBNYDCOtvxVwANKqRonsBeRicAopdRtzs83AP2UUvdV2eZbwIKjySkK+BXooZQqqGm/dTUfQdGqXzl02x38cHs8Dzw855z3V5eUUny+Pp2nv04mzN+LT2/rT0yob/Ub5x+AeTdA1hbHE0UX/0PPa6A1OXo+grpVp/MRVPER8DXQGsdZ/TfOZbXJANpU+RwFZFazzVdKKYtSaj+wG8dcB/Uubf4sjnlBl7FTGqK6MyIiTO7flvn/N5Ayi42J765h9+Hqp9EjuB3cshQG3gubZsLbAyA7pUHj1TStaXM1EYQppT5SSlmdr5lA2GnKbAA6iUiMiHgCk3Akk6oWAcMBRCQUR1PRPpejP0v28nL4eQ0bu5oY0n5EfVd31npGBTHvzgEYDXDd+2vJLCirfkOTN4yaAbf+CNYKmHMdlNV4UaVpWh3Ytm3bn+Ya6N+/v7vDOiuuJoIcEbleRIzO1/VAbm0FnGMT3YvjJvNOYJ5SKllEnhGRcc7NlgK5IrIDxxwHjyilat1vXchdthRTuRXrxQPxMfnUd3XnpGNLfz67fQDlFhsPzk3CduoN5Kqi+sC1sxxTYC64VQ9cpzUprk6b21jExcWRlJR00mvdOvePzH8236OrieAWHO34h4EsYAKOYSdOF9BipVRnpVQHpdQM57InlVJfO98rpdRDSqlYpVScUqpBGuv3z/+EfF8YNPaOhqjunHUI8+PZ8T1Yvz+P/yzfU/vG7QbBmBcd8xp8eTvYLA0TpKadA7PZTG5ubpNLBo2NUorc3FzM5jPrcOrqEBMHcfQsPkFEHgReO6PaGgFbYSHeG3ayZkAgt4f3cnc4Lru6dxSr9uTw6rIUyiw2Hr6kMyZjDXm8zy1QUQw/PgnlRY4rheIsGPwghMQ0bOCa5oKoqCgyMjKoq6cCmzOz2UxUVNQZlXG1Z3F1HqIJJoK0RZ/jYVMEjB3baDt31ORfV8U5Ri39ZS9r9uUy44oe9IisoRfi4AccndC+mwZ7fgQxQtZWuPUHR09lTWtETCYTMTH6JMVdzmXw/ab1K+qUO28OB0Nh2Mjb3B3KGTObjPzrqjjemtyL9LxjXP6fVfxt4bY/90I+ru9tMH0/PH4EJnwAmZvh15cbNmhN0xq9c7kiaHKNeeU7d+K/9wirx7dmlF/t44U0Zpf1jOCCjqG89lMKH61Oo8Ji56WJPau/wvEOdvy3+5Ww+3v45QVQdug0Cjy8oKIIWvf6YxA7mxUMRmhiV0uapp29WhOBiBRT/Q++AN7VLG/UjsyZTaURvMde6u5Qzlmgj4mnLu9OoLeJ15al0qGlL3cP61h7oUtfcNwr+OUF+OX5P5a3HeQYxyj/gGPIisheMOFDR6LQNO28V2siUEq5dyS2OmQvK6Pk2+9Y11UY0OVid4dTZx4Y0Yl92aW8sGQ3uSWVPDKqC2aTsfqNvYPgxm+gNAf2r3Sc9RcfgSWPwuyJcHQn2K2w61uYe4PjUVSdDDTtvHcuTUNNStGSpRhKy1nTN4BbQ8+foZtFhBcm9CTQ28QHq/azMiWbV69NqPkmMoBvKPS46o/PHp7w7VTwb+3opZy20vF5wa0w8RNogHmcNU1zn2bzf7gxLJR18WbCBgzBaKjhjLmJMpuMPHtFDz6+pR9F5RaueGs1b/6UWnvns6r63AI3LILbf4LQjo7PlzwHO7+BX86Lieg0TatFs0kEGd1a8PIYKxe0udDdodSboZ3DWPrgEMbERfDyjyncPHMDhcdc7FDWYTgEtP7j88B7IWGK417CsqchdRlUNPzMSZqm1b9mkwhWHVoFwKDWDTsXaEML8vHkjesS+fdVcazZm8P4t1ax/VDhme9IBMa+Ch1HwqpXYfbV8PZAOLy97oPWNM2tXBqGujE522Goc8pySDqaxMXtzp8bxaezMS2Pu2dvJre0knuGdeC+EZ1q7o1cm7ICSF8H3zzg6Kl88dMQOw78m+4juJrW3NQ2DHWzSQTNVeExC//4NpkvNx9iXHxrXrs2AYPhLPsIFB92PE2Usd7xOagt+IRCy26QeAO0HaD7H2haI1VbImg2Tw01V4E+Jl65JoEOYX68uHQ3rQK8ePyy2NMXrI5/uGOIiiPbIWUpZO+GY7mw42tImg3hcY6bzO2HgbUS7BbHMBeapjVqOhE0E3cP60B2cQXv/7qfuRvSaRVgZkr/tvxlYPSZXSGIOH7ww6s8gltZCtvmw68vwSf/3959h8dZ3Yke/x6NRtKoW73asiW5d7CxMd0YlwCmY8IGQkjALFkITwrhsjd7s8kuEJJQNpSYlqWaUEIxxTjGFGPcca+yeu9dGo1mzv3jvEKSkVwlzdjz+zyPHs28886rn44085vTF5uaQmOp2WP5ts8hOr3/6wkhvE6ahvyI26N5dWMhORVN7ClrZFN+HXOyYnno6smkDRuAfRlc7bBxGRRvgphRsOk5SBwPN6+AdY+ZJS4u/x9InHDyP0sIcVykj0B8h9aa5ZuK+N2KPWgN98zL5kdzRhJ4Ip3J/dn5ppmUFpUODUVgDwUVANe8AKMvGbifI4Q4qoHYs1icZpRS3DBzOJ/ccx5zsmL57w/3cdPzG2lsH8CNbCZdA2fcAs2VcOmj8NPNpqbw6nXw4S/NnglCCK+TGoFAa82bW4q57+2dZCWE8/S/nEFG3AB18mpt3vBDIs39jhZY/Z+w4a9mAtusO8yII0f0wPw8IUSfpGlIHJO1B6tZ+vIWWjs6uXhcIjfMHM6crDiCAgeh4li4wcxYLlwHQeFwwX1w1lKwyfgFIQaDJAJxzMob2nlpfT6vbiikrtVFlMPOldNSWXp+JklRx7cP6jEp2wGf/h4OroTYLAgMgfYGmHi1WebCFgittaZJSeYoCHHCJBGI4+bsdLP2YDXvbS/lgx1lBCjFHRdk8rOLswd+i0+tYc87sPEZCI4EtJmnYB403yYvgcVPSI1BiBMkiUCclKLaVh5euZ/3tpdy9fQ0Hrx60oktVXE8qnNgx+tmLkJzOaz7Hxi9EMZ+z0xiSxgHGefIhDUhjpEkAnHStNY8vjqHbJyBGAAAGZFJREFUR/55gLjwYOIjgpmQEsnS8zPJSggf/AA2PgMf/qL3MVsQDJ9tZjI3lkDu5zBlCZz3i76uIIRfk0QgBsz720v5bH8V9a0drDtUQ3unm4vHJXLZlBTmjk0gLHgQm27qi0C7ISQaSrdCzmrzVbUX7GFmBnPVPrjuJUifCR/9ymy/GZ4AyVNg0rUQP2bw4hPCh0kiEIOiptnJc2vzeHNLMZVNTuIjgln2gzOYNnzY0AbSXGWGp2oNf1tk1kAKDIaOVsiYA80VULEbtAdGnmfmNMRmmue6OyH/S1O7yJgztHELMYQkEYhB5fZoNuTVcO9bO6hodPLgVZO4anqad4JpKIFnLoKweLjmue4aQFOF6XP44o/gdsLo+WZOQ+k3ps8hwA63fGhqEkKchiQRiCFR29LB0pe3sDGvlvkTEvnNZRNIiQoZ+FFGR9PRaoah9rXXcmMprPw/ULbdNDHFZsKYRbD6t2bF1Nu/MLWLQ5/CpmfNRjwp00zH9MSreu/iJsQpRBKBGDKdbg/PfJnHo/88gLPTQ2CAIm2Yg2vPTOf6GenEhQd7O8S+le2A5+aZJqKOZtOMFJEMI86G0m1Qe8iskzR6AVy1DIIjup/bVG6W4h51AcSP9tZvIMQRSSIQQy6/uoVVeyqoa+3gm8J6vs6twRagOGtkDAsnJXPtGWmE2G3eDrO3A5/AzjcgZiQkT4XseWCzm8dqDpk9F9Y+YmoQ171kahx734f37oK2WnNexrlmiGvWPIjL8t7vIsRhJBEIr8upbOLtrSV8sqeCnMpmkqNCuOfi0Vw1PXVgVzwdbF8/YZqWpnwf6vKg8GszImnhw1DwFWx7FWoOmnPPvsts6xlgg7Y6cAxxJ7oQPUgiED7l60M1PPjxPrYX1ZOdEM5dc7MZERtKtCOI9BjH0PcpHA+t4Z07YPtrMCwDZt4GM34CgUHd59Tlw9pHYcsLpobgaoOSzea8hX/ou+/ieH39JDQUw/m/lAQjjokkAuFztNas3F3OHz7eT251y7fHsxLCuXJaKounpgzMZjmDwe0yfQopU82n/f5sfAZW3m/6DWJGwZ53YeqNMONW8Lghfmz3qqxgkkzxZrPWkiO6e9hrxS6o2ANx2TD/v2HXW/Dp78xzwuLhe3+C8YsH93cWpzxJBMJndbo9bCmoo7G9k9L6NlbsKGVTfh0AM0fGcOW0VBZNTCYq1O7lSE+Qx22Shdbw+UPw2QPdjwXYzWik2CzTF3FgpemU7kWZPov4sZD3pRn66u6AydfDrH+FFfeYEVA/eNt0VgvRD68lAqXUAuAxwAY8q7V+8LDHfwg8DJRYh/6itX72SNeURHD6K6pt5d1tJbz9TQm5VS0E2QK4YloKt503RMtZDKaijWY1VTB9CgdXmU/+rlbTQX3GzRCTCe314IgxayoFW79zQwms+o25v+hPZgE+ZxM8O8+sx/STNRCRBM5mcDaaBfzC4733uwqf4pVEoJSyAQeAeUAxsAm4QWu9p8c5PwTO1Fr/9FivK4nAf2it2VXSyOubC3ljczHOTg+Z8WFMSY/m+jPTOWtUrLdD9A21ubDsQpM8egqww9n/Buf9EoKsZrZ9H8D25bDojxCR2H1uUwVse9n0Y/Rsruri8ZgJedmXQNhh5b7vQ9NMJnMsfNqREsFgruk7E8jRWudaQSwHFgN7jvgsISxKKSalRTEpbRI/u3g0b2wuZktBLWv2VfL21hIuHBPP5VNTGJMYSURIIB6tSYl2DP7KqL4mZhTc/J6ZyxAUZuY4BEeYRfjW/tm88Z/5I7NOU1fTVG0u/HCF6WiuOQQvXQn1Beb2FU9+92dseMqMlsqeD99/vXtviENrYPkNkDnXNE9pDW/eAp1OmHIDjFnYPQS3o8XMxbA7zP2vnzBrQS36w+CXkTiiwawRXAMs0Fr/2Lr/A+Csnp/+rRrBA0AVpvZwj9a6qI9r3QbcBjB8+PAzCgoKBiVmcWpod7n527p8nvrsEA1tvfdYjo8IZskMM3nNZzubh1L+V6ZvIu9zc3/StWbTn7/fZJqg4rKgYJ15LHMu7Pw7XP8KZF4IJVvN0NimMnj6XJM0msvhmhfMLGtXOzx1thlGqz2w9CvTx/H3m0yzlLPRTMC7YblJDMsuMD/ntjVQXwhPzjbJ6V/ehqy5Xikef+KtpqFrgfmHJYKZWut/63FOLNCstXYqpZYC12mtLzrSdaVpSHRxuT3kVbewv7yJdpcbj9as3F3Bmv2VaA2zRsVw1fQ0Fk5MIiLkFO1sHijVOdBQCKMuNJ/m966Af/6HaT6KTDHDWqOHw7NzoTbPvEG7WsEeapbicDvNG/1rS8wyHVctM/0b658wieHdn5pP/2XbTef47V/AusfN7nNXPGWu+YX1yX/m7aZGUrTBXDskCm7//MgjsMRJ81YimA38P631fOv+fQBa6wf6Od8G1Gqto450XUkE4miK61r5x1bT2ZxX3UKIPYARMWG0d7qZlh7Nf1w2gWFhQUe/kD+q3Af/uB1Sz4BR55s1l/Z/DJf+2cyYLtsOz8wFj1UTm3iNWdzvo1+b5iOAJa+acz0eeGEhVO41SWXi1aZW0XXeJb83y3i8dStc+O+mozssziSUnna+aeZtXPfi4G9E1FRhEpN9ELZl9TJvJYJATHPPXMyooE3A97XWu3uck6y1LrNuXwncq7WedaTrSiIQx0przTdF9bzzTQkVje3YAhSr9lQQExbE3XNHkxEXyqi48MHZi/l0Vl9kJs15XDDiHDOZrr4QHptqEsitn3T3IVTnwNNzzBv4nZtMp/UzF4Gn09QwAgJNLaR0a/f1Z/8U5v3OTLwr3mySibvDzNI+556Ti72xFMKT+p7UV19kmroSJ8DNK067bVG9OXx0EfAoZvjo81rr/1JK/SewWWv9nlLqAeByoBOoBe7QWu870jUlEYiTsaukgbuXf8Ohqu5JbMlRIZw1MoZLJiRx1sgYwoIDCQ4M8O0Zzr4oZ7VZzXVYRu/jBesgKBySJ5v7rjYzKa9rdFJjGZTvNM/d8FfY+FczI3vk+bD5ObMQYPRwc87PdkBQhOmLsNlNDSPkiI0Ipmay7WXY/IJJOGfeamo4PWkNL18NeV+YBHfuL2Du/x2QYjkurnbY/LwZRjzAtR+ZUCZED51uD8V1bZTUt7G/vImthXV8lVNNXWt3x3NceDA/OieDG88aQZTDz/sXhpLWsOFpM6Kooch0Ot/ykemz+Ot5Zo2n6gNmyY4u4Ulm9rY9zNQ4RpxtRjdFpZlZ2m/fBgdXQvw4M8Jq/wdw6SNmJFVjmenUzlkNK+8zw2rLtsE3r8Dc30DSJIgeYXa/6xrtNJi6tmSd/wDM/tcBvbQkAiGOotPtYVN+HXvLGnF2evg6t4YvDlQBEBQYQGq0g5tmj2DJjOE4gqRTc0g4m833rgl1f7/JLNMRFm8+sQdHQEuV2Z60Jgc626G1DhqLredFmqYnZyMseBBm/NiMbnr1eshdY/onGnoMUhwxxzQJdbbBC4tMQugp41y47DFTc2muND8veviJ/36uNlMDqdwDZy0FWzA8McP8LokTYena7ia2ASCJQIgTsLO4gc8PVNLk7GRrQR2b8usIsQcQGxZMpFVLcNgDWDAxiSumpZIQIX0Ng6qp3KyzNPVGsxZTX7SG6oPmjb4mx7xhz7oDhvfoemyrh7d+bJpehs8ye1oHBELmRd37THg8ZsZ3fYHp/6g+aJqtPC6z3EdXn0b0CDNEdsatx7cf9u534N07zd4XANNvgrGXwavXmoST/yXc9rmZqDdAJBEIMQA25dfy0c5y6lo7aGp3AYqqpna2FzcQGKC4YloqPzl3FNGhdjo6PUSG2Il0BEpfw+misRQ+utfMq8ieb/omcj+DnFWmM3vM92DxXyA0pvs5rbVmnkXK9O5P9wdXwWs3mDf5C35tJv6texwiUgANt38Jj0ww/QSX/Bfs/9Dstd3zuidAEoEQgyinspmX1xfw2sZCnJ2eXo8FBwZwy5yR3D03W5qUTlct1aaD94uHTX/FZY+aYbB5X5hj7Q2mqWfydWa01bbXzEqyP1xhkom7E1683Kw9ddG/myVB3rjFDN2NSDJNX2Hxpv9iwhUnHKYkAiGGQHWzk493laMU2G0BNLa52FnSwLvbSkkb5mDe+ERGxYcTHx5MdKidEbGhJEV6YU9nMThKtsDrP4DGku5jWRebpqONz0D1fgiOMs1Ri5/ovSBgUzlsXAZz7jbJIWc1vHwVRKXDuT83e1uUbTfLkM++84TCk0QghBetz63h4ZX72VPaSJvL3euxsCAbjqBAtNbMG5/IfYvGEeWwU9PsJNhuIzz49BrLftprrYXC9dae10mQZr3vdvU5RCQdWwew1pC/FlKnm74Md6eZiDf5etOncQIkEQjhAzweTUVTOzXNHdS3usiraeFQZTMdbg+tzk7e31FGbFgQseHB7C1rxBagmJwWxXnZ8cyfkMS45AipPYgTJolAiFPAzuIGfv/BHpSCc7Pjaetw89WharYV1aM1jE2KYOn5mVw6OfnU2udZ+ARJBEKcwqqanHyyp5y/fZXPwcpmggMDyIgNIyMulIy4MGxKsbeskdCgQO6ZN7rPzXu01rg9WhKIH5NEIMRpwOPRrNlfyYa8WnKrWsivaaGwphWP1mQlhFNS30Zbh5sLxiTQ4fbQ7nITGmTD5fawr6yJxnYX04YPY9bIGEbFh5OdGM6ElKMszyBOG5IIhDhNuT0aj9bYbQFUNzv586oDrD9UQ4TDTkhgAG0uN0opxiSGExliZ0NeLbtKG+h62V8yPpGfXzKGQ1XN7C1rZMHEJEkOpylJBEKIb7W73BTXtbJydwV/+TTnOyOZzs2O476F4xif0seWleKUJYlACNGnkvo2Pt5VzuS0KDLjw1m+qZBnv8yjoc3FdWemU97QxvrcWialRbF4agqLJiZ/u5dDi7OT0CCbjGQ6RUgiEEIcs/rWDh76eB+vbSwiPcbB2aPi2FRg+iUCAxSzM2OpanKyr7yJ9BgHV0xNpcPtYXtRPQkRIcwdl8CYpAjCgwNpau+kuK6NpMgQJqZGStLwIkkEQojj1vMTv9aa3aWNvL+9lFV7K0iJcjB9eDRbC+v56lA1gQGK8cmRFNe1UdPS0ef1RsaFcdHYBCalRjEmKYIRsaGEBsmEuaEiiUAIMWjqWjpwBNkIsdvweDQ7SxooqW+jqd1FaFAgacMcHKho4t1tpWwpqOu1HlOUw24ttxHGoolJzBufSGx48LeP1zQ7yalsRinFzJEnt+iav5NEIITwCS63h5zKZnIqmymsbaWisZ36Vhfbi+spqGkFYHhMKPERweRWNffaLGjJjHR+u3gCwYGyeN+JOFIikHqZEGLI2G0BjEuOZFxy7xFJXU1Pa3Oq2VFcT3VTBwsmJpEZH05WQjgb82p58rNDrDtUQ5TDTouzk2ZnJwAXjklgwcQkwkMCUXQnEumPOHZSIxBCnBJW7i7npa8LsNsUYcGBhAUF0tLRyZp9lbR0fHcxvwCl8GjNiNgwshLCKbW2JnV5PNhtAVw2JYVfzR9DdGiQl36joSVNQ0KI01Zbh5ttRfV4tKbD7aGguoWCWtPMpDUcqmomt6qF5KgQxiVHEhpko6rZybvbSoly2JmRMYygQBspUSFkJoTT7nKTV91CbFgQszPjSIwMprGtk/QYBxEhp+7+1dI0JIQ4bTmCbMzOjO0+cIw7Rv74nFH88ZP95Fe34ux0s3J3Ox1WR7bDbrMm2h349vyEiGCW3XQmU9Ki2FZUz67SRto6OgmyBTAqPpz0mFCiHHaiHHZsAadWs5TUCIQQArNcR3FdKyF2GwkRwdS2dLA+t5YWZydBgQH8adV+KhqdZMWHs6essd/rBAUGMD45krFJEUQ67CRGhrBwYhLJUSHkVrewp7SR1GEO0oY5QIMtQPUaKTVYpGlICCFOUm1LB3cv/4aqJic3zhrBvHGJRISYforcqhZK69tobHNRXNfGjpIGcqtaaHa6aHd5UMrUKCoanX1ee2p6NBeOSWB3aQO7Sho4IyOGBROSiAkLIkBBZkI4cSeZLCQRCCGElxTUtPDutlL2lzcxKzOWaenRlDe0U9bQRkCAor7VxfvbS9lX3kRqtINJqVFszK+l9rCJeSlRIdy7cCyLp6aeUBzSRyCEEF4yIjaMu+Zm9zo2MbX3Cq93XphFXUsH0aF2lFJ0uj3sLGmg3eXB5fZwoKKJHcUNxEcMThOSJAIhhPABXYv5AQTaApg2fNi3988bHd/XUwaMbFckhBB+ThKBEEL4OUkEQgjh5yQRCCGEn5NEIIQQfk4SgRBC+DlJBEII4eckEQghhJ875ZaYUEpVAQUn+PQ4oHoAwxkMEuPAkBgHhq/H6Ovxge/EOEJr3efMtFMuEZwMpdTm/tba8BUS48CQGAeGr8fo6/HBqRGjNA0JIYSfk0QghBB+zt8SwTJvB3AMJMaBITEODF+P0dfjg1MgRr/qIxBCCPFd/lYjEEIIcRhJBEII4ef8JhEopRYopfYrpXKUUr/2djwASql0pdQapdRepdRupdTd1vEYpdQqpdRB6/uwo11rkOO0KaW+UUqtsO6PVEptsOJ7XSkVdLRrDHJ80UqpN5VS+6yynO2DZXiP9TfepZR6TSkV4u1yVEo9r5SqVErt6nGsz3JTxuPW62eHUmq6F2N82Ppb71BK/UMpFd3jsfusGPcrpeZ7K8Yej/1CKaWVUnHWfa+U49H4RSJQStmAJ4CFwHjgBqXUeO9GBUAn8HOt9ThgFnCnFdevgdVa62xgtXXfm+4G9va4/xDwiBVfHXCrV6Lq9hjwsdZ6LDAFE6vPlKFSKhW4CzhTaz0RsAFL8H45/g1YcNix/sptIZBtfd0GPOXFGFcBE7XWk4EDwH0A1mtnCTDBes6T1mvfGzGilEoH5gGFPQ57qxyPyC8SATATyNFa52qtO4DlwGIvx4TWukxrvdW63YR5A0vFxPa/1mn/C1zhnQhBKZUGfA941rqvgIuAN61TvB1fJHAe8ByA1rpDa12PD5WhJRBwKKUCgVCgDC+Xo9b6C6D2sMP9ldti4EVtrAeilVLJ3ohRa/2J1rrTurseSOsR43KttVNrnQfkYF77Qx6j5RHgV0DPETleKcej8ZdEkAoU9bhfbB3zGUqpDGAasAFI1FqXgUkWQIL3IuNRzD+zx7ofC9T3eCF6uyxHAVXAC1bz1bNKqTB8qAy11iXAHzGfDMuABmALvlWOXforN199Df0I+Mi67TMxKqUuB0q01tsPe8hnYuzJXxKB6uOYz4ybVUqFA28BP9NaN3o7ni5KqUuBSq31lp6H+zjVm2UZCEwHntJaTwNa8H5TWi9WO/tiYCSQAoRhmggO5zP/k33wtb87Sqn7Mc2rr3Qd6uO0IY9RKRUK3A/8pq+H+zjm9b+7vySCYiC9x/00oNRLsfSilLJjksArWuu3rcMVXdVF63ull8KbA1yulMrHNKddhKkhRFtNHOD9siwGirXWG6z7b2ISg6+UIcDFQJ7Wukpr7QLeBs7Gt8qxS3/l5lOvIaXUzcClwI26ezKUr8SYiUn6263XThqwVSmVhO/E2Iu/JIJNQLY1SiMI06H0npdj6mpvfw7Yq7X+c4+H3gNutm7fDLw71LEBaK3v01qnaa0zMGX2qdb6RmANcI234wPQWpcDRUqpMdahucAefKQMLYXALKVUqPU374rRZ8qxh/7K7T3gJmvUyyygoasJaagppRYA9wKXa61bezz0HrBEKRWslBqJ6ZDdONTxaa13aq0TtNYZ1munGJhu/a/6TDn2orX2iy9gEWaEwSHgfm/HY8V0DqZauAPYZn0twrTDrwYOWt9jfCDWC4AV1u1RmBdYDvAGEOzl2KYCm61yfAcY5mtlCPwW2AfsAl4Cgr1djsBrmD4LF+bN6tb+yg3TpPGE9frZiRkB5a0YczDt7F2vmad7nH+/FeN+YKG3Yjzs8XwgzpvleLQvWWJCCCH8nL80DQkhhOiHJAIhhPBzkgiEEMLPSSIQQgg/J4lACCH8nCQC4deUUm6l1LYeXwM2K1kpldHXipRHOD9MKbXKur22x2QzIQaV/KMJf9emtZ7q7SAss4H11pIULbp7HSIhBpXUCITog1IqXyn1kFJqo/WVZR0foZRaba0lv1opNdw6nmitjb/d+jrbupRNKfWMMnsRfKKUcvTxszKVUtuAl4HvYxakm2LVULy54KDwE5IIhL9zHNY0dH2Pxxq11jOBv2DWWMK6/aI2a+G/AjxuHX8c+FxrPQWz1tFu63g28ITWegJQD1x9eABa60NWrWQLZtnkFzGzU6dqrb25RpLwEzKzWPg1pVSz1jq8j+P5wEVa61xrYcByrXWsUqoaSNZau6zjZVrrOKVUFZCmtXb2uEYGsEqbTV5QSt0L2LXWv+8nlk1a6xlKqbeAu7RZvlqIQSc1AiH6p/u53d85fXH2uO2mj345pdTTVqdyttVEtAD4QCl1z/EEK8SJkkQgRP+u7/H9a+v2OsxKrAA3Amut26uBO+DbPZ4jj/WHaK2XYhal+x1mR7APrGahR04ufCGOjYwaEv7OYX0K7/Kx1rprCGmwUmoD5gPTDdaxu4DnlVK/xOyMdot1/G5gmVLqVswn/zswK1Ieq/MxfQPnAp+f0G8ixAmSPgIh+mD1EZypta72dixCDDZpGhJCCD8nNQIhhPBzUiMQQgg/J4lACCH8nCQCIYTwc5IIhBDCz0kiEEIIP/f/AcDK/44QdfdaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_num = np.arange(0, 150)\n",
    "plt.figure()\n",
    "plt.plot(epoch_num, hist.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(epoch_num, hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(epoch_num, hist.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(epoch_num, hist.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig('Breast Cancer ANN Epoch Plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from the graph above that validation loss stabilizes around the 100 epoch mark with training loss still decreasing. Hence, to prevent overfitting, we fit the model agin over 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 298900 samples, validate on 74726 samples\n",
      "Epoch 1/100\n",
      "298900/298900 [==============================] - 25s 83us/step - loss: 0.8984 - acc: 0.5627 - val_loss: 0.8862 - val_acc: 0.5713\n",
      "Epoch 2/100\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.8736 - acc: 0.5801 - val_loss: 0.8710 - val_acc: 0.5816\n",
      "Epoch 3/100\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.8526 - acc: 0.5929 - val_loss: 0.8526 - val_acc: 0.5926\n",
      "Epoch 4/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.8277 - acc: 0.6085 - val_loss: 0.8294 - val_acc: 0.6072\n",
      "Epoch 5/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.8014 - acc: 0.6248 - val_loss: 0.8077 - val_acc: 0.6231\n",
      "Epoch 6/100\n",
      "298900/298900 [==============================] - 23s 77us/step - loss: 0.7758 - acc: 0.6409 - val_loss: 0.7837 - val_acc: 0.6374\n",
      "Epoch 7/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.7534 - acc: 0.6535 - val_loss: 0.7651 - val_acc: 0.6485\n",
      "Epoch 8/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.7326 - acc: 0.6660 - val_loss: 0.7451 - val_acc: 0.6634\n",
      "Epoch 9/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.7162 - acc: 0.6753 - val_loss: 0.7289 - val_acc: 0.6725\n",
      "Epoch 10/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.7005 - acc: 0.6855 - val_loss: 0.7117 - val_acc: 0.6839\n",
      "Epoch 11/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.6872 - acc: 0.6928 - val_loss: 0.7025 - val_acc: 0.6878\n",
      "Epoch 12/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.6739 - acc: 0.6999 - val_loss: 0.6949 - val_acc: 0.6956\n",
      "Epoch 13/100\n",
      "298900/298900 [==============================] - 23s 79us/step - loss: 0.6655 - acc: 0.7045 - val_loss: 0.6840 - val_acc: 0.7021\n",
      "Epoch 14/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.6559 - acc: 0.7107 - val_loss: 0.6826 - val_acc: 0.7034\n",
      "Epoch 15/100\n",
      "298900/298900 [==============================] - 23s 79us/step - loss: 0.6465 - acc: 0.7151 - val_loss: 0.6672 - val_acc: 0.7101\n",
      "Epoch 16/100\n",
      "298900/298900 [==============================] - 23s 79us/step - loss: 0.6371 - acc: 0.7200 - val_loss: 0.6597 - val_acc: 0.7151\n",
      "Epoch 17/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.6297 - acc: 0.7242 - val_loss: 0.6523 - val_acc: 0.7186\n",
      "Epoch 18/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.6229 - acc: 0.7279 - val_loss: 0.6482 - val_acc: 0.7227\n",
      "Epoch 19/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.6165 - acc: 0.7318 - val_loss: 0.6353 - val_acc: 0.7287\n",
      "Epoch 20/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.6096 - acc: 0.7352 - val_loss: 0.6333 - val_acc: 0.7313\n",
      "Epoch 21/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.6038 - acc: 0.7376 - val_loss: 0.6304 - val_acc: 0.7328\n",
      "Epoch 22/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.6011 - acc: 0.7395 - val_loss: 0.6288 - val_acc: 0.7350\n",
      "Epoch 23/100\n",
      "298900/298900 [==============================] - 26s 86us/step - loss: 0.5947 - acc: 0.7433 - val_loss: 0.6199 - val_acc: 0.7398\n",
      "Epoch 24/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5891 - acc: 0.7459 - val_loss: 0.6135 - val_acc: 0.7432\n",
      "Epoch 25/100\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.5844 - acc: 0.7486 - val_loss: 0.6088 - val_acc: 0.7455\n",
      "Epoch 26/100\n",
      "298900/298900 [==============================] - 24s 81us/step - loss: 0.5804 - acc: 0.7497 - val_loss: 0.6039 - val_acc: 0.7490\n",
      "Epoch 27/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5762 - acc: 0.7533 - val_loss: 0.6013 - val_acc: 0.7497\n",
      "Epoch 28/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5719 - acc: 0.7550 - val_loss: 0.6014 - val_acc: 0.7516\n",
      "Epoch 29/100\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.5673 - acc: 0.7582 - val_loss: 0.5969 - val_acc: 0.7536\n",
      "Epoch 30/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5661 - acc: 0.7579 - val_loss: 0.5916 - val_acc: 0.7578\n",
      "Epoch 31/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5612 - acc: 0.7596 - val_loss: 0.5910 - val_acc: 0.7576\n",
      "Epoch 32/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5585 - acc: 0.7616 - val_loss: 0.5835 - val_acc: 0.7597\n",
      "Epoch 33/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5551 - acc: 0.7631 - val_loss: 0.5802 - val_acc: 0.7618\n",
      "Epoch 34/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5512 - acc: 0.7653 - val_loss: 0.5809 - val_acc: 0.7634\n",
      "Epoch 35/100\n",
      "298900/298900 [==============================] - 25s 82us/step - loss: 0.5474 - acc: 0.7683 - val_loss: 0.5778 - val_acc: 0.7654\n",
      "Epoch 36/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.5446 - acc: 0.7691 - val_loss: 0.5741 - val_acc: 0.7656\n",
      "Epoch 37/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.5423 - acc: 0.7710 - val_loss: 0.5749 - val_acc: 0.7650\n",
      "Epoch 38/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.5407 - acc: 0.7703 - val_loss: 0.5666 - val_acc: 0.7707\n",
      "Epoch 39/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.5372 - acc: 0.7729 - val_loss: 0.5654 - val_acc: 0.7733\n",
      "Epoch 40/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5347 - acc: 0.7743 - val_loss: 0.5623 - val_acc: 0.7734\n",
      "Epoch 41/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.5308 - acc: 0.7754 - val_loss: 0.5609 - val_acc: 0.7743\n",
      "Epoch 42/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.5284 - acc: 0.7774 - val_loss: 0.5565 - val_acc: 0.7768\n",
      "Epoch 43/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.5276 - acc: 0.7769 - val_loss: 0.5564 - val_acc: 0.7774\n",
      "Epoch 44/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.5254 - acc: 0.7779 - val_loss: 0.5551 - val_acc: 0.7777\n",
      "Epoch 45/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.5217 - acc: 0.7816 - val_loss: 0.5539 - val_acc: 0.7774\n",
      "Epoch 46/100\n",
      "298900/298900 [==============================] - 25s 82us/step - loss: 0.5194 - acc: 0.7810 - val_loss: 0.5547 - val_acc: 0.7789\n",
      "Epoch 47/100\n",
      "298900/298900 [==============================] - 25s 84us/step - loss: 0.5167 - acc: 0.7825 - val_loss: 0.5521 - val_acc: 0.7800\n",
      "Epoch 48/100\n",
      "298900/298900 [==============================] - 28s 93us/step - loss: 0.5169 - acc: 0.7830 - val_loss: 0.5462 - val_acc: 0.7828\n",
      "Epoch 49/100\n",
      "298900/298900 [==============================] - 27s 91us/step - loss: 0.5142 - acc: 0.7851 - val_loss: 0.5414 - val_acc: 0.7845\n",
      "Epoch 50/100\n",
      "298900/298900 [==============================] - 25s 84us/step - loss: 0.5128 - acc: 0.7850 - val_loss: 0.5493 - val_acc: 0.7820\n",
      "Epoch 51/100\n",
      "298900/298900 [==============================] - 24s 82us/step - loss: 0.5095 - acc: 0.7870 - val_loss: 0.5399 - val_acc: 0.7870\n",
      "Epoch 52/100\n",
      "298900/298900 [==============================] - 25s 85us/step - loss: 0.5094 - acc: 0.7865 - val_loss: 0.5422 - val_acc: 0.7840\n",
      "Epoch 53/100\n",
      "298900/298900 [==============================] - 25s 84us/step - loss: 0.5083 - acc: 0.7877 - val_loss: 0.5373 - val_acc: 0.7883\n",
      "Epoch 54/100\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.5033 - acc: 0.7898 - val_loss: 0.5361 - val_acc: 0.7883\n",
      "Epoch 55/100\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.5031 - acc: 0.7903 - val_loss: 0.5449 - val_acc: 0.7847\n",
      "Epoch 56/100\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.5021 - acc: 0.7906 - val_loss: 0.5387 - val_acc: 0.7867\n",
      "Epoch 57/100\n",
      "298900/298900 [==============================] - 24s 82us/step - loss: 0.4992 - acc: 0.7916 - val_loss: 0.5314 - val_acc: 0.7897\n",
      "Epoch 58/100\n",
      "298900/298900 [==============================] - 27s 92us/step - loss: 0.5003 - acc: 0.7910 - val_loss: 0.5319 - val_acc: 0.7917\n",
      "Epoch 59/100\n",
      "298900/298900 [==============================] - 27s 91us/step - loss: 0.4981 - acc: 0.7923 - val_loss: 0.5331 - val_acc: 0.7911\n",
      "Epoch 60/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4942 - acc: 0.7939 - val_loss: 0.5315 - val_acc: 0.7910\n",
      "Epoch 61/100\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.4952 - acc: 0.7934 - val_loss: 0.5282 - val_acc: 0.7937\n",
      "Epoch 62/100\n",
      "298900/298900 [==============================] - 27s 90us/step - loss: 0.4944 - acc: 0.7945 - val_loss: 0.5284 - val_acc: 0.7945\n",
      "Epoch 63/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.4909 - acc: 0.7954 - val_loss: 0.5281 - val_acc: 0.7939\n",
      "Epoch 64/100\n",
      "298900/298900 [==============================] - 27s 91us/step - loss: 0.4897 - acc: 0.7964 - val_loss: 0.5223 - val_acc: 0.7968\n",
      "Epoch 65/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4867 - acc: 0.7979 - val_loss: 0.5237 - val_acc: 0.7959\n",
      "Epoch 66/100\n",
      "298900/298900 [==============================] - 26s 86us/step - loss: 0.4899 - acc: 0.7968 - val_loss: 0.5244 - val_acc: 0.7949\n",
      "Epoch 67/100\n",
      "298900/298900 [==============================] - 25s 85us/step - loss: 0.4876 - acc: 0.7979 - val_loss: 0.5225 - val_acc: 0.7955\n",
      "Epoch 68/100\n",
      "298900/298900 [==============================] - 27s 89us/step - loss: 0.4856 - acc: 0.7987 - val_loss: 0.5196 - val_acc: 0.7976\n",
      "Epoch 69/100\n",
      "298900/298900 [==============================] - 33s 109us/step - loss: 0.4854 - acc: 0.7986 - val_loss: 0.5194 - val_acc: 0.7995\n",
      "Epoch 70/100\n",
      "298900/298900 [==============================] - 34s 112us/step - loss: 0.4832 - acc: 0.7992 - val_loss: 0.5184 - val_acc: 0.7999\n",
      "Epoch 71/100\n",
      "298900/298900 [==============================] - 27s 90us/step - loss: 0.4817 - acc: 0.8001 - val_loss: 0.5182 - val_acc: 0.7987\n",
      "Epoch 72/100\n",
      "298900/298900 [==============================] - 28s 92us/step - loss: 0.4803 - acc: 0.8010 - val_loss: 0.5149 - val_acc: 0.8013\n",
      "Epoch 73/100\n",
      "298900/298900 [==============================] - 27s 89us/step - loss: 0.4795 - acc: 0.8022 - val_loss: 0.5158 - val_acc: 0.8000\n",
      "Epoch 74/100\n",
      "298900/298900 [==============================] - 28s 95us/step - loss: 0.4770 - acc: 0.8018 - val_loss: 0.5128 - val_acc: 0.8021\n",
      "Epoch 75/100\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.4765 - acc: 0.8027 - val_loss: 0.5092 - val_acc: 0.8026\n",
      "Epoch 76/100\n",
      "298900/298900 [==============================] - 23s 76us/step - loss: 0.4764 - acc: 0.8030 - val_loss: 0.5106 - val_acc: 0.8029\n",
      "Epoch 77/100\n",
      "298900/298900 [==============================] - 30s 100us/step - loss: 0.4751 - acc: 0.8037 - val_loss: 0.5121 - val_acc: 0.8025\n",
      "Epoch 78/100\n",
      "298900/298900 [==============================] - 25s 85us/step - loss: 0.4730 - acc: 0.8045 - val_loss: 0.5145 - val_acc: 0.8009\n",
      "Epoch 79/100\n",
      "298900/298900 [==============================] - 25s 85us/step - loss: 0.4721 - acc: 0.8054 - val_loss: 0.5109 - val_acc: 0.8039\n",
      "Epoch 80/100\n",
      "298900/298900 [==============================] - 27s 90us/step - loss: 0.4716 - acc: 0.8047 - val_loss: 0.5116 - val_acc: 0.8021\n",
      "Epoch 81/100\n",
      "298900/298900 [==============================] - 25s 85us/step - loss: 0.4729 - acc: 0.8050 - val_loss: 0.5117 - val_acc: 0.8037\n",
      "Epoch 82/100\n",
      "298900/298900 [==============================] - 22s 73us/step - loss: 0.4695 - acc: 0.8057 - val_loss: 0.5068 - val_acc: 0.8059\n",
      "Epoch 83/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4707 - acc: 0.8058 - val_loss: 0.5134 - val_acc: 0.8034\n",
      "Epoch 84/100\n",
      "298900/298900 [==============================] - 24s 82us/step - loss: 0.4682 - acc: 0.8071 - val_loss: 0.5120 - val_acc: 0.8036\n",
      "Epoch 85/100\n",
      "298900/298900 [==============================] - 24s 82us/step - loss: 0.4682 - acc: 0.8074 - val_loss: 0.5057 - val_acc: 0.8053\n",
      "Epoch 86/100\n",
      "298900/298900 [==============================] - 27s 90us/step - loss: 0.4663 - acc: 0.8079 - val_loss: 0.5074 - val_acc: 0.8048\n",
      "Epoch 87/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4659 - acc: 0.8082 - val_loss: 0.4998 - val_acc: 0.8095\n",
      "Epoch 88/100\n",
      "298900/298900 [==============================] - 25s 84us/step - loss: 0.4658 - acc: 0.8079 - val_loss: 0.5102 - val_acc: 0.8034\n",
      "Epoch 89/100\n",
      "298900/298900 [==============================] - 23s 78us/step - loss: 0.4631 - acc: 0.8092 - val_loss: 0.5077 - val_acc: 0.8055\n",
      "Epoch 90/100\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.4636 - acc: 0.8090 - val_loss: 0.5018 - val_acc: 0.8092\n",
      "Epoch 91/100\n",
      "298900/298900 [==============================] - 24s 79us/step - loss: 0.4621 - acc: 0.8101 - val_loss: 0.4985 - val_acc: 0.8108\n",
      "Epoch 92/100\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.4621 - acc: 0.8099 - val_loss: 0.5076 - val_acc: 0.8042\n",
      "Epoch 93/100\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.4606 - acc: 0.8112 - val_loss: 0.4966 - val_acc: 0.8103\n",
      "Epoch 94/100\n",
      "298900/298900 [==============================] - 22s 73us/step - loss: 0.4593 - acc: 0.8117 - val_loss: 0.4985 - val_acc: 0.8107\n",
      "Epoch 95/100\n",
      "298900/298900 [==============================] - 25s 83us/step - loss: 0.4589 - acc: 0.8119 - val_loss: 0.5009 - val_acc: 0.8086\n",
      "Epoch 96/100\n",
      "298900/298900 [==============================] - 24s 80us/step - loss: 0.4601 - acc: 0.8114 - val_loss: 0.5006 - val_acc: 0.8103\n",
      "Epoch 97/100\n",
      "298900/298900 [==============================] - 24s 82us/step - loss: 0.4562 - acc: 0.8127 - val_loss: 0.4941 - val_acc: 0.8127\n",
      "Epoch 98/100\n",
      "298900/298900 [==============================] - 27s 91us/step - loss: 0.4553 - acc: 0.8129 - val_loss: 0.4972 - val_acc: 0.8112\n",
      "Epoch 99/100\n",
      "298900/298900 [==============================] - 26s 86us/step - loss: 0.4566 - acc: 0.8126 - val_loss: 0.4981 - val_acc: 0.81150.4557 - ETA: 2s - loss: - ETA: 0s - loss: \n",
      "Epoch 100/100\n",
      "298900/298900 [==============================] - 24s 81us/step - loss: 0.4545 - acc: 0.8141 - val_loss: 0.5004 - val_acc: 0.8091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d6908305f8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Sequential()\n",
    "    \n",
    "model.add(Dense(300, input_dim=train_data.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(212, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(106, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(dummy_train_target.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, dummy_train_target, epochs=100, batch_size=200, verbose=1, validation_data=(test_data,dummy_test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the Training set and Test set accuracy using keras.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298900/298900 [==============================] - 18s 60us/step\n",
      "74726/74726 [==============================] - 4s 47us/step\n",
      "Train Accuracy= 0.904998327198137\n",
      "Test Accuracy= 0.8091293525696439\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(train_data, dummy_train_target)\n",
    "_, test_accuracy = model.evaluate(test_data, dummy_test_target)\n",
    "\n",
    "print('Train Accuracy=',accuracy)\n",
    "print('Test Accuracy=',test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance comparison between the Training Set and Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298900/298900 [==============================] - 14s 47us/step\n",
      "74726/74726 [==============================] - 3s 44us/step\n",
      "Training Set Performance\n",
      "[[93397  1838  4284]\n",
      " [ 1113 96527  2116]\n",
      " [12371  6674 80580]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     5-10yrs       0.87      0.94      0.91     99519\n",
      "      <=5yrs       0.92      0.97      0.94     99756\n",
      "      >10yrs       0.93      0.81      0.86     99625\n",
      "\n",
      "    accuracy                           0.90    298900\n",
      "   macro avg       0.91      0.90      0.90    298900\n",
      "weighted avg       0.91      0.90      0.90    298900\n",
      "\n",
      "\n",
      "\n",
      "Test Set Performance\n",
      "[[21595  1095  2333]\n",
      " [  898 22607  1281]\n",
      " [ 5709  2947 16261]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     5-10yrs       0.77      0.86      0.81     25023\n",
      "      <=5yrs       0.85      0.91      0.88     24786\n",
      "      >10yrs       0.82      0.65      0.73     24917\n",
      "\n",
      "    accuracy                           0.81     74726\n",
      "   macro avg       0.81      0.81      0.81     74726\n",
      "weighted avg       0.81      0.81      0.81     74726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat_train_class = model.predict_classes(train_data, verbose=1)\n",
    "y_hat_test_class = model.predict_classes(test_data, verbose=1)\n",
    "\n",
    "print('Training Set Performance')\n",
    "conf_matrix_ann = confusion_matrix(encoder.inverse_transform(encoded_train_target), encoder.inverse_transform(y_hat_train_class))\n",
    "print(conf_matrix_ann)\n",
    "cr_ann = classification_report(encoder.inverse_transform(encoded_train_target), encoder.inverse_transform(y_hat_train_class))\n",
    "print(cr_ann)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Test Set Performance')\n",
    "conf_matrix_ann = confusion_matrix(encoder.inverse_transform(encoded_test_target), encoder.inverse_transform(y_hat_test_class))\n",
    "print(conf_matrix_ann)\n",
    "\n",
    "cr_ann = classification_report(encoder.inverse_transform(encoded_test_target), encoder.inverse_transform(y_hat_test_class))\n",
    "print(cr_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both the RNN and CNN models, we need the input to be 3-dimensional including the timesteps. Therefore, reshaping the 2-D array into 3-D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(298900, 1, 414)\n",
      "(74726, 1, 414)\n"
     ]
    }
   ],
   "source": [
    "scaler=StandardScaler()\n",
    "train_data=scaler.fit_transform(train_data)\n",
    "test_data=scaler.transform(test_data)\n",
    "\n",
    "train_data=train_data.reshape(train_data.shape[0],1,train_data.shape[1])\n",
    "test_data=test_data.reshape(test_data.shape[0],1,test_data.shape[1])\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the structure of the RNN with 3 hidden layers with [128,64,32] neurons each and ReLU activation function on the hidden layers and softmax activation on the output layer. The output layer has three neuron in line with the number of classes for prediction. 10% dropout layer is used after the first and last hidden layer.\n",
    "\n",
    "Loss Function- Categorical Crossentropy\n",
    "\n",
    "Optimizer- ADAM\n",
    "\n",
    "Observed Metrics- Accuracy\n",
    "\n",
    "An initial fit of the model is developed over a large number of epochs to determine point of overfitting of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 298900 samples, validate on 74726 samples\n",
      "Epoch 1/150\n",
      "298900/298900 [==============================] - 51s 169us/step - loss: 0.8995 - acc: 0.5641 - val_loss: 0.8782 - val_acc: 0.5763\n",
      "Epoch 2/150\n",
      "298900/298900 [==============================] - 49s 164us/step - loss: 0.8638 - acc: 0.5868 - val_loss: 0.8554 - val_acc: 0.5910\n",
      "Epoch 3/150\n",
      "298900/298900 [==============================] - 47s 158us/step - loss: 0.8271 - acc: 0.6090 - val_loss: 0.8248 - val_acc: 0.6104\n",
      "Epoch 4/150\n",
      "298900/298900 [==============================] - 48s 159us/step - loss: 0.7893 - acc: 0.6318 - val_loss: 0.7973 - val_acc: 0.6279\n",
      "Epoch 5/150\n",
      "298900/298900 [==============================] - 48s 161us/step - loss: 0.7535 - acc: 0.6513 - val_loss: 0.7714 - val_acc: 0.6444\n",
      "Epoch 6/150\n",
      "298900/298900 [==============================] - 48s 161us/step - loss: 0.7231 - acc: 0.6678 - val_loss: 0.7507 - val_acc: 0.6563\n",
      "Epoch 7/150\n",
      "298900/298900 [==============================] - 48s 161us/step - loss: 0.6970 - acc: 0.6827 - val_loss: 0.7284 - val_acc: 0.6701\n",
      "Epoch 8/150\n",
      "298900/298900 [==============================] - 51s 170us/step - loss: 0.6758 - acc: 0.6938 - val_loss: 0.7158 - val_acc: 0.6811\n",
      "Epoch 9/150\n",
      "298900/298900 [==============================] - 50s 166us/step - loss: 0.6558 - acc: 0.7047 - val_loss: 0.7037 - val_acc: 0.6877\n",
      "Epoch 10/150\n",
      "298900/298900 [==============================] - 48s 161us/step - loss: 0.6381 - acc: 0.7150 - val_loss: 0.6939 - val_acc: 0.6948\n",
      "Epoch 11/150\n",
      "298900/298900 [==============================] - 49s 165us/step - loss: 0.6235 - acc: 0.7224 - val_loss: 0.6776 - val_acc: 0.7061\n",
      "Epoch 12/150\n",
      "298900/298900 [==============================] - 54s 182us/step - loss: 0.6085 - acc: 0.7305 - val_loss: 0.6770 - val_acc: 0.7110\n",
      "Epoch 13/150\n",
      "298900/298900 [==============================] - 49s 165us/step - loss: 0.5981 - acc: 0.7364 - val_loss: 0.6606 - val_acc: 0.7153\n",
      "Epoch 14/150\n",
      "298900/298900 [==============================] - 49s 163us/step - loss: 0.5866 - acc: 0.7425 - val_loss: 0.6497 - val_acc: 0.7232\n",
      "Epoch 15/150\n",
      "298900/298900 [==============================] - 47s 157us/step - loss: 0.5753 - acc: 0.7478 - val_loss: 0.6448 - val_acc: 0.7276\n",
      "Epoch 16/150\n",
      "298900/298900 [==============================] - 47s 158us/step - loss: 0.5656 - acc: 0.7528 - val_loss: 0.6408 - val_acc: 0.7298\n",
      "Epoch 17/150\n",
      "298900/298900 [==============================] - 47s 157us/step - loss: 0.5584 - acc: 0.7571 - val_loss: 0.6367 - val_acc: 0.7346\n",
      "Epoch 18/150\n",
      "298900/298900 [==============================] - 48s 160us/step - loss: 0.5497 - acc: 0.7611 - val_loss: 0.6270 - val_acc: 0.7393\n",
      "Epoch 19/150\n",
      "298900/298900 [==============================] - 48s 160us/step - loss: 0.5426 - acc: 0.7649 - val_loss: 0.6247 - val_acc: 0.7413\n",
      "Epoch 20/150\n",
      "298900/298900 [==============================] - 47s 157us/step - loss: 0.5353 - acc: 0.7686 - val_loss: 0.6148 - val_acc: 0.7465\n",
      "Epoch 21/150\n",
      "298900/298900 [==============================] - 47s 158us/step - loss: 0.5302 - acc: 0.7712 - val_loss: 0.6157 - val_acc: 0.7492\n",
      "Epoch 22/150\n",
      "298900/298900 [==============================] - 48s 159us/step - loss: 0.5238 - acc: 0.7753 - val_loss: 0.6097 - val_acc: 0.7508\n",
      "Epoch 23/150\n",
      "298900/298900 [==============================] - 47s 157us/step - loss: 0.5176 - acc: 0.7781 - val_loss: 0.6140 - val_acc: 0.7527\n",
      "Epoch 24/150\n",
      "298900/298900 [==============================] - 47s 159us/step - loss: 0.5128 - acc: 0.7806 - val_loss: 0.6030 - val_acc: 0.7549\n",
      "Epoch 25/150\n",
      "298900/298900 [==============================] - 48s 159us/step - loss: 0.5071 - acc: 0.7840 - val_loss: 0.5966 - val_acc: 0.7570\n",
      "Epoch 26/150\n",
      "298900/298900 [==============================] - 49s 162us/step - loss: 0.5027 - acc: 0.7862 - val_loss: 0.5959 - val_acc: 0.7586\n",
      "Epoch 27/150\n",
      "298900/298900 [==============================] - 50s 167us/step - loss: 0.4987 - acc: 0.7880 - val_loss: 0.5936 - val_acc: 0.7622\n",
      "Epoch 28/150\n",
      "298900/298900 [==============================] - 51s 172us/step - loss: 0.4938 - acc: 0.7902 - val_loss: 0.5920 - val_acc: 0.7645\n",
      "Epoch 29/150\n",
      "298900/298900 [==============================] - 50s 168us/step - loss: 0.4896 - acc: 0.7923 - val_loss: 0.5894 - val_acc: 0.7654\n",
      "Epoch 30/150\n",
      "298900/298900 [==============================] - 45s 150us/step - loss: 0.4867 - acc: 0.7942 - val_loss: 0.5844 - val_acc: 0.7686\n",
      "Epoch 31/150\n",
      "298900/298900 [==============================] - 51s 172us/step - loss: 0.4820 - acc: 0.7967 - val_loss: 0.5885 - val_acc: 0.7682\n",
      "Epoch 32/150\n",
      "298900/298900 [==============================] - 57s 189us/step - loss: 0.4788 - acc: 0.7988 - val_loss: 0.5867 - val_acc: 0.7694\n",
      "Epoch 33/150\n",
      "298900/298900 [==============================] - 51s 171us/step - loss: 0.4745 - acc: 0.7999 - val_loss: 0.5809 - val_acc: 0.7723\n",
      "Epoch 34/150\n",
      "298900/298900 [==============================] - 46s 153us/step - loss: 0.4721 - acc: 0.8016 - val_loss: 0.5785 - val_acc: 0.7727\n",
      "Epoch 35/150\n",
      "298900/298900 [==============================] - 47s 157us/step - loss: 0.4696 - acc: 0.8034 - val_loss: 0.5787 - val_acc: 0.7742\n",
      "Epoch 36/150\n",
      "298900/298900 [==============================] - 44s 148us/step - loss: 0.4673 - acc: 0.8041 - val_loss: 0.5708 - val_acc: 0.7765\n",
      "Epoch 37/150\n",
      "298900/298900 [==============================] - 43s 143us/step - loss: 0.4632 - acc: 0.8056 - val_loss: 0.5728 - val_acc: 0.7792\n",
      "Epoch 38/150\n",
      "298900/298900 [==============================] - 44s 149us/step - loss: 0.4603 - acc: 0.8078 - val_loss: 0.5641 - val_acc: 0.7803\n",
      "Epoch 39/150\n",
      "298900/298900 [==============================] - 51s 171us/step - loss: 0.4576 - acc: 0.8086 - val_loss: 0.5717 - val_acc: 0.7781\n",
      "Epoch 40/150\n",
      "298900/298900 [==============================] - 47s 156us/step - loss: 0.4543 - acc: 0.8106 - val_loss: 0.5717 - val_acc: 0.7800\n",
      "Epoch 41/150\n",
      "298900/298900 [==============================] - 49s 163us/step - loss: 0.4525 - acc: 0.8116 - val_loss: 0.5729 - val_acc: 0.7813\n",
      "Epoch 42/150\n",
      "298900/298900 [==============================] - 47s 157us/step - loss: 0.4504 - acc: 0.8128 - val_loss: 0.5705 - val_acc: 0.7843\n",
      "Epoch 43/150\n",
      "298900/298900 [==============================] - 47s 157us/step - loss: 0.4492 - acc: 0.8137 - val_loss: 0.5669 - val_acc: 0.7856\n",
      "Epoch 44/150\n",
      "298900/298900 [==============================] - 47s 158us/step - loss: 0.4486 - acc: 0.8141 - val_loss: 0.5618 - val_acc: 0.7843\n",
      "Epoch 45/150\n",
      "298900/298900 [==============================] - 47s 156us/step - loss: 0.4426 - acc: 0.8170 - val_loss: 0.5572 - val_acc: 0.7866\n",
      "Epoch 46/150\n",
      "298900/298900 [==============================] - 46s 155us/step - loss: 0.4409 - acc: 0.8176 - val_loss: 0.5551 - val_acc: 0.7872\n",
      "Epoch 47/150\n",
      "298900/298900 [==============================] - 46s 155us/step - loss: 0.4413 - acc: 0.8179 - val_loss: 0.5594 - val_acc: 0.7888\n",
      "Epoch 48/150\n",
      "298900/298900 [==============================] - 47s 157us/step - loss: 0.4376 - acc: 0.8199 - val_loss: 0.5550 - val_acc: 0.7912\n",
      "Epoch 49/150\n",
      "298900/298900 [==============================] - 47s 157us/step - loss: 0.4358 - acc: 0.8202 - val_loss: 0.5590 - val_acc: 0.7898\n",
      "Epoch 50/150\n",
      "298900/298900 [==============================] - 46s 153us/step - loss: 0.4323 - acc: 0.8229 - val_loss: 0.5605 - val_acc: 0.7902\n",
      "Epoch 51/150\n",
      "298900/298900 [==============================] - 48s 161us/step - loss: 0.4307 - acc: 0.8223 - val_loss: 0.5550 - val_acc: 0.7918\n",
      "Epoch 52/150\n",
      "298900/298900 [==============================] - 46s 153us/step - loss: 0.4310 - acc: 0.8232 - val_loss: 0.5527 - val_acc: 0.7936\n",
      "Epoch 53/150\n",
      "298900/298900 [==============================] - 50s 167us/step - loss: 0.4288 - acc: 0.8235 - val_loss: 0.5522 - val_acc: 0.7940\n",
      "Epoch 54/150\n",
      "298900/298900 [==============================] - 53s 176us/step - loss: 0.4262 - acc: 0.8253 - val_loss: 0.5491 - val_acc: 0.7947\n",
      "Epoch 55/150\n",
      "298900/298900 [==============================] - 51s 171us/step - loss: 0.4260 - acc: 0.8254 - val_loss: 0.5519 - val_acc: 0.7932\n",
      "Epoch 56/150\n",
      "298900/298900 [==============================] - 55s 183us/step - loss: 0.4251 - acc: 0.8256 - val_loss: 0.5463 - val_acc: 0.7956\n",
      "Epoch 57/150\n",
      "298900/298900 [==============================] - 59s 199us/step - loss: 0.4231 - acc: 0.8266 - val_loss: 0.5446 - val_acc: 0.7965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/150\n",
      "298900/298900 [==============================] - 50s 167us/step - loss: 0.4208 - acc: 0.8275 - val_loss: 0.5467 - val_acc: 0.7960\n",
      "Epoch 59/150\n",
      "298900/298900 [==============================] - 55s 185us/step - loss: 0.4197 - acc: 0.8276 - val_loss: 0.5544 - val_acc: 0.7952\n",
      "Epoch 60/150\n",
      "298900/298900 [==============================] - 46s 155us/step - loss: 0.4192 - acc: 0.8286 - val_loss: 0.5481 - val_acc: 0.7976\n",
      "Epoch 61/150\n",
      "298900/298900 [==============================] - 48s 161us/step - loss: 0.4164 - acc: 0.8291 - val_loss: 0.5464 - val_acc: 0.7977\n",
      "Epoch 62/150\n",
      "298900/298900 [==============================] - 46s 153us/step - loss: 0.4169 - acc: 0.8297 - val_loss: 0.5488 - val_acc: 0.7996\n",
      "Epoch 63/150\n",
      "298900/298900 [==============================] - 47s 157us/step - loss: 0.4137 - acc: 0.8315 - val_loss: 0.5363 - val_acc: 0.8003\n",
      "Epoch 64/150\n",
      "298900/298900 [==============================] - 45s 150us/step - loss: 0.4119 - acc: 0.8321 - val_loss: 0.5438 - val_acc: 0.8004\n",
      "Epoch 65/150\n",
      "298900/298900 [==============================] - 44s 147us/step - loss: 0.4121 - acc: 0.8328 - val_loss: 0.5518 - val_acc: 0.7985\n",
      "Epoch 66/150\n",
      "298900/298900 [==============================] - 45s 151us/step - loss: 0.4099 - acc: 0.8327 - val_loss: 0.5397 - val_acc: 0.8006\n",
      "Epoch 67/150\n",
      "298900/298900 [==============================] - 45s 150us/step - loss: 0.4093 - acc: 0.8331 - val_loss: 0.5540 - val_acc: 0.8000\n",
      "Epoch 68/150\n",
      "298900/298900 [==============================] - 44s 147us/step - loss: 0.4097 - acc: 0.8332 - val_loss: 0.5448 - val_acc: 0.8017\n",
      "Epoch 69/150\n",
      "298900/298900 [==============================] - 45s 150us/step - loss: 0.4088 - acc: 0.8336 - val_loss: 0.5380 - val_acc: 0.8029\n",
      "Epoch 70/150\n",
      "298900/298900 [==============================] - 45s 151us/step - loss: 0.4045 - acc: 0.8354 - val_loss: 0.5403 - val_acc: 0.8042\n",
      "Epoch 71/150\n",
      "298900/298900 [==============================] - 45s 150us/step - loss: 0.4040 - acc: 0.8353 - val_loss: 0.5421 - val_acc: 0.8041\n",
      "Epoch 72/150\n",
      "298900/298900 [==============================] - 43s 144us/step - loss: 0.4040 - acc: 0.8361 - val_loss: 0.5327 - val_acc: 0.8032\n",
      "Epoch 73/150\n",
      "298900/298900 [==============================] - 43s 144us/step - loss: 0.4027 - acc: 0.8369 - val_loss: 0.5452 - val_acc: 0.8033\n",
      "Epoch 74/150\n",
      "298900/298900 [==============================] - 50s 169us/step - loss: 0.4022 - acc: 0.8367 - val_loss: 0.5397 - val_acc: 0.8044\n",
      "Epoch 75/150\n",
      "298900/298900 [==============================] - 49s 164us/step - loss: 0.4003 - acc: 0.8378 - val_loss: 0.5403 - val_acc: 0.8048\n",
      "Epoch 76/150\n",
      "298900/298900 [==============================] - 51s 171us/step - loss: 0.3988 - acc: 0.8378 - val_loss: 0.5378 - val_acc: 0.8039\n",
      "Epoch 77/150\n",
      "298900/298900 [==============================] - 53s 177us/step - loss: 0.3984 - acc: 0.8383 - val_loss: 0.5444 - val_acc: 0.8045\n",
      "Epoch 78/150\n",
      "298900/298900 [==============================] - 51s 171us/step - loss: 0.3987 - acc: 0.8384 - val_loss: 0.5361 - val_acc: 0.8065\n",
      "Epoch 79/150\n",
      "298900/298900 [==============================] - 50s 167us/step - loss: 0.3976 - acc: 0.8390 - val_loss: 0.5291 - val_acc: 0.8069\n",
      "Epoch 80/150\n",
      "298900/298900 [==============================] - 51s 170us/step - loss: 0.3956 - acc: 0.8395 - val_loss: 0.5488 - val_acc: 0.8053\n",
      "Epoch 81/150\n",
      "298900/298900 [==============================] - 52s 174us/step - loss: 0.3968 - acc: 0.8396 - val_loss: 0.5362 - val_acc: 0.8071\n",
      "Epoch 82/150\n",
      "298900/298900 [==============================] - 51s 170us/step - loss: 0.3942 - acc: 0.8406 - val_loss: 0.5353 - val_acc: 0.8071\n",
      "Epoch 83/150\n",
      "298900/298900 [==============================] - 51s 169us/step - loss: 0.3932 - acc: 0.8413 - val_loss: 0.5339 - val_acc: 0.8076\n",
      "Epoch 84/150\n",
      "298900/298900 [==============================] - 53s 178us/step - loss: 0.3937 - acc: 0.8409 - val_loss: 0.5291 - val_acc: 0.8114\n",
      "Epoch 85/150\n",
      "298900/298900 [==============================] - 56s 188us/step - loss: 0.3900 - acc: 0.8424 - val_loss: 0.5345 - val_acc: 0.8078\n",
      "Epoch 86/150\n",
      "298900/298900 [==============================] - 50s 167us/step - loss: 0.3920 - acc: 0.8419 - val_loss: 0.5257 - val_acc: 0.8098\n",
      "Epoch 87/150\n",
      "298900/298900 [==============================] - 48s 162us/step - loss: 0.3903 - acc: 0.8422 - val_loss: 0.5281 - val_acc: 0.8112\n",
      "Epoch 88/150\n",
      "298900/298900 [==============================] - 45s 152us/step - loss: 0.3903 - acc: 0.8420 - val_loss: 0.5405 - val_acc: 0.8093\n",
      "Epoch 89/150\n",
      "298900/298900 [==============================] - 46s 153us/step - loss: 0.3889 - acc: 0.8428 - val_loss: 0.5319 - val_acc: 0.8108\n",
      "Epoch 90/150\n",
      "298900/298900 [==============================] - 48s 160us/step - loss: 0.3874 - acc: 0.8436 - val_loss: 0.5393 - val_acc: 0.8075\n",
      "Epoch 91/150\n",
      "298900/298900 [==============================] - 43s 145us/step - loss: 0.3859 - acc: 0.8445 - val_loss: 0.5222 - val_acc: 0.8096\n",
      "Epoch 92/150\n",
      "298900/298900 [==============================] - 44s 146us/step - loss: 0.3860 - acc: 0.8450 - val_loss: 0.5339 - val_acc: 0.8108\n",
      "Epoch 93/150\n",
      "298900/298900 [==============================] - 44s 147us/step - loss: 0.3866 - acc: 0.8453 - val_loss: 0.5296 - val_acc: 0.8114\n",
      "Epoch 94/150\n",
      "298900/298900 [==============================] - 44s 148us/step - loss: 0.3837 - acc: 0.8461 - val_loss: 0.5302 - val_acc: 0.8108\n",
      "Epoch 95/150\n",
      "298900/298900 [==============================] - 44s 146us/step - loss: 0.3846 - acc: 0.8454 - val_loss: 0.5323 - val_acc: 0.8096\n",
      "Epoch 96/150\n",
      "298900/298900 [==============================] - 47s 156us/step - loss: 0.3832 - acc: 0.8458 - val_loss: 0.5282 - val_acc: 0.8129\n",
      "Epoch 97/150\n",
      "298900/298900 [==============================] - 43s 144us/step - loss: 0.3817 - acc: 0.8467 - val_loss: 0.5269 - val_acc: 0.8120\n",
      "Epoch 98/150\n",
      "298900/298900 [==============================] - 43s 143us/step - loss: 0.3833 - acc: 0.8460 - val_loss: 0.5279 - val_acc: 0.8128\n",
      "Epoch 99/150\n",
      "298900/298900 [==============================] - 46s 156us/step - loss: 0.3804 - acc: 0.8470 - val_loss: 0.5301 - val_acc: 0.8131\n",
      "Epoch 100/150\n",
      "298900/298900 [==============================] - 46s 153us/step - loss: 0.3823 - acc: 0.8460 - val_loss: 0.5302 - val_acc: 0.8134\n",
      "Epoch 101/150\n",
      "298900/298900 [==============================] - 45s 152us/step - loss: 0.3800 - acc: 0.8477 - val_loss: 0.5224 - val_acc: 0.8128\n",
      "Epoch 102/150\n",
      "298900/298900 [==============================] - 48s 160us/step - loss: 0.3805 - acc: 0.8474 - val_loss: 0.5339 - val_acc: 0.8129\n",
      "Epoch 103/150\n",
      "298900/298900 [==============================] - 45s 150us/step - loss: 0.3791 - acc: 0.8483 - val_loss: 0.5258 - val_acc: 0.8130\n",
      "Epoch 104/150\n",
      "298900/298900 [==============================] - 47s 157us/step - loss: 0.3785 - acc: 0.8491 - val_loss: 0.5294 - val_acc: 0.8117\n",
      "Epoch 105/150\n",
      "298900/298900 [==============================] - 45s 152us/step - loss: 0.3778 - acc: 0.8483 - val_loss: 0.5235 - val_acc: 0.8142\n",
      "Epoch 106/150\n",
      "298900/298900 [==============================] - 46s 154us/step - loss: 0.3765 - acc: 0.8494 - val_loss: 0.5293 - val_acc: 0.8129cc: 0.8\n",
      "Epoch 107/150\n",
      "298900/298900 [==============================] - 44s 147us/step - loss: 0.3757 - acc: 0.8493 - val_loss: 0.5214 - val_acc: 0.8163\n",
      "Epoch 108/150\n",
      "298900/298900 [==============================] - 44s 146us/step - loss: 0.3756 - acc: 0.8498 - val_loss: 0.5281 - val_acc: 0.8138\n",
      "Epoch 109/150\n",
      "298900/298900 [==============================] - 47s 158us/step - loss: 0.3756 - acc: 0.8493 - val_loss: 0.5320 - val_acc: 0.8144\n",
      "Epoch 110/150\n",
      "298900/298900 [==============================] - 46s 152us/step - loss: 0.3730 - acc: 0.8514 - val_loss: 0.5235 - val_acc: 0.8138\n",
      "Epoch 111/150\n",
      "298900/298900 [==============================] - 46s 153us/step - loss: 0.3753 - acc: 0.8496 - val_loss: 0.5410 - val_acc: 0.8133\n",
      "Epoch 112/150\n",
      "298900/298900 [==============================] - 46s 155us/step - loss: 0.3745 - acc: 0.8507 - val_loss: 0.5262 - val_acc: 0.8162\n",
      "Epoch 113/150\n",
      "298900/298900 [==============================] - 48s 162us/step - loss: 0.3730 - acc: 0.8511 - val_loss: 0.5228 - val_acc: 0.8179\n",
      "Epoch 114/150\n",
      "298900/298900 [==============================] - 46s 153us/step - loss: 0.3724 - acc: 0.8508 - val_loss: 0.5188 - val_acc: 0.8171\n",
      "Epoch 115/150\n",
      "298900/298900 [==============================] - 44s 148us/step - loss: 0.3705 - acc: 0.8519 - val_loss: 0.5254 - val_acc: 0.8172\n",
      "Epoch 116/150\n",
      "298900/298900 [==============================] - 43s 143us/step - loss: 0.3710 - acc: 0.8520 - val_loss: 0.5246 - val_acc: 0.8161\n",
      "Epoch 117/150\n",
      "298900/298900 [==============================] - 46s 152us/step - loss: 0.3695 - acc: 0.8530 - val_loss: 0.5234 - val_acc: 0.8182\n",
      "Epoch 118/150\n",
      "298900/298900 [==============================] - 51s 171us/step - loss: 0.3690 - acc: 0.8529 - val_loss: 0.5203 - val_acc: 0.8194\n",
      "Epoch 119/150\n",
      "298900/298900 [==============================] - 52s 175us/step - loss: 0.3707 - acc: 0.8528 - val_loss: 0.5284 - val_acc: 0.8170\n",
      "Epoch 120/150\n",
      "298900/298900 [==============================] - 52s 173us/step - loss: 0.3699 - acc: 0.8526 - val_loss: 0.5317 - val_acc: 0.8186\n",
      "Epoch 121/150\n",
      "298900/298900 [==============================] - 54s 182us/step - loss: 0.3687 - acc: 0.8528 - val_loss: 0.5221 - val_acc: 0.8188\n",
      "Epoch 122/150\n",
      "298900/298900 [==============================] - 58s 192us/step - loss: 0.3671 - acc: 0.8532 - val_loss: 0.5162 - val_acc: 0.8183\n",
      "Epoch 123/150\n",
      "298900/298900 [==============================] - 51s 171us/step - loss: 0.3665 - acc: 0.8542 - val_loss: 0.5281 - val_acc: 0.8153\n",
      "Epoch 124/150\n",
      "298900/298900 [==============================] - 52s 175us/step - loss: 0.3668 - acc: 0.8542 - val_loss: 0.5192 - val_acc: 0.8176\n",
      "Epoch 125/150\n",
      "298900/298900 [==============================] - 55s 186us/step - loss: 0.3664 - acc: 0.8543 - val_loss: 0.5177 - val_acc: 0.8186\n",
      "Epoch 126/150\n",
      "298900/298900 [==============================] - 51s 172us/step - loss: 0.3669 - acc: 0.8542 - val_loss: 0.5224 - val_acc: 0.8181\n",
      "Epoch 127/150\n",
      "298900/298900 [==============================] - 49s 165us/step - loss: 0.3653 - acc: 0.8544 - val_loss: 0.5307 - val_acc: 0.8187\n",
      "Epoch 128/150\n",
      "298900/298900 [==============================] - 54s 181us/step - loss: 0.3660 - acc: 0.8552 - val_loss: 0.5269 - val_acc: 0.8181\n",
      "Epoch 129/150\n",
      "298900/298900 [==============================] - 55s 185us/step - loss: 0.3652 - acc: 0.8543 - val_loss: 0.5220 - val_acc: 0.8198\n",
      "Epoch 130/150\n",
      "298900/298900 [==============================] - 52s 175us/step - loss: 0.3649 - acc: 0.8557 - val_loss: 0.5345 - val_acc: 0.8176\n",
      "Epoch 131/150\n",
      "298900/298900 [==============================] - 48s 162us/step - loss: 0.3637 - acc: 0.8556 - val_loss: 0.5289 - val_acc: 0.8195\n",
      "Epoch 132/150\n",
      "298900/298900 [==============================] - 51s 170us/step - loss: 0.3627 - acc: 0.8559 - val_loss: 0.5226 - val_acc: 0.8185\n",
      "Epoch 133/150\n",
      "298900/298900 [==============================] - 49s 162us/step - loss: 0.3642 - acc: 0.8554 - val_loss: 0.5297 - val_acc: 0.8187\n",
      "Epoch 134/150\n",
      "298900/298900 [==============================] - 51s 172us/step - loss: 0.3619 - acc: 0.8569 - val_loss: 0.5206 - val_acc: 0.8204\n",
      "Epoch 135/150\n",
      "298900/298900 [==============================] - 51s 170us/step - loss: 0.3613 - acc: 0.8571 - val_loss: 0.5358 - val_acc: 0.8179\n",
      "Epoch 136/150\n",
      "298900/298900 [==============================] - 50s 167us/step - loss: 0.3610 - acc: 0.8571 - val_loss: 0.5325 - val_acc: 0.8200\n",
      "Epoch 137/150\n",
      "298900/298900 [==============================] - 50s 169us/step - loss: 0.3606 - acc: 0.8576 - val_loss: 0.5269 - val_acc: 0.8210\n",
      "Epoch 138/150\n",
      "298900/298900 [==============================] - 48s 162us/step - loss: 0.3616 - acc: 0.8566 - val_loss: 0.5179 - val_acc: 0.8197\n",
      "Epoch 139/150\n",
      "298900/298900 [==============================] - 46s 154us/step - loss: 0.3611 - acc: 0.8573 - val_loss: 0.5323 - val_acc: 0.8188\n",
      "Epoch 140/150\n",
      "298900/298900 [==============================] - 46s 154us/step - loss: 0.3609 - acc: 0.8570 - val_loss: 0.5268 - val_acc: 0.8206\n",
      "Epoch 141/150\n",
      "298900/298900 [==============================] - 47s 156us/step - loss: 0.3583 - acc: 0.8584 - val_loss: 0.5322 - val_acc: 0.8203\n",
      "Epoch 142/150\n",
      "298900/298900 [==============================] - 47s 157us/step - loss: 0.3603 - acc: 0.8569 - val_loss: 0.5156 - val_acc: 0.8225\n",
      "Epoch 143/150\n",
      "298900/298900 [==============================] - 50s 169us/step - loss: 0.3613 - acc: 0.8566 - val_loss: 0.5199 - val_acc: 0.8222\n",
      "Epoch 144/150\n",
      "298900/298900 [==============================] - 47s 158us/step - loss: 0.3582 - acc: 0.8581 - val_loss: 0.5305 - val_acc: 0.8201\n",
      "Epoch 145/150\n",
      "298900/298900 [==============================] - 51s 170us/step - loss: 0.3592 - acc: 0.8573 - val_loss: 0.5130 - val_acc: 0.8217\n",
      "Epoch 146/150\n",
      "298900/298900 [==============================] - 49s 164us/step - loss: 0.3566 - acc: 0.8587 - val_loss: 0.5161 - val_acc: 0.8232\n",
      "Epoch 147/150\n",
      "298900/298900 [==============================] - 47s 157us/step - loss: 0.3579 - acc: 0.8584 - val_loss: 0.5154 - val_acc: 0.8227\n",
      "Epoch 148/150\n",
      "298900/298900 [==============================] - 51s 170us/step - loss: 0.3556 - acc: 0.8596 - val_loss: 0.5240 - val_acc: 0.82550.355\n",
      "Epoch 149/150\n",
      "298900/298900 [==============================] - 52s 174us/step - loss: 0.3548 - acc: 0.8602 - val_loss: 0.5240 - val_acc: 0.8215\n",
      "Epoch 150/150\n",
      "298900/298900 [==============================] - 46s 155us/step - loss: 0.3567 - acc: 0.8596 - val_loss: 0.5194 - val_acc: 0.8232\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "    \n",
    "model.add(LSTM(128, input_shape=train_data.shape[1:], return_sequences=True,activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(64, return_sequences=True,activation='relu'))\n",
    "model.add(LSTM(32, return_sequences=False,activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(dummy_train_target.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "hist=model.fit(train_data, dummy_train_target, epochs=150, batch_size=200, verbose=1, validation_data=(test_data,dummy_test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_num = np.arange(0, 150)\n",
    "plt.figure()\n",
    "plt.plot(epoch_num, hist.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(epoch_num, hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(epoch_num, hist.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(epoch_num, hist.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig('Breast Cancer RNN Epoch Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above graph, the validation loss stabilizes near the 70th epoch while the training loss keeps decreasing. Hence, to prevent overfitting, the model is retrained to 70 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 298900 samples, validate on 74726 samples\n",
      "Epoch 1/70\n",
      "298900/298900 [==============================] - 57s 192us/step - loss: 0.8987 - acc: 0.5640 - val_loss: 0.8781 - val_acc: 0.5772\n",
      "Epoch 2/70\n",
      "298900/298900 [==============================] - 54s 181us/step - loss: 0.8617 - acc: 0.5889 - val_loss: 0.8513 - val_acc: 0.5941\n",
      "Epoch 3/70\n",
      "298900/298900 [==============================] - 54s 180us/step - loss: 0.8239 - acc: 0.6108 - val_loss: 0.8231 - val_acc: 0.6126\n",
      "Epoch 4/70\n",
      "298900/298900 [==============================] - 53s 178us/step - loss: 0.7844 - acc: 0.6350 - val_loss: 0.7920 - val_acc: 0.6344\n",
      "Epoch 5/70\n",
      "298900/298900 [==============================] - 51s 171us/step - loss: 0.7503 - acc: 0.6541 - val_loss: 0.7700 - val_acc: 0.6473\n",
      "Epoch 6/70\n",
      "298900/298900 [==============================] - 53s 178us/step - loss: 0.7198 - acc: 0.6706 - val_loss: 0.7499 - val_acc: 0.6614\n",
      "Epoch 7/70\n",
      "298900/298900 [==============================] - 52s 176us/step - loss: 0.6944 - acc: 0.6850 - val_loss: 0.7325 - val_acc: 0.6715\n",
      "Epoch 8/70\n",
      "298900/298900 [==============================] - 52s 175us/step - loss: 0.6724 - acc: 0.6969 - val_loss: 0.7154 - val_acc: 0.6836\n",
      "Epoch 9/70\n",
      "298900/298900 [==============================] - 53s 177us/step - loss: 0.6525 - acc: 0.7084 - val_loss: 0.7031 - val_acc: 0.6931\n",
      "Epoch 10/70\n",
      "298900/298900 [==============================] - 54s 180us/step - loss: 0.6371 - acc: 0.7162 - val_loss: 0.6907 - val_acc: 0.6989\n",
      "Epoch 11/70\n",
      "298900/298900 [==============================] - 53s 177us/step - loss: 0.6218 - acc: 0.7233 - val_loss: 0.6793 - val_acc: 0.7069 - loss: 0.6217 - acc: \n",
      "Epoch 12/70\n",
      "298900/298900 [==============================] - 53s 176us/step - loss: 0.6086 - acc: 0.7322 - val_loss: 0.6680 - val_acc: 0.7136\n",
      "Epoch 13/70\n",
      "298900/298900 [==============================] - 53s 177us/step - loss: 0.5979 - acc: 0.7380 - val_loss: 0.6678 - val_acc: 0.7154\n",
      "Epoch 14/70\n",
      "298900/298900 [==============================] - 52s 173us/step - loss: 0.5852 - acc: 0.7431 - val_loss: 0.6510 - val_acc: 0.7224\n",
      "Epoch 15/70\n",
      "298900/298900 [==============================] - 53s 177us/step - loss: 0.5752 - acc: 0.7494 - val_loss: 0.6483 - val_acc: 0.7275\n",
      "Epoch 16/70\n",
      "298900/298900 [==============================] - 54s 181us/step - loss: 0.5650 - acc: 0.7540 - val_loss: 0.6408 - val_acc: 0.7306\n",
      "Epoch 17/70\n",
      "298900/298900 [==============================] - 57s 191us/step - loss: 0.5567 - acc: 0.7577 - val_loss: 0.6369 - val_acc: 0.7362\n",
      "Epoch 18/70\n",
      "298900/298900 [==============================] - 52s 173us/step - loss: 0.5495 - acc: 0.7619 - val_loss: 0.6340 - val_acc: 0.7394\n",
      "Epoch 19/70\n",
      "298900/298900 [==============================] - 54s 180us/step - loss: 0.5423 - acc: 0.7652 - val_loss: 0.6272 - val_acc: 0.7411\n",
      "Epoch 20/70\n",
      "298900/298900 [==============================] - 53s 178us/step - loss: 0.5347 - acc: 0.7699 - val_loss: 0.6215 - val_acc: 0.7458\n",
      "Epoch 21/70\n",
      "298900/298900 [==============================] - 51s 171us/step - loss: 0.5295 - acc: 0.7728 - val_loss: 0.6158 - val_acc: 0.7491\n",
      "Epoch 22/70\n",
      "298900/298900 [==============================] - 52s 175us/step - loss: 0.5218 - acc: 0.7762 - val_loss: 0.6197 - val_acc: 0.7483\n",
      "Epoch 23/70\n",
      "298900/298900 [==============================] - 53s 179us/step - loss: 0.5170 - acc: 0.7794 - val_loss: 0.6116 - val_acc: 0.7516\n",
      "Epoch 24/70\n",
      "298900/298900 [==============================] - 54s 179us/step - loss: 0.5124 - acc: 0.7814 - val_loss: 0.6062 - val_acc: 0.7551\n",
      "Epoch 25/70\n",
      "298900/298900 [==============================] - 52s 174us/step - loss: 0.5066 - acc: 0.7841 - val_loss: 0.6051 - val_acc: 0.7560\n",
      "Epoch 26/70\n",
      "298900/298900 [==============================] - 52s 174us/step - loss: 0.5015 - acc: 0.7865 - val_loss: 0.5994 - val_acc: 0.7608\n",
      "Epoch 27/70\n",
      "298900/298900 [==============================] - 54s 180us/step - loss: 0.4966 - acc: 0.7887 - val_loss: 0.6007 - val_acc: 0.7608\n",
      "Epoch 28/70\n",
      "298900/298900 [==============================] - 52s 174us/step - loss: 0.4934 - acc: 0.7906 - val_loss: 0.6015 - val_acc: 0.7605\n",
      "Epoch 29/70\n",
      "298900/298900 [==============================] - 51s 172us/step - loss: 0.4893 - acc: 0.7931 - val_loss: 0.5950 - val_acc: 0.7663\n",
      "Epoch 30/70\n",
      "298900/298900 [==============================] - 56s 187us/step - loss: 0.4856 - acc: 0.7952 - val_loss: 0.5950 - val_acc: 0.7664\n",
      "Epoch 31/70\n",
      "298900/298900 [==============================] - 54s 180us/step - loss: 0.4817 - acc: 0.7964 - val_loss: 0.5914 - val_acc: 0.7683\n",
      "Epoch 32/70\n",
      "298900/298900 [==============================] - 54s 179us/step - loss: 0.4783 - acc: 0.7986 - val_loss: 0.5864 - val_acc: 0.7720\n",
      "Epoch 33/70\n",
      "298900/298900 [==============================] - 54s 179us/step - loss: 0.4748 - acc: 0.7992 - val_loss: 0.5881 - val_acc: 0.7723\n",
      "Epoch 34/70\n",
      "298900/298900 [==============================] - 62s 208us/step - loss: 0.4725 - acc: 0.8014 - val_loss: 0.5857 - val_acc: 0.7730\n",
      "Epoch 35/70\n",
      "298900/298900 [==============================] - 58s 194us/step - loss: 0.4678 - acc: 0.8044 - val_loss: 0.5844 - val_acc: 0.7733\n",
      "Epoch 36/70\n",
      "298900/298900 [==============================] - 55s 184us/step - loss: 0.4652 - acc: 0.8052 - val_loss: 0.5796 - val_acc: 0.7745\n",
      "Epoch 37/70\n",
      "298900/298900 [==============================] - 53s 177us/step - loss: 0.4619 - acc: 0.8069 - val_loss: 0.5796 - val_acc: 0.7738\n",
      "Epoch 38/70\n",
      "298900/298900 [==============================] - 54s 181us/step - loss: 0.4578 - acc: 0.8083 - val_loss: 0.5785 - val_acc: 0.7788\n",
      "Epoch 39/70\n",
      "298900/298900 [==============================] - 50s 169us/step - loss: 0.4563 - acc: 0.8097 - val_loss: 0.5791 - val_acc: 0.7776\n",
      "Epoch 40/70\n",
      "298900/298900 [==============================] - 56s 187us/step - loss: 0.4543 - acc: 0.8111 - val_loss: 0.5742 - val_acc: 0.7808\n",
      "Epoch 41/70\n",
      "298900/298900 [==============================] - 56s 186us/step - loss: 0.4509 - acc: 0.8127 - val_loss: 0.5684 - val_acc: 0.7831\n",
      "Epoch 42/70\n",
      "298900/298900 [==============================] - 52s 175us/step - loss: 0.4502 - acc: 0.8124 - val_loss: 0.5690 - val_acc: 0.7832\n",
      "Epoch 43/70\n",
      "298900/298900 [==============================] - 52s 175us/step - loss: 0.4458 - acc: 0.8147 - val_loss: 0.5737 - val_acc: 0.7836\n",
      "Epoch 44/70\n",
      "298900/298900 [==============================] - 56s 188us/step - loss: 0.4456 - acc: 0.8156 - val_loss: 0.5724 - val_acc: 0.7826\n",
      "Epoch 45/70\n",
      "298900/298900 [==============================] - 54s 182us/step - loss: 0.4419 - acc: 0.8168 - val_loss: 0.5708 - val_acc: 0.7842\n",
      "Epoch 46/70\n",
      "298900/298900 [==============================] - 54s 181us/step - loss: 0.4415 - acc: 0.8167 - val_loss: 0.5685 - val_acc: 0.7854\n",
      "Epoch 47/70\n",
      "298900/298900 [==============================] - 52s 173us/step - loss: 0.4381 - acc: 0.8181 - val_loss: 0.5640 - val_acc: 0.7875\n",
      "Epoch 48/70\n",
      "298900/298900 [==============================] - 56s 186us/step - loss: 0.4370 - acc: 0.8192 - val_loss: 0.5607 - val_acc: 0.7897\n",
      "Epoch 49/70\n",
      "298900/298900 [==============================] - 51s 169us/step - loss: 0.4335 - acc: 0.8211 - val_loss: 0.5622 - val_acc: 0.7884\n",
      "Epoch 50/70\n",
      "298900/298900 [==============================] - 45s 152us/step - loss: 0.4332 - acc: 0.8209 - val_loss: 0.5620 - val_acc: 0.7879\n",
      "Epoch 51/70\n",
      "298900/298900 [==============================] - 55s 184us/step - loss: 0.4300 - acc: 0.8222 - val_loss: 0.5648 - val_acc: 0.7897\n",
      "Epoch 52/70\n",
      "298900/298900 [==============================] - 51s 170us/step - loss: 0.4306 - acc: 0.8226 - val_loss: 0.5605 - val_acc: 0.7921\n",
      "Epoch 53/70\n",
      "298900/298900 [==============================] - 50s 166us/step - loss: 0.4269 - acc: 0.8244 - val_loss: 0.5554 - val_acc: 0.7917\n",
      "Epoch 54/70\n",
      "298900/298900 [==============================] - 51s 171us/step - loss: 0.4250 - acc: 0.8251 - val_loss: 0.5639 - val_acc: 0.7933\n",
      "Epoch 55/70\n",
      "298900/298900 [==============================] - 61s 205us/step - loss: 0.4250 - acc: 0.8242 - val_loss: 0.5648 - val_acc: 0.7933\n",
      "Epoch 56/70\n",
      "298900/298900 [==============================] - 53s 178us/step - loss: 0.4236 - acc: 0.8261 - val_loss: 0.5651 - val_acc: 0.7929\n",
      "Epoch 57/70\n",
      "298900/298900 [==============================] - 52s 175us/step - loss: 0.4205 - acc: 0.8268 - val_loss: 0.5636 - val_acc: 0.7934\n",
      "Epoch 58/70\n",
      "298900/298900 [==============================] - 53s 178us/step - loss: 0.4205 - acc: 0.8282 - val_loss: 0.5520 - val_acc: 0.7951\n",
      "Epoch 59/70\n",
      "298900/298900 [==============================] - 50s 167us/step - loss: 0.4163 - acc: 0.8292 - val_loss: 0.5594 - val_acc: 0.7968\n",
      "Epoch 60/70\n",
      "298900/298900 [==============================] - 55s 184us/step - loss: 0.4170 - acc: 0.8294 - val_loss: 0.5615 - val_acc: 0.7959\n",
      "Epoch 61/70\n",
      "298900/298900 [==============================] - 50s 167us/step - loss: 0.4156 - acc: 0.8294 - val_loss: 0.5552 - val_acc: 0.7977\n",
      "Epoch 62/70\n",
      "298900/298900 [==============================] - 48s 161us/step - loss: 0.4139 - acc: 0.8315 - val_loss: 0.5578 - val_acc: 0.7955\n",
      "Epoch 63/70\n",
      "298900/298900 [==============================] - 48s 160us/step - loss: 0.4123 - acc: 0.8320 - val_loss: 0.5579 - val_acc: 0.7951\n",
      "Epoch 64/70\n",
      "298900/298900 [==============================] - 48s 161us/step - loss: 0.4120 - acc: 0.8320 - val_loss: 0.5570 - val_acc: 0.7968\n",
      "Epoch 65/70\n",
      "298900/298900 [==============================] - 47s 158us/step - loss: 0.4095 - acc: 0.8338 - val_loss: 0.5470 - val_acc: 0.7982\n",
      "Epoch 66/70\n",
      "298900/298900 [==============================] - 48s 161us/step - loss: 0.4090 - acc: 0.8329 - val_loss: 0.5591 - val_acc: 0.7979\n",
      "Epoch 67/70\n",
      "298900/298900 [==============================] - 49s 165us/step - loss: 0.4076 - acc: 0.8341 - val_loss: 0.5492 - val_acc: 0.7999\n",
      "Epoch 68/70\n",
      "298900/298900 [==============================] - 49s 163us/step - loss: 0.4059 - acc: 0.8346 - val_loss: 0.5497 - val_acc: 0.7992\n",
      "Epoch 69/70\n",
      "298900/298900 [==============================] - 49s 165us/step - loss: 0.4055 - acc: 0.8352 - val_loss: 0.5535 - val_acc: 0.7991\n",
      "Epoch 70/70\n",
      "298900/298900 [==============================] - 50s 168us/step - loss: 0.4034 - acc: 0.8366 - val_loss: 0.5527 - val_acc: 0.8011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d6a02ed668>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Sequential()\n",
    "    \n",
    "model.add(LSTM(128, input_shape=train_data.shape[1:], return_sequences=True,activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(64, return_sequences=True,activation='relu'))\n",
    "model.add(LSTM(32, return_sequences=False,activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(dummy_train_target.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, dummy_train_target, epochs=70, batch_size=200, verbose=1, validation_data=(test_data,dummy_test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set and Test Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298900/298900 [==============================] - 33s 109us/step\n",
      "74726/74726 [==============================] - 8s 106us/step\n",
      "Training Set Performance\n",
      "\n",
      "[[91857   777  6885]\n",
      " [ 1453 95027  3276]\n",
      " [12724  5933 80968]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     5-10yrs       0.87      0.92      0.89     99519\n",
      "      <=5yrs       0.93      0.95      0.94     99756\n",
      "      >10yrs       0.89      0.81      0.85     99625\n",
      "\n",
      "    accuracy                           0.90    298900\n",
      "   macro avg       0.90      0.90      0.90    298900\n",
      "weighted avg       0.90      0.90      0.90    298900\n",
      "\n",
      "\n",
      "Test Set Performance\n",
      "\n",
      "[[21280   781  2962]\n",
      " [  944 22230  1612]\n",
      " [ 5571  2993 16353]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     5-10yrs       0.77      0.85      0.81     25023\n",
      "      <=5yrs       0.85      0.90      0.88     24786\n",
      "      >10yrs       0.78      0.66      0.71     24917\n",
      "\n",
      "    accuracy                           0.80     74726\n",
      "   macro avg       0.80      0.80      0.80     74726\n",
      "weighted avg       0.80      0.80      0.80     74726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat_train_class = model.predict_classes(train_data, verbose=1)\n",
    "y_hat_test_class = model.predict_classes(test_data, verbose=1)\n",
    "\n",
    "print('Training Set Performance\\n')\n",
    "conf_matrix_ann = confusion_matrix(encoder.inverse_transform(encoded_train_target), encoder.inverse_transform(y_hat_train_class))\n",
    "print(conf_matrix_ann)\n",
    "cr_ann = classification_report(encoder.inverse_transform(encoded_train_target), encoder.inverse_transform(y_hat_train_class))\n",
    "print(cr_ann)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Test Set Performance\\n')\n",
    "conf_matrix_ann = confusion_matrix(encoder.inverse_transform(encoded_test_target), encoder.inverse_transform(y_hat_test_class))\n",
    "print(conf_matrix_ann)\n",
    "\n",
    "cr_ann = classification_report(encoder.inverse_transform(encoded_test_target), encoder.inverse_transform(y_hat_test_class))\n",
    "print(cr_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the structure of the CNN with 3 hidden layers with [256,128,64] neurons each and ReLU activation function on the hidden layers and softmax activation on the output layer. The output layer has three neuron in line with the number of classes for prediction. 10% Dropout layer is used after 2nd and 3rd hidden layer.\n",
    "\n",
    "Loss Function- Categorical Crossentropy\n",
    "\n",
    "Optimizer- ADAM\n",
    "\n",
    "Observed Metrics- Accuracy\n",
    "\n",
    "An initial fit of the model is developed over a large number of epochs to determine point of overfitting of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 298900 samples, validate on 74726 samples\n",
      "Epoch 1/100\n",
      "298900/298900 [==============================] - 34s 113us/step - loss: 0.9013 - acc: 0.5627 - val_loss: 0.8818 - val_acc: 0.5743\n",
      "Epoch 2/100\n",
      "298900/298900 [==============================] - 31s 103us/step - loss: 0.8727 - acc: 0.5820 - val_loss: 0.8653 - val_acc: 0.5834\n",
      "Epoch 3/100\n",
      "298900/298900 [==============================] - 31s 102us/step - loss: 0.8482 - acc: 0.5970 - val_loss: 0.8453 - val_acc: 0.5962\n",
      "Epoch 4/100\n",
      "298900/298900 [==============================] - 33s 112us/step - loss: 0.8165 - acc: 0.6165 - val_loss: 0.8224 - val_acc: 0.6126\n",
      "Epoch 5/100\n",
      "298900/298900 [==============================] - 29s 96us/step - loss: 0.7826 - acc: 0.6369 - val_loss: 0.7967 - val_acc: 0.6305\n",
      "Epoch 6/100\n",
      "298900/298900 [==============================] - 28s 95us/step - loss: 0.7505 - acc: 0.6556 - val_loss: 0.7761 - val_acc: 0.6424\n",
      "Epoch 7/100\n",
      "298900/298900 [==============================] - 28s 95us/step - loss: 0.7205 - acc: 0.6717 - val_loss: 0.7532 - val_acc: 0.6564\n",
      "Epoch 8/100\n",
      "298900/298900 [==============================] - 28s 95us/step - loss: 0.6961 - acc: 0.6860 - val_loss: 0.7397 - val_acc: 0.6665\n",
      "Epoch 9/100\n",
      "298900/298900 [==============================] - 31s 102us/step - loss: 0.6747 - acc: 0.6974 - val_loss: 0.7205 - val_acc: 0.6783\n",
      "Epoch 10/100\n",
      "298900/298900 [==============================] - 32s 106us/step - loss: 0.6546 - acc: 0.7082 - val_loss: 0.7152 - val_acc: 0.6839\n",
      "Epoch 11/100\n",
      "298900/298900 [==============================] - 31s 105us/step - loss: 0.6381 - acc: 0.7168 - val_loss: 0.6994 - val_acc: 0.6908\n",
      "Epoch 12/100\n",
      "298900/298900 [==============================] - 36s 119us/step - loss: 0.6253 - acc: 0.7238 - val_loss: 0.6896 - val_acc: 0.6987\n",
      "Epoch 13/100\n",
      "298900/298900 [==============================] - 30s 102us/step - loss: 0.6099 - acc: 0.7313 - val_loss: 0.6798 - val_acc: 0.7042\n",
      "Epoch 14/100\n",
      "298900/298900 [==============================] - 30s 99us/step - loss: 0.5980 - acc: 0.7373 - val_loss: 0.6716 - val_acc: 0.7120\n",
      "Epoch 15/100\n",
      "298900/298900 [==============================] - 36s 121us/step - loss: 0.5882 - acc: 0.7425 - val_loss: 0.6632 - val_acc: 0.7161\n",
      "Epoch 16/100\n",
      "298900/298900 [==============================] - 33s 111us/step - loss: 0.5775 - acc: 0.7481 - val_loss: 0.6559 - val_acc: 0.7187\n",
      "Epoch 17/100\n",
      "298900/298900 [==============================] - 28s 94us/step - loss: 0.5677 - acc: 0.7523 - val_loss: 0.6536 - val_acc: 0.7209\n",
      "Epoch 18/100\n",
      "298900/298900 [==============================] - 28s 93us/step - loss: 0.5604 - acc: 0.7566 - val_loss: 0.6554 - val_acc: 0.7234\n",
      "Epoch 19/100\n",
      "298900/298900 [==============================] - 28s 92us/step - loss: 0.5511 - acc: 0.7604 - val_loss: 0.6453 - val_acc: 0.7289\n",
      "Epoch 20/100\n",
      "298900/298900 [==============================] - 27s 91us/step - loss: 0.5464 - acc: 0.7642 - val_loss: 0.6410 - val_acc: 0.7359\n",
      "Epoch 21/100\n",
      "298900/298900 [==============================] - 28s 95us/step - loss: 0.5396 - acc: 0.7663 - val_loss: 0.6402 - val_acc: 0.7359\n",
      "Epoch 22/100\n",
      "298900/298900 [==============================] - 30s 101us/step - loss: 0.5327 - acc: 0.7705 - val_loss: 0.6387 - val_acc: 0.7342s\n",
      "Epoch 23/100\n",
      "298900/298900 [==============================] - 28s 93us/step - loss: 0.5266 - acc: 0.7729 - val_loss: 0.6302 - val_acc: 0.7373 0.5266 - acc: 0.77\n",
      "Epoch 24/100\n",
      "298900/298900 [==============================] - 27s 92us/step - loss: 0.5205 - acc: 0.7762 - val_loss: 0.6308 - val_acc: 0.7422\n",
      "Epoch 25/100\n",
      "298900/298900 [==============================] - 29s 98us/step - loss: 0.5147 - acc: 0.7792 - val_loss: 0.6250 - val_acc: 0.7436\n",
      "Epoch 26/100\n",
      "298900/298900 [==============================] - 29s 98us/step - loss: 0.5100 - acc: 0.7813 - val_loss: 0.6247 - val_acc: 0.7449\n",
      "Epoch 27/100\n",
      "298900/298900 [==============================] - 28s 92us/step - loss: 0.5046 - acc: 0.7832 - val_loss: 0.6207 - val_acc: 0.7482\n",
      "Epoch 28/100\n",
      "298900/298900 [==============================] - 27s 91us/step - loss: 0.5001 - acc: 0.7861 - val_loss: 0.6259 - val_acc: 0.7478\n",
      "Epoch 29/100\n",
      "298900/298900 [==============================] - 27s 91us/step - loss: 0.4958 - acc: 0.7889 - val_loss: 0.6165 - val_acc: 0.7508\n",
      "Epoch 30/100\n",
      "298900/298900 [==============================] - 28s 94us/step - loss: 0.4912 - acc: 0.7903 - val_loss: 0.6141 - val_acc: 0.7534\n",
      "Epoch 31/100\n",
      "298900/298900 [==============================] - 29s 97us/step - loss: 0.4874 - acc: 0.7929 - val_loss: 0.6094 - val_acc: 0.7564\n",
      "Epoch 32/100\n",
      "298900/298900 [==============================] - 28s 94us/step - loss: 0.4831 - acc: 0.7939 - val_loss: 0.6105 - val_acc: 0.7566\n",
      "Epoch 33/100\n",
      "298900/298900 [==============================] - 28s 93us/step - loss: 0.4802 - acc: 0.7954 - val_loss: 0.6095 - val_acc: 0.7566\n",
      "Epoch 34/100\n",
      "298900/298900 [==============================] - 27s 92us/step - loss: 0.4760 - acc: 0.7983 - val_loss: 0.6081 - val_acc: 0.7588\n",
      "Epoch 35/100\n",
      "298900/298900 [==============================] - 29s 98us/step - loss: 0.4724 - acc: 0.7990 - val_loss: 0.6111 - val_acc: 0.7595\n",
      "Epoch 36/100\n",
      "298900/298900 [==============================] - 31s 104us/step - loss: 0.4685 - acc: 0.8009 - val_loss: 0.6064 - val_acc: 0.7611\n",
      "Epoch 37/100\n",
      "298900/298900 [==============================] - 27s 92us/step - loss: 0.4659 - acc: 0.8025 - val_loss: 0.6059 - val_acc: 0.7620\n",
      "Epoch 38/100\n",
      "298900/298900 [==============================] - 27s 89us/step - loss: 0.4614 - acc: 0.8039 - val_loss: 0.6026 - val_acc: 0.7648\n",
      "Epoch 39/100\n",
      "298900/298900 [==============================] - 30s 101us/step - loss: 0.4598 - acc: 0.8051 - val_loss: 0.5986 - val_acc: 0.7661\n",
      "Epoch 40/100\n",
      "298900/298900 [==============================] - 29s 96us/step - loss: 0.4563 - acc: 0.8066 - val_loss: 0.5979 - val_acc: 0.7681\n",
      "Epoch 41/100\n",
      "298900/298900 [==============================] - 27s 92us/step - loss: 0.4535 - acc: 0.8084 - val_loss: 0.6009 - val_acc: 0.7667\n",
      "Epoch 42/100\n",
      "298900/298900 [==============================] - 27s 89us/step - loss: 0.4509 - acc: 0.8089 - val_loss: 0.5988 - val_acc: 0.7690\n",
      "Epoch 43/100\n",
      "298900/298900 [==============================] - 28s 95us/step - loss: 0.4493 - acc: 0.8102 - val_loss: 0.5983 - val_acc: 0.7700\n",
      "Epoch 44/100\n",
      "298900/298900 [==============================] - 28s 95us/step - loss: 0.4463 - acc: 0.8115 - val_loss: 0.5983 - val_acc: 0.7716\n",
      "Epoch 45/100\n",
      "298900/298900 [==============================] - 27s 91us/step - loss: 0.4440 - acc: 0.8127 - val_loss: 0.5966 - val_acc: 0.7701\n",
      "Epoch 46/100\n",
      "298900/298900 [==============================] - 28s 94us/step - loss: 0.4416 - acc: 0.8146 - val_loss: 0.6003 - val_acc: 0.7736\n",
      "Epoch 47/100\n",
      "298900/298900 [==============================] - 27s 91us/step - loss: 0.4391 - acc: 0.8159 - val_loss: 0.5961 - val_acc: 0.7742\n",
      "Epoch 48/100\n",
      "298900/298900 [==============================] - 29s 98us/step - loss: 0.4369 - acc: 0.8161 - val_loss: 0.5946 - val_acc: 0.7754\n",
      "Epoch 49/100\n",
      "298900/298900 [==============================] - 28s 94us/step - loss: 0.4340 - acc: 0.8169 - val_loss: 0.5961 - val_acc: 0.7750\n",
      "Epoch 50/100\n",
      "298900/298900 [==============================] - 28s 93us/step - loss: 0.4323 - acc: 0.8189 - val_loss: 0.5931 - val_acc: 0.7781\n",
      "Epoch 51/100\n",
      "298900/298900 [==============================] - 27s 91us/step - loss: 0.4313 - acc: 0.8188 - val_loss: 0.5984 - val_acc: 0.7756\n",
      "Epoch 52/100\n",
      "298900/298900 [==============================] - 27s 92us/step - loss: 0.4279 - acc: 0.8210 - val_loss: 0.5946 - val_acc: 0.7775\n",
      "Epoch 53/100\n",
      "298900/298900 [==============================] - 29s 97us/step - loss: 0.4260 - acc: 0.8216 - val_loss: 0.5962 - val_acc: 0.7781\n",
      "Epoch 54/100\n",
      "298900/298900 [==============================] - 28s 93us/step - loss: 0.4226 - acc: 0.8233 - val_loss: 0.6027 - val_acc: 0.7777\n",
      "Epoch 55/100\n",
      "298900/298900 [==============================] - 28s 94us/step - loss: 0.4215 - acc: 0.8234 - val_loss: 0.5926 - val_acc: 0.7791\n",
      "Epoch 56/100\n",
      "298900/298900 [==============================] - 28s 93us/step - loss: 0.4197 - acc: 0.8248 - val_loss: 0.5886 - val_acc: 0.7795\n",
      "Epoch 57/100\n",
      "298900/298900 [==============================] - 31s 103us/step - loss: 0.4178 - acc: 0.8254 - val_loss: 0.5977 - val_acc: 0.7805\n",
      "Epoch 58/100\n",
      "298900/298900 [==============================] - 29s 97us/step - loss: 0.4168 - acc: 0.8261 - val_loss: 0.5914 - val_acc: 0.7821\n",
      "Epoch 59/100\n",
      "298900/298900 [==============================] - 31s 105us/step - loss: 0.4151 - acc: 0.8266 - val_loss: 0.5878 - val_acc: 0.7845\n",
      "Epoch 60/100\n",
      "298900/298900 [==============================] - 33s 112us/step - loss: 0.4133 - acc: 0.8278 - val_loss: 0.5842 - val_acc: 0.7836\n",
      "Epoch 61/100\n",
      "298900/298900 [==============================] - 31s 105us/step - loss: 0.4116 - acc: 0.8288 - val_loss: 0.5852 - val_acc: 0.7842\n",
      "Epoch 62/100\n",
      "298900/298900 [==============================] - 30s 101us/step - loss: 0.4103 - acc: 0.8282 - val_loss: 0.5939 - val_acc: 0.7839\n",
      "Epoch 63/100\n",
      "298900/298900 [==============================] - 28s 95us/step - loss: 0.4090 - acc: 0.8293 - val_loss: 0.5844 - val_acc: 0.7850\n",
      "Epoch 64/100\n",
      "298900/298900 [==============================] - 28s 94us/step - loss: 0.4097 - acc: 0.8280 - val_loss: 0.5899 - val_acc: 0.7833\n",
      "Epoch 65/100\n",
      "298900/298900 [==============================] - 28s 93us/step - loss: 0.4060 - acc: 0.8307 - val_loss: 0.5892 - val_acc: 0.7847\n",
      "Epoch 66/100\n",
      "298900/298900 [==============================] - 28s 92us/step - loss: 0.4052 - acc: 0.8312 - val_loss: 0.5810 - val_acc: 0.7885\n",
      "Epoch 67/100\n",
      "298900/298900 [==============================] - 27s 92us/step - loss: 0.4036 - acc: 0.8319 - val_loss: 0.5865 - val_acc: 0.7865\n",
      "Epoch 68/100\n",
      "298900/298900 [==============================] - 28s 94us/step - loss: 0.4017 - acc: 0.8332 - val_loss: 0.5778 - val_acc: 0.7904\n",
      "Epoch 69/100\n",
      "298900/298900 [==============================] - 27s 92us/step - loss: 0.4012 - acc: 0.8337 - val_loss: 0.5754 - val_acc: 0.7903\n",
      "Epoch 70/100\n",
      "298900/298900 [==============================] - 30s 99us/step - loss: 0.3972 - acc: 0.8352 - val_loss: 0.5818 - val_acc: 0.7910\n",
      "Epoch 71/100\n",
      "298900/298900 [==============================] - 28s 93us/step - loss: 0.3979 - acc: 0.8351 - val_loss: 0.5912 - val_acc: 0.7873\n",
      "Epoch 72/100\n",
      "298900/298900 [==============================] - 29s 97us/step - loss: 0.3963 - acc: 0.8349 - val_loss: 0.5884 - val_acc: 0.7895\n",
      "Epoch 73/100\n",
      "298900/298900 [==============================] - 27s 92us/step - loss: 0.3942 - acc: 0.8360 - val_loss: 0.5890 - val_acc: 0.7912\n",
      "Epoch 74/100\n",
      "298900/298900 [==============================] - 28s 93us/step - loss: 0.3942 - acc: 0.8362 - val_loss: 0.5792 - val_acc: 0.7906\n",
      "Epoch 75/100\n",
      "298900/298900 [==============================] - 30s 99us/step - loss: 0.3930 - acc: 0.8377 - val_loss: 0.5889 - val_acc: 0.7910\n",
      "Epoch 76/100\n",
      "298900/298900 [==============================] - 28s 93us/step - loss: 0.3898 - acc: 0.8383 - val_loss: 0.5951 - val_acc: 0.7917\n",
      "Epoch 77/100\n",
      "298900/298900 [==============================] - 29s 97us/step - loss: 0.3889 - acc: 0.8388 - val_loss: 0.5911 - val_acc: 0.7921\n",
      "Epoch 78/100\n",
      "298900/298900 [==============================] - 31s 105us/step - loss: 0.3874 - acc: 0.8393 - val_loss: 0.5745 - val_acc: 0.7923\n",
      "Epoch 79/100\n",
      "298900/298900 [==============================] - 33s 109us/step - loss: 0.3861 - acc: 0.8399 - val_loss: 0.5901 - val_acc: 0.7918\n",
      "Epoch 80/100\n",
      "298900/298900 [==============================] - 31s 102us/step - loss: 0.3859 - acc: 0.8401 - val_loss: 0.5862 - val_acc: 0.7933\n",
      "Epoch 81/100\n",
      "298900/298900 [==============================] - 30s 101us/step - loss: 0.3844 - acc: 0.8415 - val_loss: 0.5769 - val_acc: 0.7969\n",
      "Epoch 82/100\n",
      "298900/298900 [==============================] - 29s 97us/step - loss: 0.3824 - acc: 0.8414 - val_loss: 0.5906 - val_acc: 0.7942\n",
      "Epoch 83/100\n",
      "298900/298900 [==============================] - 28s 94us/step - loss: 0.3833 - acc: 0.8415 - val_loss: 0.5817 - val_acc: 0.7960\n",
      "Epoch 84/100\n",
      "298900/298900 [==============================] - 29s 97us/step - loss: 0.3824 - acc: 0.8419 - val_loss: 0.5885 - val_acc: 0.7942\n",
      "Epoch 85/100\n",
      "298900/298900 [==============================] - 33s 110us/step - loss: 0.3804 - acc: 0.8421 - val_loss: 0.5838 - val_acc: 0.7947\n",
      "Epoch 86/100\n",
      "298900/298900 [==============================] - 29s 97us/step - loss: 0.3798 - acc: 0.8434 - val_loss: 0.5848 - val_acc: 0.7972\n",
      "Epoch 87/100\n",
      "298900/298900 [==============================] - 30s 99us/step - loss: 0.3790 - acc: 0.8437 - val_loss: 0.5867 - val_acc: 0.7974\n",
      "Epoch 88/100\n",
      "298900/298900 [==============================] - 27s 92us/step - loss: 0.3785 - acc: 0.8440 - val_loss: 0.5788 - val_acc: 0.7995\n",
      "Epoch 89/100\n",
      "298900/298900 [==============================] - 31s 102us/step - loss: 0.3755 - acc: 0.8462 - val_loss: 0.5859 - val_acc: 0.7973\n",
      "Epoch 90/100\n",
      "298900/298900 [==============================] - 28s 93us/step - loss: 0.3768 - acc: 0.8446 - val_loss: 0.5846 - val_acc: 0.7993\n",
      "Epoch 91/100\n",
      "298900/298900 [==============================] - 34s 113us/step - loss: 0.3749 - acc: 0.8454 - val_loss: 0.5826 - val_acc: 0.7966\n",
      "Epoch 92/100\n",
      "298900/298900 [==============================] - 32s 107us/step - loss: 0.3746 - acc: 0.8455 - val_loss: 0.5857 - val_acc: 0.7957\n",
      "Epoch 93/100\n",
      "298900/298900 [==============================] - 29s 97us/step - loss: 0.3729 - acc: 0.8459 - val_loss: 0.5816 - val_acc: 0.7989\n",
      "Epoch 94/100\n",
      "298900/298900 [==============================] - 31s 105us/step - loss: 0.3725 - acc: 0.8471 - val_loss: 0.5848 - val_acc: 0.7989\n",
      "Epoch 95/100\n",
      "298900/298900 [==============================] - 33s 112us/step - loss: 0.3715 - acc: 0.8466 - val_loss: 0.5926 - val_acc: 0.7967\n",
      "Epoch 96/100\n",
      "298900/298900 [==============================] - 28s 94us/step - loss: 0.3699 - acc: 0.8476 - val_loss: 0.5756 - val_acc: 0.8019\n",
      "Epoch 97/100\n",
      "298900/298900 [==============================] - 28s 94us/step - loss: 0.3695 - acc: 0.8473 - val_loss: 0.5765 - val_acc: 0.8009\n",
      "Epoch 98/100\n",
      "298900/298900 [==============================] - 31s 103us/step - loss: 0.3676 - acc: 0.8489 - val_loss: 0.5852 - val_acc: 0.7987\n",
      "Epoch 99/100\n",
      "298900/298900 [==============================] - 31s 104us/step - loss: 0.3666 - acc: 0.8493 - val_loss: 0.5819 - val_acc: 0.8008\n",
      "Epoch 100/100\n",
      "298900/298900 [==============================] - 27s 92us/step - loss: 0.3665 - acc: 0.8495 - val_loss: 0.5763 - val_acc: 0.7999\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "    \n",
    "model.add(Conv1D(256, 1, input_shape=train_data.shape[1:], activation='relu'))\n",
    "model.add(Conv1D(128, 1, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(64, 1, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(dummy_train_target.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "hist=model.fit(train_data, dummy_train_target, epochs=100, batch_size=200, verbose=1, validation_data=(test_data,dummy_test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_num = np.arange(0, 100)\n",
    "plt.figure()\n",
    "plt.plot(epoch_num, hist.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(epoch_num, hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(epoch_num, hist.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(epoch_num, hist.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig('Breast Cancer CNN Epoch Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form the graph above, it can be seen that validation loss flattens after 80th eophc while training loss decreases. Hence, to prevent overfitting, the model is retrained to 80 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 298900 samples, validate on 74726 samples\n",
      "Epoch 1/80\n",
      "298900/298900 [==============================] - 32s 107us/step - loss: 0.9012 - acc: 0.5629 - val_loss: 0.8825 - val_acc: 0.5745\n",
      "Epoch 2/80\n",
      "298900/298900 [==============================] - 30s 99us/step - loss: 0.8718 - acc: 0.5827 - val_loss: 0.8651 - val_acc: 0.5847\n",
      "Epoch 3/80\n",
      "298900/298900 [==============================] - 32s 106us/step - loss: 0.8457 - acc: 0.5984 - val_loss: 0.8480 - val_acc: 0.5983\n",
      "Epoch 4/80\n",
      "298900/298900 [==============================] - 31s 105us/step - loss: 0.8137 - acc: 0.6176 - val_loss: 0.8209 - val_acc: 0.6114\n",
      "Epoch 5/80\n",
      "298900/298900 [==============================] - 31s 105us/step - loss: 0.7805 - acc: 0.6368 - val_loss: 0.7935 - val_acc: 0.6310\n",
      "Epoch 6/80\n",
      "298900/298900 [==============================] - 35s 117us/step - loss: 0.7484 - acc: 0.6553 - val_loss: 0.7756 - val_acc: 0.6434\n",
      "Epoch 7/80\n",
      "298900/298900 [==============================] - 31s 103us/step - loss: 0.7198 - acc: 0.6726 - val_loss: 0.7538 - val_acc: 0.6559\n",
      "Epoch 8/80\n",
      "298900/298900 [==============================] - 31s 103us/step - loss: 0.6950 - acc: 0.6855 - val_loss: 0.7372 - val_acc: 0.66960s - loss: 0.6949 - acc: 0.6\n",
      "Epoch 9/80\n",
      "298900/298900 [==============================] - 28s 93us/step - loss: 0.6731 - acc: 0.6973 - val_loss: 0.7254 - val_acc: 0.6778\n",
      "Epoch 10/80\n",
      "298900/298900 [==============================] - 28s 93us/step - loss: 0.6539 - acc: 0.7081 - val_loss: 0.7071 - val_acc: 0.6858\n",
      "Epoch 11/80\n",
      "298900/298900 [==============================] - 30s 101us/step - loss: 0.6376 - acc: 0.7166 - val_loss: 0.6994 - val_acc: 0.6923\n",
      "Epoch 12/80\n",
      "298900/298900 [==============================] - 27s 90us/step - loss: 0.6217 - acc: 0.7249 - val_loss: 0.6903 - val_acc: 0.6970\n",
      "Epoch 13/80\n",
      "298900/298900 [==============================] - 28s 94us/step - loss: 0.6091 - acc: 0.7312 - val_loss: 0.6822 - val_acc: 0.7048\n",
      "Epoch 14/80\n",
      "298900/298900 [==============================] - 35s 115us/step - loss: 0.5980 - acc: 0.7371 - val_loss: 0.6776 - val_acc: 0.7058:\n",
      "Epoch 15/80\n",
      "298900/298900 [==============================] - 27s 91us/step - loss: 0.5883 - acc: 0.7426 - val_loss: 0.6726 - val_acc: 0.7117\n",
      "Epoch 16/80\n",
      "298900/298900 [==============================] - 29s 97us/step - loss: 0.5752 - acc: 0.7490 - val_loss: 0.6581 - val_acc: 0.7197\n",
      "Epoch 17/80\n",
      "298900/298900 [==============================] - 33s 111us/step - loss: 0.5662 - acc: 0.7527 - val_loss: 0.6511 - val_acc: 0.7239\n",
      "Epoch 18/80\n",
      "298900/298900 [==============================] - 33s 110us/step - loss: 0.5580 - acc: 0.7573 - val_loss: 0.6478 - val_acc: 0.7278\n",
      "Epoch 19/80\n",
      "298900/298900 [==============================] - 36s 122us/step - loss: 0.5511 - acc: 0.7610 - val_loss: 0.6451 - val_acc: 0.7298\n",
      "Epoch 20/80\n",
      "298900/298900 [==============================] - 37s 124us/step - loss: 0.5451 - acc: 0.7643 - val_loss: 0.6422 - val_acc: 0.7330ETA: 0s - loss: 0.5447 - acc: 0.76 - ETA: 0s - loss: 0.5448 - acc: 0\n",
      "Epoch 21/80\n",
      "298900/298900 [==============================] - 34s 115us/step - loss: 0.5353 - acc: 0.7690 - val_loss: 0.6379 - val_acc: 0.7363\n",
      "Epoch 22/80\n",
      "298900/298900 [==============================] - 35s 117us/step - loss: 0.5293 - acc: 0.7721 - val_loss: 0.6362 - val_acc: 0.7383\n",
      "Epoch 23/80\n",
      "298900/298900 [==============================] - 30s 102us/step - loss: 0.5247 - acc: 0.7748 - val_loss: 0.6306 - val_acc: 0.7392\n",
      "Epoch 24/80\n",
      "298900/298900 [==============================] - 31s 105us/step - loss: 0.5187 - acc: 0.7774 - val_loss: 0.6314 - val_acc: 0.7419\n",
      "Epoch 25/80\n",
      "298900/298900 [==============================] - 75s 251us/step - loss: 0.5129 - acc: 0.7807 - val_loss: 0.6219 - val_acc: 0.7462\n",
      "Epoch 26/80\n",
      "298900/298900 [==============================] - 45s 151us/step - loss: 0.5068 - acc: 0.7837 - val_loss: 0.6209 - val_acc: 0.7501\n",
      "Epoch 27/80\n",
      "298900/298900 [==============================] - 45s 149us/step - loss: 0.5023 - acc: 0.7859 - val_loss: 0.6258 - val_acc: 0.7465\n",
      "Epoch 28/80\n",
      "298900/298900 [==============================] - 42s 140us/step - loss: 0.4964 - acc: 0.7883 - val_loss: 0.6175 - val_acc: 0.7518\n",
      "Epoch 29/80\n",
      "298900/298900 [==============================] - 42s 141us/step - loss: 0.4933 - acc: 0.7900 - val_loss: 0.6129 - val_acc: 0.7544\n",
      "Epoch 30/80\n",
      "298900/298900 [==============================] - 43s 144us/step - loss: 0.4885 - acc: 0.7926 - val_loss: 0.6115 - val_acc: 0.7560\n",
      "Epoch 31/80\n",
      "298900/298900 [==============================] - 44s 148us/step - loss: 0.4856 - acc: 0.7943 - val_loss: 0.6073 - val_acc: 0.7552\n",
      "Epoch 32/80\n",
      "298900/298900 [==============================] - 45s 149us/step - loss: 0.4799 - acc: 0.7965 - val_loss: 0.6124 - val_acc: 0.7556\n",
      "Epoch 33/80\n",
      "298900/298900 [==============================] - 46s 153us/step - loss: 0.4763 - acc: 0.7980 - val_loss: 0.6067 - val_acc: 0.7593\n",
      "Epoch 34/80\n",
      "298900/298900 [==============================] - 46s 155us/step - loss: 0.4734 - acc: 0.7997 - val_loss: 0.6035 - val_acc: 0.7617\n",
      "Epoch 35/80\n",
      "298900/298900 [==============================] - 46s 153us/step - loss: 0.4701 - acc: 0.8009 - val_loss: 0.6024 - val_acc: 0.7620\n",
      "Epoch 36/80\n",
      "298900/298900 [==============================] - 45s 152us/step - loss: 0.4669 - acc: 0.8035 - val_loss: 0.5990 - val_acc: 0.7631\n",
      "Epoch 37/80\n",
      "298900/298900 [==============================] - 45s 149us/step - loss: 0.4619 - acc: 0.8052 - val_loss: 0.6038 - val_acc: 0.7646\n",
      "Epoch 38/80\n",
      "298900/298900 [==============================] - 44s 148us/step - loss: 0.4599 - acc: 0.8069 - val_loss: 0.5973 - val_acc: 0.7670\n",
      "Epoch 39/80\n",
      "298900/298900 [==============================] - 44s 146us/step - loss: 0.4557 - acc: 0.8077 - val_loss: 0.5948 - val_acc: 0.7694\n",
      "Epoch 40/80\n",
      "298900/298900 [==============================] - 45s 152us/step - loss: 0.4536 - acc: 0.8090 - val_loss: 0.6038 - val_acc: 0.7667\n",
      "Epoch 41/80\n",
      "298900/298900 [==============================] - 46s 154us/step - loss: 0.4503 - acc: 0.8105 - val_loss: 0.5984 - val_acc: 0.7705\n",
      "Epoch 42/80\n",
      "298900/298900 [==============================] - 46s 155us/step - loss: 0.4486 - acc: 0.8118 - val_loss: 0.5980 - val_acc: 0.7692\n",
      "Epoch 43/80\n",
      "298900/298900 [==============================] - 47s 156us/step - loss: 0.4445 - acc: 0.8133 - val_loss: 0.5888 - val_acc: 0.7738\n",
      "Epoch 44/80\n",
      "298900/298900 [==============================] - 45s 150us/step - loss: 0.4430 - acc: 0.8139 - val_loss: 0.5961 - val_acc: 0.7727\n",
      "Epoch 45/80\n",
      "298900/298900 [==============================] - 45s 152us/step - loss: 0.4404 - acc: 0.8154 - val_loss: 0.5925 - val_acc: 0.7749\n",
      "Epoch 46/80\n",
      "298900/298900 [==============================] - 45s 151us/step - loss: 0.4367 - acc: 0.8168 - val_loss: 0.5917 - val_acc: 0.7752\n",
      "Epoch 47/80\n",
      "298900/298900 [==============================] - 45s 149us/step - loss: 0.4337 - acc: 0.8177 - val_loss: 0.6063 - val_acc: 0.7723\n",
      "Epoch 48/80\n",
      "298900/298900 [==============================] - 45s 151us/step - loss: 0.4350 - acc: 0.8187 - val_loss: 0.5944 - val_acc: 0.7744\n",
      "Epoch 49/80\n",
      "298900/298900 [==============================] - 45s 150us/step - loss: 0.4300 - acc: 0.8197 - val_loss: 0.5884 - val_acc: 0.7791\n",
      "Epoch 50/80\n",
      "298900/298900 [==============================] - 45s 150us/step - loss: 0.4284 - acc: 0.8213 - val_loss: 0.5876 - val_acc: 0.7800\n",
      "Epoch 51/80\n",
      "298900/298900 [==============================] - 44s 149us/step - loss: 0.4262 - acc: 0.8224 - val_loss: 0.5911 - val_acc: 0.7791\n",
      "Epoch 52/80\n",
      "298900/298900 [==============================] - 45s 151us/step - loss: 0.4250 - acc: 0.8235 - val_loss: 0.5860 - val_acc: 0.7801\n",
      "Epoch 53/80\n",
      "298900/298900 [==============================] - 46s 154us/step - loss: 0.4227 - acc: 0.8246 - val_loss: 0.5880 - val_acc: 0.7808\n",
      "Epoch 54/80\n",
      "298900/298900 [==============================] - 49s 162us/step - loss: 0.4212 - acc: 0.8247 - val_loss: 0.5831 - val_acc: 0.7815\n",
      "Epoch 55/80\n",
      "298900/298900 [==============================] - 46s 155us/step - loss: 0.4171 - acc: 0.8267 - val_loss: 0.5921 - val_acc: 0.7817\n",
      "Epoch 56/80\n",
      "298900/298900 [==============================] - 47s 158us/step - loss: 0.4138 - acc: 0.8275 - val_loss: 0.5862 - val_acc: 0.7837\n",
      "Epoch 57/80\n",
      "298900/298900 [==============================] - 44s 147us/step - loss: 0.4143 - acc: 0.8282 - val_loss: 0.5831 - val_acc: 0.7845\n",
      "Epoch 58/80\n",
      "298900/298900 [==============================] - 44s 149us/step - loss: 0.4106 - acc: 0.8294 - val_loss: 0.5818 - val_acc: 0.7863\n",
      "Epoch 59/80\n",
      "298900/298900 [==============================] - 44s 148us/step - loss: 0.4107 - acc: 0.8312 - val_loss: 0.5850 - val_acc: 0.7847\n",
      "Epoch 60/80\n",
      "298900/298900 [==============================] - 46s 153us/step - loss: 0.4082 - acc: 0.8314 - val_loss: 0.5887 - val_acc: 0.7852\n",
      "Epoch 61/80\n",
      "298900/298900 [==============================] - 47s 157us/step - loss: 0.4083 - acc: 0.8311 - val_loss: 0.5850 - val_acc: 0.7861\n",
      "Epoch 62/80\n",
      "298900/298900 [==============================] - 48s 162us/step - loss: 0.4049 - acc: 0.8319 - val_loss: 0.5906 - val_acc: 0.7877\n",
      "Epoch 63/80\n",
      "298900/298900 [==============================] - 46s 155us/step - loss: 0.4029 - acc: 0.8337 - val_loss: 0.5878 - val_acc: 0.7882\n",
      "Epoch 64/80\n",
      "298900/298900 [==============================] - 46s 154us/step - loss: 0.4039 - acc: 0.8334 - val_loss: 0.5846 - val_acc: 0.7875\n",
      "Epoch 65/80\n",
      "298900/298900 [==============================] - 45s 152us/step - loss: 0.4004 - acc: 0.8340 - val_loss: 0.5875 - val_acc: 0.7893\n",
      "Epoch 66/80\n",
      "298900/298900 [==============================] - 45s 149us/step - loss: 0.3990 - acc: 0.8351 - val_loss: 0.5783 - val_acc: 0.7911\n",
      "Epoch 67/80\n",
      "298900/298900 [==============================] - 48s 162us/step - loss: 0.3976 - acc: 0.8364 - val_loss: 0.5786 - val_acc: 0.7910\n",
      "Epoch 68/80\n",
      "298900/298900 [==============================] - 44s 146us/step - loss: 0.3954 - acc: 0.8363 - val_loss: 0.5817 - val_acc: 0.7918\n",
      "Epoch 69/80\n",
      "298900/298900 [==============================] - 45s 152us/step - loss: 0.3949 - acc: 0.8362 - val_loss: 0.5817 - val_acc: 0.7899\n",
      "Epoch 70/80\n",
      "298900/298900 [==============================] - 50s 166us/step - loss: 0.3933 - acc: 0.8378 - val_loss: 0.5855 - val_acc: 0.7913\n",
      "Epoch 71/80\n",
      "298900/298900 [==============================] - 46s 154us/step - loss: 0.3933 - acc: 0.8376 - val_loss: 0.5846 - val_acc: 0.7902\n",
      "Epoch 72/80\n",
      "298900/298900 [==============================] - 44s 146us/step - loss: 0.3927 - acc: 0.8377 - val_loss: 0.5789 - val_acc: 0.7950\n",
      "Epoch 73/80\n",
      "298900/298900 [==============================] - 45s 152us/step - loss: 0.3883 - acc: 0.8406 - val_loss: 0.5803 - val_acc: 0.7951\n",
      "Epoch 74/80\n",
      "298900/298900 [==============================] - 57s 192us/step - loss: 0.3881 - acc: 0.8402 - val_loss: 0.5830 - val_acc: 0.7936\n",
      "Epoch 75/80\n",
      "298900/298900 [==============================] - 29s 96us/step - loss: 0.3867 - acc: 0.8409 - val_loss: 0.5825 - val_acc: 0.7948\n",
      "Epoch 76/80\n",
      "298900/298900 [==============================] - 28s 92us/step - loss: 0.3852 - acc: 0.8418 - val_loss: 0.5843 - val_acc: 0.7950\n",
      "Epoch 77/80\n",
      "298900/298900 [==============================] - 39s 130us/step - loss: 0.3845 - acc: 0.8412 - val_loss: 0.5814 - val_acc: 0.7945\n",
      "Epoch 78/80\n",
      "298900/298900 [==============================] - 39s 131us/step - loss: 0.3846 - acc: 0.8423 - val_loss: 0.5760 - val_acc: 0.7977\n",
      "Epoch 79/80\n",
      "298900/298900 [==============================] - 39s 130us/step - loss: 0.3828 - acc: 0.8433 - val_loss: 0.5744 - val_acc: 0.7988\n",
      "Epoch 80/80\n",
      "298900/298900 [==============================] - 39s 131us/step - loss: 0.3820 - acc: 0.8428 - val_loss: 0.5756 - val_acc: 0.7979\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv1D(256, 1, input_shape=train_data.shape[1:], activation='relu'))\n",
    "model.add(Conv1D(128, 1, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(64, 1, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(dummy_train_target.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "hist=model.fit(train_data, dummy_train_target, epochs=80, batch_size=200, verbose=1, validation_data=(test_data,dummy_test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Set and Test Performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298900/298900 [==============================] - 105s 351us/step\n",
      "74726/74726 [==============================] - 8s 107us/step\n",
      "\n",
      "Training Set Performance\n",
      "\n",
      "[[88039  2424  9056]\n",
      " [  627 96451  2678]\n",
      " [10062  8214 81349]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     5-10yrs       0.89      0.88      0.89     99519\n",
      "      <=5yrs       0.90      0.97      0.93     99756\n",
      "      >10yrs       0.87      0.82      0.84     99625\n",
      "\n",
      "    accuracy                           0.89    298900\n",
      "   macro avg       0.89      0.89      0.89    298900\n",
      "weighted avg       0.89      0.89      0.89    298900\n",
      "\n",
      "\n",
      "Test Set Performance\n",
      "\n",
      "[[20114  1323  3586]\n",
      " [  582 22807  1397]\n",
      " [ 4783  3433 16701]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     5-10yrs       0.79      0.80      0.80     25023\n",
      "      <=5yrs       0.83      0.92      0.87     24786\n",
      "      >10yrs       0.77      0.67      0.72     24917\n",
      "\n",
      "    accuracy                           0.80     74726\n",
      "   macro avg       0.80      0.80      0.79     74726\n",
      "weighted avg       0.80      0.80      0.79     74726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat_train_class = model.predict_classes(train_data, verbose=1)\n",
    "y_hat_test_class = model.predict_classes(test_data, verbose=1)\n",
    "\n",
    "print('\\nTraining Set Performance\\n')\n",
    "conf_matrix_ann = confusion_matrix(encoder.inverse_transform(encoded_train_target), encoder.inverse_transform(y_hat_train_class))\n",
    "print(conf_matrix_ann)\n",
    "cr_ann = classification_report(encoder.inverse_transform(encoded_train_target), encoder.inverse_transform(y_hat_train_class))\n",
    "print(cr_ann)\n",
    "\n",
    "print('\\nTest Set Performance\\n')\n",
    "conf_matrix_ann = confusion_matrix(encoder.inverse_transform(encoded_test_target), encoder.inverse_transform(y_hat_test_class))\n",
    "print(conf_matrix_ann)\n",
    "\n",
    "cr_ann = classification_report(encoder.inverse_transform(encoded_test_target), encoder.inverse_transform(y_hat_test_class))\n",
    "print(cr_ann)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
